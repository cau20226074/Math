{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4907929f",
   "metadata": {},
   "source": [
    "# Linear Algebra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da0cd4e",
   "metadata": {},
   "source": [
    "- Linear Algebra and Its Applications, 6th Edition 의 내용을 증명과 함께 정리해보았다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f49ca92",
   "metadata": {},
   "source": [
    "*****"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65e35c6",
   "metadata": {},
   "source": [
    "## Chapter 1 : Linear Equations in Linear Algebra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d79325",
   "metadata": {},
   "source": [
    "### 1) System of Linear Equations\n",
    "\n",
    "linear equation : $a_1x_1 + a_2x_2 + \\cdots + a_nx_n = b$ where $b$ and the coefficients $a_1,...,a_n$ are real or complex numbers <br>\n",
    "A system of linear equations (linear system) : collection of linear equations involving the same variables\n",
    "- solution of the linear system : a list $(s_1,...,s_n)$ that makes each equation a true statement when $s_1,...,s_n$ is substituted for $x_1,...,x_n$ \n",
    "- solution set of the linear system : the set of all possible solutions\n",
    "  - equivalent : Two linear systems have the same solution set\n",
    "  - consistent : A system of linear equations has unique solution or infinitely many solutions\n",
    "  - inconsistent : A system of linear equations has no solution\n",
    "  \n",
    "The essential information of a linear system can be recorded compactly in a rectangular array called a matrix\n",
    "- A system of linear equations <br>\n",
    "  $1x_1 - 2x_2 + 1x_3 = \\ \\ 0$ <br>\n",
    "  $0x_1 + 2x_2 - 8x_3 = \\ \\ 8$ <br>\n",
    "  $5x_1 + 0x_2 - 5x_3 = 10$ \n",
    "- coefficient matrix ( matrix of coefficients ) <br>\n",
    "  $\\begin{bmatrix} 1 & -2 & 1 \\\\ 0 & 2 & -8 \\\\ 5 & 0 & -5 \\end{bmatrix}$\n",
    "- augment matrix <br>\n",
    "  $\\begin{bmatrix} 1 & -2 & 1 & 0 \\\\ 0 & 2 & -8 & 8 \\\\ 5 & 0 & -5 & 10 \\end{bmatrix}$\n",
    "  \n",
    "elementary row operations\n",
    "- (Replacement) replace one row by the sum of itself and a multiple of another row\n",
    "- (Interchange) Interchange two rows\n",
    "- (Scaling) Multiply all entries in a row by a nonzero constant\n",
    "\n",
    "row equivalent : there is a sequence of elementary row operations that transforms one matrix into the other <br>\n",
    "a system that is changed to a new one via row operation can be returned to original system via row operation. therefore, <br>\n",
    "If the augmented matrices of two linear systems are row equivalent, then the two systems have the same solution set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3d55b2",
   "metadata": {},
   "source": [
    "### 2) Row Reduction and Echelon Forms\n",
    "\n",
    "reduced (row) echelon form [ 1 ~ 3 : (row) echelon form ]\n",
    "1. All nonzero rows are above any rwos of all zeros\n",
    "2. Each leading entry of a row is in a column to the right of the leading entry of the row above it\n",
    "3. All entries in a column below a leading entry are zeros\n",
    "4. The leading entry in each nonzero row is $1$.\n",
    "5. Each leading $1$ is the only nonzero entry in its column\n",
    "\n",
    "Theorem 1 (Uniqueness of the Reduced Echelon Form) <br>\n",
    ": Each matrix is row equivalent to one and only one reduced echelon matrix\n",
    "\n",
    "pivot position in $A$ : a location in $A$ that corresponds to a leading 1 in the reduced echelon form of A <br>\n",
    "A nonzero entry, or pivot, must be placed in pivot position <br>\n",
    "\n",
    "pivot column of $A$ : a column of $A$ that contains a pivot position <br>\n",
    "\n",
    "Solutions of Linear Systems\n",
    "- basic variable : The variables corresponding to pivot columns in the matrix\n",
    "- free variable : The other variable  [ we are free to choose any value for this varaible ]\n",
    "\n",
    "Theorem 2 (Existence and Uniquenesss Theorem)\n",
    "- A linear system is consistent if and only if an echelon form of the augmented matrix has no row of the form <br>\n",
    "  $\\begin{bmatrix} 0 & \\cdots & 0 & b \\end{bmatrix}$ with $b$ nonzero\n",
    "- If linear system is consistent\n",
    "  - If there are no free variables, then the solution set contains a unique solution\n",
    "  - If there is at least one free variable, then the solution set contains infinitely many solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bd221b",
   "metadata": {},
   "source": [
    "### 3) Vector Equations\n",
    "\n",
    "A matrix with only one column is called a column vector or simply a vector <br>\n",
    "For all $u, v, w$ in $R^n$ and all scalars $c$ and $d$:\n",
    "- $u+v = v+u$\n",
    "- $(u+v)+w = u+(v+w)$\n",
    "- $u+0=0+u=u$\n",
    "- $u+(-u)=-u+u=0$ where $-u$ denotes $(-1)u$\n",
    "- $c(u+v) = cu+cv$\n",
    "- $(c+d)u = cu+du$\n",
    "- $c(du) = (cd)u$\n",
    "- $1u = u$\n",
    "\n",
    "linear combination of $v_1,...,v_p$ with weights $c_1,...,c_p$ : $y = c_1v_1 + \\cdots + c_pv_p$ <br>\n",
    "- A vector equation $x_1a_1 + x_2a_2 + \\cdots + x_na_n = b$ has the same solution set <br>\n",
    "  as the linear system whose augmented matrix is $\\begin{bmatrix} a_1 & a_2 & \\cdots & a_n & b \\end{bmatrix}$\n",
    "- In particular, $b$ can be generated by a linear combination of $a_1,...,a_n$ if and only if <br>\n",
    "  there exists a solution to the linear system corresponding to the above matrix\n",
    "\n",
    "If $v_1,...,v_p$ are in $R^n$, then the set of all linear combinations of $v_1,...,v_p$ is decoted by Span $\\{v_1,...,v_p\\}$ <br>\n",
    "and is called the subset of $R^n$ spanned (generated) by $v_1,...,v_p$ <br>\n",
    "That is, Span$\\{v_1,...,v_p\\}$ is the collection of all vectors that can be written in the form <br>\n",
    "$c_1v_1 + c_2v_2 + \\cdots + c_pv_p$ with $c_1,...,c_p$ scalars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7069a063",
   "metadata": {},
   "source": [
    "### 4) The Matrix Equation\n",
    "\n",
    "If $A$ is an $m \\times n$ matrix, with columns $a_1, ..., a_n$, and if $x$ is in $R^n$, <br>\n",
    "then $Ax$ is the linear combination of the columns of $A$ using the corresponding entries in $x$ as weights \n",
    "- Theorem 3\n",
    "  - If $b$ is in $R^m$, the matrix equation $Ax = b$ has the same solution set <br>\n",
    "    as the vector equation $x_1a_1 + x_2a_2 + \\cdots + x_na_n = b$\n",
    "- Theorem 4 : for a particular $A$, the following statements are logically equivalent\n",
    "  - For each $b$ in $R^m$, the equation $Ax = b$ has a solution\n",
    "  - Each $b$ in $R^m$ is a linear combination of the columns of $A$\n",
    "  - The columns of $A$ span $R^m$\n",
    "  - $A$ has a pivot position in every row\n",
    "  \n",
    "Theorem 5 : If $A$ is an $m \\times n$, $u$ and $v$ are vectors in $R^n$, and $c$ is a scalar, then: <br>\n",
    "- $A(u+v) = Au+Av$\n",
    "- $A(cu) = c(Au)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5320561a",
   "metadata": {},
   "source": [
    "### 5) Solution Sets of Linear Systems\n",
    "\n",
    "A system of linear euqations is homogeneous : it can be written in the form $Ax = 0$\n",
    "- trivial solution : $Ax = 0$ always has at least one solution, namely $x = 0$\n",
    "- nontrivial solution : nonzero vector $x$ that satisfies $Ax = 0$\n",
    "  - $Ax = 0$ has a nontrivial solution if and only if the equation has at least one free variable\n",
    "\n",
    "Let's describe all solutions of $Ax = b$, where $A = \\begin{bmatrix} 3 & 5 & -4 \\\\ -3 & -2 & 4 \\\\ 6 & 1 & -8 \\end{bmatrix}$ and $b = \\begin{bmatrix} 7 \\\\ -1 \\\\ -4 \\end{bmatrix}$\n",
    "- $\\begin{bmatrix} A & b \\end{bmatrix} = \\begin{bmatrix} 3 & 5 & -4 & 7 \\\\ -3 & -2 & 4 & -1 \\\\ 6 & 1 & -8 & -4 \\end{bmatrix} ~ \\begin{bmatrix} 1 & 0 & -4/3 & -1 \\\\ 0 & 1 & 0 & 2 \\\\ 0 & 0 & 0 & 0 \\end{bmatrix}$\n",
    "- therefore, $x = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix} = \\begin{bmatrix} -1 + (4/3)x_3 \\\\ 2 \\\\ x_3 \\end{bmatrix} = \\begin{bmatrix} -1 \\\\ 2 \\\\ 0 \\end{bmatrix} + \\begin{bmatrix} (4/3)x_3 \\\\ 0 \\\\ x_3 \\end{bmatrix} = \\begin{bmatrix} -1 \\\\ 2 \\\\ 0 \\end{bmatrix} + x_3 \\begin{bmatrix} 4/3 \\\\ 0 \\\\ 1 \\end{bmatrix}$\n",
    "- therefore, the solution set of $Ax = b$ in parametric vector form is $x = p + tv$ <br>\n",
    "  where $p = \\begin{bmatrix} -1 \\\\ 2 \\\\ 0 \\end{bmatrix}$, $v = \\begin{bmatrix} 4/3 \\\\ 0 \\\\ 1 \\end{bmatrix}$, and $t$ is in $R$, because $x_3$ is free variable\n",
    "- and, the solution set of $Ax = 0$ in parametric vector form is $x = tv$ <br>\n",
    "  because $p$ is the solution of $Ax = b$, and $v$ is the solution of $Ax = 0$\n",
    "- Theorem 6\n",
    "  - Suppose the equation $Ax = b$ is consistent for some given $b$, and let $p$ be a solution\n",
    "  - Then the solution set of $Ax = b$ is the set of all vectors of the form $w = p + v_h$, <br>\n",
    "    where v_h is any solution of the homogeneous equation $Ax = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec44ab28",
   "metadata": {},
   "source": [
    "### 6) Applications of Linear Systems\n",
    "\n",
    "![Network flow](Linear Algebra/network flow.png)\n",
    "\n",
    "$A : 30 + x_2 = 80 + x_1 \\Longrightarrow -x_1 + x_2 = 50$ <br>\n",
    "$B : x_3 + x_5 = x_2 + x_4 \\Longrightarrow -x_2 + x_3 - x_4 + x_5 = 0$ <br>\n",
    "$C : 100 + x_6 = 40 + x_5 \\Longrightarrow -x_5 + x_6 = -60$ <br>\n",
    "$D : 40 + x_4 = 90 + x_6 \\Longrightarrow x_4 - x_6 = 50$ <br>\n",
    "$E : 60 + x_1 = 20 + x_3 \\Longrightarrow x_1 - x_3 = -40$ <br>\n",
    "\n",
    "row reduce the augmented matrix of the linear system: <br>\n",
    "$\\begin{bmatrix}\n",
    "-1 & 1 & 0 & 0 & 0 & 0 & 50 \\\\\n",
    "0 & -1 & 1 & -1 & 1 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & -1 & 1 & -60 \\\\\n",
    "0 & 0 & 0 & 1 & 0 & -1 & 50 \\\\\n",
    "1 & 0 & -1 & 0 & 0 & 0 & -40\n",
    "\\end{bmatrix}$\n",
    "~\n",
    "$\\begin{bmatrix}\n",
    "1 & 0 & -1 & 0 & 0 & 0 & -40 \\\\\n",
    "0 & 1 & -1 & 0 & 0 & 0 & 10 \\\\\n",
    "0 & 0 & 0 & 1 & 0 & -1 & 50 \\\\\n",
    "0 & 0 & 0 & 0 & 1 & -1 & 60 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 & 0\n",
    "\\end{bmatrix}$\n",
    ", <br>\n",
    "$\\therefore x = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\\\ x_5 \\\\ x_6 \\end{bmatrix}\n",
    "= \\begin{bmatrix} x_3 - 40 \\\\ x_3 + 10 \\\\ x_3 \\\\ x_6 + 50 \\\\ x_6 + 60 \\\\ x_6 \\end{bmatrix}\n",
    "= \\begin{bmatrix} -40 \\\\ 10 \\\\ 0 \\\\ 50 \\\\ 60 \\\\ 0 \\end{bmatrix} + x_3 \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix} + x_6 \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 1 \\\\ 1 \\end{bmatrix}$, where $x_3 \\geq 40$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fa994f",
   "metadata": {},
   "source": [
    "### 7) Linear Independence\n",
    "\n",
    "An indexed set of vectors $\\{v_1,..,v_p\\}$ in $R^n$ is said to be\n",
    "- linearly independent : $x_1v_1 + x_2v_2 + \\cdots + x_pv_p = Ax = 0 \\ $ has only the trivial solution <br>\n",
    "- linearly dependent : there exist weights $c_1,...,c_p$, not all zero, such that $c_1v_1 + c_2v_2 + \\cdots + c_pv_p = 0$\n",
    "  - $c_1v_1 + c_2v_2 + \\cdots + c_pv_p = 0$ is called a linear dependence relation\n",
    "  \n",
    "Theorem 7 : Characterization of Linearly Dependent Sets\n",
    "- An indexed set $S = \\{v_1,...,v_p\\}$ of two or more vectors is linearly dependent if and only if <br>\n",
    "  at least one of the vectors in $S$ is a linear combination of the others.\n",
    "- if S is linearly dependent and $v_1 \\neq 0$, <br>\n",
    "  then some $v_j$ (with $j>1$) is a linear combination of the preceding vectors, $v_1,...,v_{j-1}$\n",
    "  \n",
    "Theorem 8 : If a set contains more vectors than there are entries in each vector, then the set is linearly dependent\n",
    "- any set $\\{v_1,...,v_p\\}$ in $R^n$ is linearly dependent if $p > n$\n",
    "\n",
    "Theorem 9 : If a set $S = \\{v_1,...,v_p\\}$ in $R^n$ contains the zero vector, then the set is linearly dependent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ec70c1",
   "metadata": {},
   "source": [
    "### 8) Introduction to Linear Transformations\n",
    "\n",
    "Transformation (function, mapping) $\\ T : V \\rightarrow W$ : a rule that assigns to each vector $x$ in $V$ a vector $T(x)$ in $W$\n",
    "- domain of $T$ : $V$\n",
    "- codomain of $T$ : $W$\n",
    "- image of $x$ : the vector $T(x)$\n",
    "- range of $T$ : the set of all images $T(x)$\n",
    "\n",
    "Linear Transformation : transformation $T$ that has the properties\n",
    "- $T(u+v) = T(u) + T(v)$ for all $u, v$ in the domain of $T$\n",
    "- $T(cu) = cT(u)$ for all scalars $c$ and all $u$ in the domain of $T$\n",
    "\n",
    "Matrix Transformation : $T(x) = Ax$ where $A$ is an $m \\times n$ matrix, denoted by $x \\mapsto Ax$\n",
    "- The range of $T$ is the set of all linear combinations of the columns of $A$\n",
    "- Every matrix transformation is linear transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55d6d22",
   "metadata": {},
   "source": [
    "### 9) The Matrix of a Linear Transformation\n",
    "\n",
    "Theorem 10\n",
    "- Let $T : R^n \\rightarrow R^m$ be a linear transformation. <br>\n",
    "  Then there exists a unique matrix $A$ such that $T(x) = Ax$ for all $x$ in $R^n$\n",
    "- $A$ is the $m \\times n$ matrix whose $j$th column is the vector $T(e_j)$, where $e_j$ is the $j$th column of the identity matrix in $R^n$: <br>\n",
    "  $A = \\begin{bmatrix} T(e_1) & \\cdots & T(e_n) \\end{bmatrix}$, called by the standard matrix for $T$\n",
    "- Warning : Every linear transformation is not matrix transformation\n",
    "  - $T$ is matrix transformation if $T$ is linear transformation from $R^n$ to $R^m$\n",
    "\n",
    "A mapping $T : R^n \\rightarrow R^m$ is said to be\n",
    "- onto : each $b$ in $R^m$ is the image of at **least** one $x$ in $R^n$\n",
    "  - if $T(x) = Ax$, then 'onto' is that $Ax = b$ has a solution for each $b$\n",
    "- one-to-one : each $b$ in $R^m$ is the image of at **most** one $x$ in $R^n$\n",
    "  - if $T(x) = Ax$, then 'one-to-one' is that $Ax = b$ has not a free variable\n",
    "\n",
    "Theorem 11 : Let $T : R^n \\rightarrow R^m$ be a linear transformation. Then:\n",
    "- $T$ is one-to-one if and only if the equation $T(x) = 0$ has only the trivial solution\n",
    "\n",
    "Theorem 12 : Let $T : R^n \\rightarrow R^m$ be a linear transformation, and let $A$ be the standard matrix for $T$. Then:\n",
    "- $T$ maps $R^n$ onto $R^m$ if and only if the columns of $A$ span $R^m$\n",
    "- $T$ is one-to-one if and only if the columns of $A$ are linearly independent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebce324",
   "metadata": {},
   "source": [
    "### 10) Linear Models in Business, Science, and Engineering\n",
    "\n",
    "In a certain region, about 7% of a city's population moves to the surrounding suburbs each year, <br>\n",
    "and about 5% of the suburban population moves into the city. <br>\n",
    "In 2020, there were 800,000 residents in the city and 500,000 in the suburbs. <br>\n",
    "Set up a difference equation that describes this situation, where $x_0$ is the initial population in 2020. <br>\n",
    "Then estimate the populations in the city and in the suburbs two years later, in 2022\n",
    "- $x_0 = \\begin{bmatrix} city \\\\ suburbs \\end{bmatrix} = \\begin{bmatrix} 800,000 \\\\ 500,000 \\end{bmatrix}$\n",
    "- difference equation (recurrence relation) : $x_{k+1} = Mx_k$ for $k = 0, 1, 2, ...$ \n",
    "  - migration matrix $M = \\begin{bmatrix} city \\rightarrow city & suburbs \\rightarrow city \\\\ city \\rightarrow suburbs & suburbs \\rightarrow subrubs \\end{bmatrix} = \\begin{bmatrix} 0.93 & 0.05 \\\\ 0.07 & 0.95 \\end{bmatrix}$\n",
    "- $\\therefore \\ x_2 = {\\begin{bmatrix} 0.93 & 0.05 \\\\ 0.07 & 0.95 \\end{bmatrix}}^2 x_0 = \\begin{bmatrix} 741,720 \\\\ 558,280 \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db483a1",
   "metadata": {},
   "source": [
    "*****"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdbefad",
   "metadata": {},
   "source": [
    "## Chapter 2 : Matrix Algebra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5fadac",
   "metadata": {},
   "source": [
    "### 1) Matrix Operations\n",
    "\n",
    "Theorem 1 : Let $A$, $B$, and $C$ be matrices of the same size, and let $r$ and $s$ be scalars\n",
    "- $A+B = B+A$\n",
    "- $(A+B)+C = A+(B+C)$\n",
    "- $A+0 = A$\n",
    "- $r(A+B) = rA + rB$\n",
    "- $(r+s)A = rA + sA$\n",
    "- $r(sA) = (rs)A$\n",
    "\n",
    "If $A$ is an $m \\times n$ matrix, and if $B$ is an $n \\times p$ matrix with columns $b_1,...,b_p$, <br>\n",
    "then the product $AB$ is the $m \\times p$ matrix whose columns are $Ab_1,...,Ab_p$. <br>\n",
    "that is, $AB = A \\begin{bmatrix} b_1 & b_2 & \\cdots & b_p \\end{bmatrix} = \\begin{bmatrix} Ab_1 & Ab_2 & \\cdots & Ab_p \\end{bmatrix}$\n",
    "- $(AB)_{ij} = a_{i1}b_{1j} + a_{i2}b_{2j} + \\cdots + a_{in}b_{nj}$\n",
    "\n",
    "Theorem 2 : Let $A$ be an $m \\times n$ matrix, and let $B$ and $C$ have sizes for which the indicated sums and products are defined\n",
    "- $A(BC) = (AB)C$  \n",
    "- $A(B+C) = AB+AC$\n",
    "- $(B+C)A = BA+CA$\n",
    "- $r(AB) = (rA)B = A(rB)$ for any scalar $r$\n",
    "- $I_m A = A = A I_n$\n",
    "\n",
    "Theorem 3 : Let $A$ and $B$ denote matrices whose sizes are appropriate for the following sums and products\n",
    "- $(A^T)^T = A$\n",
    "- $(A+B)^T = A^T + B^T$\n",
    "- $(rA)^T = r A^T$ for any scalar $r$\n",
    "- $(AB)^T = B^TA^T$\n",
    "  - Let $A = \\begin{bmatrix} a_1 \\\\ a_2 \\\\ \\vdots \\\\ a_m \\end{bmatrix}$ where $a_i$ is row vector in $R^n$, and $B = \\begin{bmatrix} b_1 & b_2 & \\cdots & b_p \\end{bmatrix}$ where $b_j$ is column vector in $R^n$ \n",
    "  - $(AB)^T = {\\begin{bmatrix} a_1b_1 & a_1b_2 & \\cdots & a_1b_p \\\\\n",
    "                               a_2b_1 & a_2b_2 & \\cdots & a_2b_p \\\\\n",
    "                               \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "                               a_mb_1 & a_mb_2 & \\cdots & a_mb_p \\end{bmatrix}}^T\n",
    "             = \\begin{bmatrix} a_1b_1 & a_2b_1 & \\cdots & a_mb_1 \\\\\n",
    "                               a_1b_2 & a_2b_2 & \\cdots & a_mb_2 \\\\\n",
    "                               \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "                               a_1b_p & a_2b_p & \\cdots & a_mb_p \\end{bmatrix}$\n",
    "  - $B^TA^T = \\begin{bmatrix} b_1 \\\\ b_2 \\\\ \\vdots \\\\ b_p \\end{bmatrix} \\begin{bmatrix} a_1 & a_2 & \\cdots & a_m \\end{bmatrix}\n",
    "             = \\begin{bmatrix} a_1b_1 & a_2b_1 & \\cdots & a_mb_1 \\\\\n",
    "                               a_1b_2 & a_2b_2 & \\cdots & a_mb_2 \\\\\n",
    "                               \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "                               a_1b_p & a_2b_p & \\cdots & a_mb_p \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083d0592",
   "metadata": {},
   "source": [
    "### 2) The Inverse of a Matrix\n",
    "\n",
    "Theorem 4 : Let $A = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}$\n",
    "- If $ad - bc \\neq 0$, then $A$ is invertible and $A^{-1} = \\frac{1}{ad-bc} \\begin{bmatrix} d & -b \\\\ -c & a \\end{bmatrix}$\n",
    "- If $ad - bc = 0$, then $A$ is not invertible\n",
    "\n",
    "Theorem 5 : If $A$ is invertible $n \\times n$ matrix, then the equation $Ax = b$ has the unique solution $x = A^{-1}b$ for each $b$ in $R^n$\n",
    "\n",
    "Theorem 6\n",
    "- if $A$ is an invertible matrix, then $A^{-1}$ is invertible and $(A^{-1})^{-1} = A$\n",
    "- If $A$ and $B$ are $n \\times n$ invertible matrices, then so is $AB$, and $(AB)^{-1} = B^{-1}A^{-1}$\n",
    "  - Conversely, If $AB$ is invertible, then there exists $C$ such that $(AB)C = I$ and $C(AB) = I$ <br>\n",
    "    $\\Rightarrow A(BC) = I$ and $(CA)B = I$. Therefore by The Invetible Matrix Theorem, $A$ and $B$ is invertible <br>\n",
    "    and $(AB)^{-1} AB = I \\Rightarrow (AB)^{-1} = B^{-1}A^{-1}$ \n",
    "- If $A$ is an invertible matrix, then so is $A^T$, and $(A^T)^{-1} = (A^{-1})^T$\n",
    "  - $(A^{-1})^T A^T = (A A^{-1})^T = I$ and $(A^T)^{-1} A^T = I$ $ \\ \\Longrightarrow \\ (A^T)^{-1} = (A^{-1})^T$\n",
    "  \n",
    "elementary matrix : one that is obtained by performing a single elementary row operation on an identity matrix\n",
    "- since row operations are reversible, each elementary matrix $E$ is invertible\n",
    "\n",
    "Theorem 7\n",
    "- An $n \\times n$ matrix $A$ is invertible if and only if $A$ is row equivalent to $I_n$\n",
    "- any sequence of elementary row operations that reduces $A$ to $I_n$ also transforms $I_n$ into $A^{-1}$\n",
    "  - if $A$ is invertible, then $Ax = b$ has a solution for each $b$ (Theorem 5) <br>\n",
    "    so that $A$ has a pivot position in every row (Theorem 4 in Chapter 1)\n",
    "  - and since $A$ is square matrix, the reduced echelon form of $A$ is $I_n$ <br>\n",
    "    so that $A$ ~ $E_1A$ ~ $E_2(E_1A)$ ~ $\\cdots$ ~ $E_p(E_{p-1} \\cdots E_1A) = E_p \\cdots E_1A = I_n$ <br>\n",
    "    $\\Longrightarrow A = (E_p \\cdots E_1)^{-1} I_n = (E_p \\cdots E_1)^{-1} \\Longrightarrow A^{-1} = E_p \\cdots E_1 = E_p \\cdots E_1 I_n$\n",
    "  - $\\therefore E_p \\cdots E_1$ transform not only $A$ to $I_n$, but also $I_n$ into $A^{-1}$ <br>\n",
    "    so that $\\begin{bmatrix} A & I \\end{bmatrix}$ ~ $\\begin{bmatrix} I & A^{-1} \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb1170f",
   "metadata": {},
   "source": [
    "### 3) Characterizations of Invertible Matrices\n",
    "\n",
    "Theorem 8 ( The Invertible Matrix Theorem ) : Let $A$ be a square $n \\times n$ matrix. Then the following statements are equivalent.\n",
    "1. $A$ is an invertible matrix\n",
    "2. $A$ is row equivalent to the $n \\times n$ identity matrix \n",
    "3. $A$ has $n$ pivot positions \n",
    "4. The equation $Ax = 0$ has only the trivial solution\n",
    "5. The columns of $A$ form a linaerly independent set\n",
    "6. The linear transformation $x \\mapsto Ax$ is one-to-one\n",
    "7. The equation $Ax = b$ has at least one solution for each $b$ in $R^n$\n",
    "8. The columns of $A$ span $R^n$\n",
    "9. The linear transformation $x \\mapsto Ax$ maps $R^n$ onto $R^n$\n",
    "10. There is an $n \\times n$ matrix $C$ such that $CA = I$\n",
    "11. There is an $n \\times n$ matrix $D$ such that $AD = I$\n",
    "12. $A^T$ is an invertible matrix\n",
    "- 1 $\\Rightarrow$ 10 $\\Rightarrow$ 4 $\\Rightarrow$ 3 $\\Rightarrow$ 2 $\\Rightarrow$ 1 | 1 $\\Rightarrow$ 11 $\\Rightarrow$ 7 $\\Rightarrow$ 1 | (4, 5, 6) | (7, 8, 9) | (1, 12)\n",
    "\n",
    "Theorem 9 : Let $T : R^n \\rightarrow R^n$ be a linear transformation and let $A$ be the standard matrix for $T$. then:\n",
    "- $T$ is invertible if and only if $A$ is an invertible matrix.\n",
    "- the linear transformation $S$ given by $S(x) = A^{-1}x$ is the unique function satisfying <br>\n",
    "  $S(T(x)) = x$, $T(S(x)) = x$ for all $x$ in $R^n$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73abd1b1",
   "metadata": {},
   "source": [
    "### 4) Partitioned Matrices\n",
    "\n",
    "Theorem 10 : Column-Row Expansion of $AB$\n",
    "- If $A$ is $m \\times n$ and $B$ is $n \\times p$, then <br>\n",
    "  $AB = \\begin{bmatrix} col_{1}(A) & col_{2}(A) & \\cdots & col_{n}(A) \\end{bmatrix} \\begin{bmatrix} row_{1}(B) \\\\ row_{2}(B) \\\\ \\vdots \\\\ row_{n}(B) \\end{bmatrix}$ <br>\n",
    "  $= col_{1}(A) \\ row_{1}(B) + \\cdots + col_{n}(A) \\ row_{n}(B)$\n",
    "\n",
    "Let's find a formula for $A^{-1}$ where $A$ is block upper triangular matrix $\\begin{bmatrix} A_{11} & A_{12} \\\\ 0 & A_{22} \\end{bmatrix}$ and $A_{11}$ is $p \\times p$, $A_{22}$ is $q \\times q$\n",
    "- Denote $A^{-1}$ by $B$ and partition $B$ so that $\\begin{bmatrix} A_{11} & A_{12} \\\\ 0 & A_{22} \\end{bmatrix} \\begin{bmatrix} B_{11} & B_{12} \\\\ B_{21} & B_{22} \\end{bmatrix} = \\begin{bmatrix} I_{p} & 0 \\\\ 0 & I_q \\end{bmatrix}$\n",
    "- then, $A_{11}B_{11} + A_{12}B_{21} = I_p$ | $A_{11}B_{12} + A_{12}B_{22} = 0$ | $A_{22}B_{21} = 0$ | $A_{22}B_{22} = I_q$\n",
    "- since $A_{22}$ and $A_{11}$ is sqaure, by the Invertible Matrix Theorem, <br>\n",
    "  $A^{-1} = \\begin{bmatrix} B_{11} & B_{12} \\\\ B_{21} & B_{22} \\end{bmatrix} = \\begin{bmatrix} {A_{11}^{-1}} & -{A_{11}^{-1}}A_{12}{A_{22}^{-1}} \\\\ 0 & {A_{22}^{-1}} \\end{bmatrix}$\n",
    "  \n",
    "Schur complement : Let $M = \\begin{bmatrix} A & B \\\\ C & D \\end{bmatrix}$ where $A$ is $p \\times p$, $B$ is $p \\times q$, $C$ is $q \\times p$, and $D$ is $q \\times q$\n",
    "- If $D$ is invertible, then the Schur complement of the block $D$ of $M$ is $M/D = A - BD^{-1}C$, <br>\n",
    "  which is derived by $\\begin{bmatrix} A & B \\\\ C & D \\end{bmatrix} \\begin{bmatrix} I_p & 0 \\\\ -D^{-1} C & I_q \\end{bmatrix} = \\begin{bmatrix} A - BD^{-1}C & B \\\\ 0 & D \\end{bmatrix}$\n",
    "- If $A$ is invertible, then the Schur complement of the block $A$ of $M$ is $M/A = D - CA^{-1}B$, <br>\n",
    "  which is derived by $\\begin{bmatrix} A & B \\\\ C & D \\end{bmatrix} \\begin{bmatrix} I_p & -A^{-1}B \\\\ 0 & I_q \\end{bmatrix} = \\begin{bmatrix} A & 0 \\\\ C & D - CA^{-1}B \\end{bmatrix}$\n",
    "- LDU decomposition : $A = LDU$ where $L$ is lower, $D$ is diagonal, and $U$ is upper\n",
    "  - If $D$ is invertible, then $M = \\begin{bmatrix} A & B \\\\ C & D \\end{bmatrix} = \\begin{bmatrix} I_p & BD^{-1} \\\\ 0 & I_q \\end{bmatrix} \\begin{bmatrix} A - BD^{-1}C & 0 \\\\ 0 & D \\end{bmatrix} \\begin{bmatrix} I_p & 0 \\\\ D^{-1}C & I_q \\end{bmatrix}$\n",
    "  - if $A$ is invertible, then $M = \\begin{bmatrix} A & B \\\\ C & D \\end{bmatrix} = \\begin{bmatrix} I_p & 0 \\\\ CA^{-1} & I_q \\end{bmatrix} \\begin{bmatrix} A & 0 \\\\ 0 & D - CA^{-1}B \\end{bmatrix} \\begin{bmatrix} I_p & A^{-1}B \\\\ 0 & I_q \\end{bmatrix}$\n",
    "  - we can check $\\begin{bmatrix} I_p & -BD^{-1} \\\\ 0 & I_q \\end{bmatrix} \\begin{bmatrix} A & B \\\\ C & D \\end{bmatrix} \\begin{bmatrix} I_p & 0 \\\\ -D^{-1} C & I_q \\end{bmatrix}$ <br>\n",
    "    $= \\begin{bmatrix} I_p & -BD^{-1} \\\\ 0 & I_q \\end{bmatrix} \\begin{bmatrix} A - BD^{-1}C & B \\\\ 0 & D \\end{bmatrix} = \\begin{bmatrix} A - BD^{-1}C & 0 \\\\ 0 & D \\end{bmatrix}$ <br>\n",
    "    and the inverse of $\\begin{bmatrix} I_p & -BD^{-1} \\\\ 0 & I_q \\end{bmatrix}$ is $\\begin{bmatrix} I_p & BD^{-1} \\\\ 0 & I_q \\end{bmatrix}$ <br>\n",
    "    and the inverse of $\\begin{bmatrix} I_p & 0 \\\\ -D^{-1} C & I_q \\end{bmatrix}$ is $\\begin{bmatrix} I_p & 0 \\\\ D^{-1} C & I_q \\end{bmatrix}$\n",
    "  - Therefore, $\\begin{bmatrix} A & B \\\\ C & D \\end{bmatrix} = \\begin{bmatrix} I_p & BD^{-1} \\\\ 0 & I_q \\end{bmatrix} \\begin{bmatrix} I_p & -BD^{-1} \\\\ 0 & I_q \\end{bmatrix} \\begin{bmatrix} A & B \\\\ C & D \\end{bmatrix} \\begin{bmatrix} I_p & 0 \\\\ -D^{-1} C & I_q \\end{bmatrix} \\begin{bmatrix} I_p & 0 \\\\ D^{-1}C & I_q \\end{bmatrix}$ <br>\n",
    "    $= \\begin{bmatrix} I_p & BD^{-1} \\\\ 0 & I_q \\end{bmatrix} \\begin{bmatrix} A - BD^{-1}C & 0 \\\\ 0 & D \\end{bmatrix} \\begin{bmatrix} I_p & 0 \\\\ D^{-1}C & I_q \\end{bmatrix}$, and similarly we can prove when $A$ is invertible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2418a05f",
   "metadata": {},
   "source": [
    "### 5) Matrix Factorizations\n",
    "\n",
    "( above LDU (lower-diagonal-upper) decomposition is one of the matrix factorizations ) <br>\n",
    "\n",
    "LU Factorization : Suppose $A$ is $m \\times n$ matrix\n",
    "- Algorithm : Suppose $U$ is echelon form of $A$, then $E_p \\cdots E_1 A = U$ so that <br>\n",
    "  $A = (E_p \\cdots E_1)^{-1} U = LU \\ \\ $ [ so $L$ is $m \\times m$ ] \n",
    "- usefulness : $Ax = b$ can be written as $L(Ux) = b$ where $L$ is lower and $U$ is upper, so that <br>\n",
    "  we can easily find $x$ by solving the pair of equations $Ly = b$, $Ux = y$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ed330f",
   "metadata": {},
   "source": [
    "### 6) The Leontief Input-Output Model\n",
    "\n",
    "Theorem 11 (for the Leontief Input-Output Model)\n",
    "- Let $C$ be the consumption matrix for an economy, and let $d$ be the final demand.\n",
    "- If $C$ and $d$ have nonnegative entries and if each column sum of $C$ is less than $1$, <br>\n",
    "  then $(I-C)^{-1}$ exists and the production vector $x = (I-C)^{-1}d$ <br>\n",
    "  has nonnegative entries and is the unique solution of $x = Cx + d$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece0801e",
   "metadata": {},
   "source": [
    "### 7) Applications to Computer Graphics\n",
    "\n",
    "homogeneous coordinates for $(x_1,...,x_n)$ is $(X_1,...,X_n,H)$ where $x_1 = \\frac{X_1}{H}, ..., x_n = \\frac{X_n}{H}$\n",
    "- if $H = 0$, then homogeneous coordinates become vector\n",
    "- scaling by c : $\\begin{bmatrix} c & 0 & 0 & 0 \\\\ 0 & c & 0 & 0 \\\\ 0 & 0 & c & 0 \\\\ 0 & 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ z \\\\ 1\\end{bmatrix}$\n",
    "- rotation about the y-axis through an angle of $\\varphi$ : $\\begin{bmatrix} \\sin \\varphi & 0 & sin (\\varphi+\\pi/2) & 0 \\\\ 0 & 1 & 0 & 0 \\\\ \\cos \\varphi & 0 & cos (\\varphi+\\pi/2) & 0 \\\\ 0 & 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ z \\\\ 1\\end{bmatrix}$\n",
    "- translate that adds $(a, b, c)$ to each point of figure : $\\begin{bmatrix} 1 & 0 & 0 & a \\\\ 0 & 1 & 0 & b \\\\ 0 & 0 & 1 & c \\\\ 0 & 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ z \\\\ 1\\end{bmatrix}$\n",
    "- perspective projection : maps each point $(x, y, z)$ onto an image point $(x^*, y^*, 0)$ when the eye of a viewer is $(0, 0, d)$ <br>\n",
    "  $= (x, y, z, 1)$ to map to $(\\frac{x}{1 - z/d}, \\frac{y}{1 - z/d}, 0, 1)$ <br>\n",
    "  $= (x, y, z, 1)$ to map to $(x, y, 0, 1-z/d)$ : $\\begin{bmatrix} 1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\\\ 0 & 0 & -1/d & 1 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ z \\\\ 1\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a906c2f",
   "metadata": {},
   "source": [
    "### 8) Subspaces of $R^n$\n",
    "\n",
    "subspace of $R^n$ : any set $H$ in $R^n$ that has three properties\n",
    "- The zero vector is in $H$\n",
    "- For each $u$ and $v$ in $H$, the sum $u+v$ is in $H$\n",
    "- For each $u$ in $H$ and each scalar $c$, the vector $cu$ is in $H$\n",
    "\n",
    "column space of $A$ : the set Col $A$ of all linear combinations of the columns of $A$ <br>\n",
    "null space of $A$ : the set Nul $A$ of all solutions of the homogeneous equation $Ax = 0$\n",
    "\n",
    "Theroem 12 : The null space of an $m \\times n$ matrix $A$ is a subspace of $R^n$\n",
    "- $A0 = 0$\n",
    "- Suppose $u$ and $v$ is in $A$. Then $A(u+v) = Au + Av = 0$. Thus $u+v$ is in Nul $A$\n",
    "- Suppose $u$ is in $A$ and $c$ is scalar. Then $A(cu) = c(Au) = c(0) = 0$. Thus $cu$ is in Nul $A$\n",
    "\n",
    "basis for a subspace $H$ of $R^n$ : a linearly independent set in $H$ that spans $H$\n",
    "- for $e_1 = \\begin{bmatrix} 1 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix}$, $e_2 = \\begin{bmatrix} 0 \\\\ 1 \\\\ \\vdots \\\\ 0 \\end{bmatrix}$, ..., $e_n = \\begin{bmatrix} 0 \\\\ \\vdots \\\\ 0 \\\\ 1 \\end{bmatrix}$, the set $\\{e_1,...,e_n\\}$ is called the standard basis for $R^n$\n",
    "- the solution set of $Ax = 0$ in parametric vector form actually identifies a basis for Nul $A$\n",
    "\n",
    "Theorem 13 : The pivot columns of a matrix $A$ form a basis for the column space of $A$\n",
    "- Warning : The pivot columns of echelon form of $A$ are not in the column space of $A$\n",
    "  - It's true that the pivot columns of echelon form of $A$ can make other columns of echelon form of $A$ <br>\n",
    "    and row operations do not affect linear dependence relations, <br>\n",
    "    so that the pivot columns of $A$ can make other columns of $A$\n",
    "  - but the pivot columns of echelon form of $A$ cannot make the columns of $A$ <br>\n",
    "    if the last row of echelon form of $A$ is zero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb15da1",
   "metadata": {},
   "source": [
    "### 9) Dimension and Rank\n",
    "Suppose the set $B = \\{b_1, ..., b_p\\}$ is a basis for a subspace $H$\n",
    "- For each $x$ in $H$, the coordinates of $x$ relative to the basis $B$ are the weights $c_1,...,c_p$ such that $x = c_1b_1 + ... + c_pb_p$\n",
    "- the vector $[x]_\\mathcal{B} = \\begin{bmatrix} c_1 \\\\ \\vdots \\\\ c_p \\end{bmatrix}$ is called the coordinate vector of $x$ (relative to $\\mathcal{B}$) or the $\\mathcal{B}$-coordinate vector of $x$\n",
    "\n",
    "The dimension of a nonzero subspace $H$ : the number of vectors in any basis for $H$, denoted by dim $H$\n",
    "- The dimension of the zero subspace $\\{0\\}$ is defined to be zero\n",
    "\n",
    "Theorem 14 : The Rank Theorem\n",
    "- If a matrix $A$ has $n$ columns, then rank $A$ + dim Nul $A$ $= n$\n",
    "  - The rank of a matrix $A$ : dim Col $A$, denoted by rank $A$\n",
    "  \n",
    "Theorem 15 : The Basis Theorem\n",
    "- Let $H$ be a $p$-dimensional subspace of $R^n$\n",
    "- Any linearly independent set of exactly $p$ elements in $H$ is automatically a basis for $H$.\n",
    "- Also, any set of $p$ elements of $H$ that spans $H$ is automatically a basis for $H$\n",
    "\n",
    "The Invertible Matrix Theorem (continued) : Let $A$ be a square $n \\times n$ matrix. Then the following statements are equivalent.\n",
    "1. $A$ is an invertible matrix\n",
    "2. $A$ is row equivalent to the $n \\times n$ identity matrix \n",
    "3. $A$ has $n$ pivot positions \n",
    "4. The equation $Ax = 0$ has only the trivial solution\n",
    "5. The columns of $A$ form a linaerly independent set\n",
    "6. The linear transformation $x \\mapsto Ax$ is one-to-one\n",
    "7. The equation $Ax = b$ has at least one solution for each $b$ in $R^n$\n",
    "8. The columns of $A$ span $R^n$\n",
    "9. The linear transformation $x \\mapsto Ax$ maps $R^n$ onto $R^n$\n",
    "10. There is an $n \\times n$ matrix $C$ such that $CA = I$\n",
    "11. There is an $n \\times n$ matrix $D$ such that $AD = I$\n",
    "12. $A^T$ is an invertible matrix\n",
    "13. The columns of $A$ form a basis of $R^n$\n",
    "14. Col $A = R^n$\n",
    "15. rank $A = n$\n",
    "16. dim Nul $A = 0$\n",
    "17. Nul $A = \\{0\\}$\n",
    "- 1 $\\Rightarrow$ 10 $\\Rightarrow$ 4 $\\Rightarrow$ 3 $\\Rightarrow$ 2 $\\Rightarrow$ 1 | 1 $\\Rightarrow$ 11 $\\Rightarrow$ 7 $\\Rightarrow$ 1 | (4, 5, 6) | (7, 8, 9) | (1, 12)\n",
    "- 5 and 8 $\\Rightarrow$ 13 | 7 $\\Rightarrow$ 14 $\\Rightarrow$ 15 $\\Rightarrow$ 16 $\\Rightarrow$ 17 $\\Rightarrow$ 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a54112",
   "metadata": {},
   "source": [
    "*****"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8334527d",
   "metadata": {},
   "source": [
    "## Chapter 3 : Determinants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2655ebd6",
   "metadata": {},
   "source": [
    "### 1) Introduction to Determinants\n",
    "\n",
    "For $n \\geq 2$, the determinant of an $n \\times n$ matrix $A = [a_{ij}]$ is $\\det A = |A| = \\sum_{j=1}^{n} (-1)^{1+j} a_{1j} \\det A_{1j}$ <br> where $A_{ij}$ is the submatrix formed by deleting the $i$th row and $j$th column of $A$ <br>\n",
    "- $(i, j)$-cofactor of $A$ : $C_{ij} = (-1)^{i+j} \\det A_{ij}$\n",
    "- Definition of determinant is called a cofactor expansion across the first row\n",
    "\n",
    "Theorem 1 : $\\det A$ can be computed by a cofactor expansion across any row or down any column\n",
    "- $\\det A = a_{i1}C_{i1} + a_{i2}C_{i2} + \\cdots + a_{in}C_{in}$\n",
    "- $\\det A = a_{1j}C_{1j} + a_{2j}C_{2j} + \\cdots + a_{nj}C_{nj}$\n",
    "\n",
    "Theorem 2 : If $A$ is a triangular matrix, then det $A$ is the product of the entries on the main diagonal of $A$ <br>\n",
    "\n",
    "Let $M$ be a partitioned matrix of the form $M = \\begin{bmatrix} A & B \\\\ C & D \\end{bmatrix}$ where $A$ and $D$ are square matrices\n",
    "- If $A$ is invertible, then $\\det (M) = \\det (A) \\det (D - C A^{-1} B)$, and <br>\n",
    "- If $D$ is invertible, then $\\det (M) = \\det (A - B D^{-1} C) \\det (D) \\ \\ $ <br>\n",
    "- proof : (link)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41d6a34",
   "metadata": {},
   "source": [
    "### 2) Properties of Determinants\n",
    "\n",
    "Theorem 3 : Let $A$ be a square matrix. Then:\n",
    "- If a multiple of one row of $A$ is added to another row to produce a matrix $B$, then $\\det B = \\det A$\n",
    "- If two rows of $A$ are interchanged to produce $B$, then $\\det B = - \\det A$\n",
    "- If one row of $A$ is multiplied by k to produce $B$, then $\\det B = k \\det A$\n",
    "\n",
    "Theorem 3 proof\n",
    "- Theorem 3 can be reformulated as follow: <br>\n",
    "  If $A$ is an $n \\times n$ matrix and $E$ is an $n \\times n$ elementary matrix, <br>\n",
    "  then $\\det EA = (\\det E)(\\det A)$ where $\\det E = 1$ (replace), $-1$ (interchange), $r$ (scale)\n",
    "- If $A$ is $2 \\times 2$ matrix $\\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}$, we can verify Theorem 3 is true\n",
    "  - $\\begin{bmatrix} a & b \\\\ c + a & d + b \\end{bmatrix} = a(d+b) - b(c+a) = ad - bc = \\det A$\n",
    "  - $\\begin{bmatrix} c & d \\\\ a & b \\end{bmatrix} = bc - ad = -(ad - bc) - \\det A$\n",
    "  - $\\begin{bmatrix} na & nb \\\\ c & d \\end{bmatrix} = nad - nbc = n(ad - bc) = n \\det A$\n",
    "- Suppose the theorem is true when $A$ is $k \\times k$ matrix with $k \\geq 2$\n",
    "- Let $n = k + 1$, and Let $A$ be $n \\times n$ and $B = EA$ where $E$ is elementary matrix\n",
    "- The action of $E$ on $A$ involves either two rows or only one row. <br>\n",
    "  so we can expand $\\det EA$ across a row that is unchanged by the action of $E$, say, row $i$\n",
    "- since $A_{ij}, B_{ij}$ is $k \\times k$, the induction assumption implies that $\\det B_{ij} = \\alpha \\det A_{ij}$ <br>\n",
    "  where $\\alpha = 1, -1, r \\ $ depending on the nature of $E$\n",
    "- $\\therefore \\det EA = \\det B = a_{i1}(-1)^{i+1} \\det B_{i1} + \\cdots + a_{in}(-1)^{i+n} \\det B_{in}$ <br>\n",
    "  $= \\alpha a_{i1}(-1)^{i+1} \\det A_{i1} + \\cdots + \\alpha a_{in}(-1)^{i+n} \\det A_{in} = \\alpha \\det A$\n",
    "- ( In addition, $A^T$ show that elementary column operations have same results as elementary row operations )\n",
    "\n",
    "Theorem 4 : $n \\times n$ matrix $A$ is invertible if and only if $\\det A \\neq 0$\n",
    "- If $A$ is invertible, then $A$ is row equivalent to $I_n$, therefore $\\det A = \\det E_1 \\cdots \\det E_p \\det I_n \\neq 0$\n",
    "- 'If $\\det A \\neq 0$, then $A$ is invertible' $\\Rightarrow$ 'If $A$ is not invertible, then $\\det A = 0$' <br>\n",
    "  and if $A$ is not invertible, then echelon form of $A$ has at least one zero row, therefore $\\det A = 0$\n",
    "\n",
    "Theorem 5 : If $A$ is an $n \\times n$ matrix, then $\\det A^T = \\det A \\ \\ $ ( by Theorem 1 ) <br>\n",
    "\n",
    "Theorem 6 : If $A$ and $B$ are $n \\times n$, then $\\det AB = (\\det A)(\\det B)$\n",
    "- If $AB$ is invertible, then there is $C$ such that $(AB)C = I$ $\\Rightarrow A(BC) = I$, which implies $A$ is invertible <br>\n",
    "  and by contraposition, If $A$ is not invertible, then $AB$ is not invertible. <br>\n",
    "  Therefore, If $A$ is not invertible, then $\\det AB = (\\det A)(\\det B) \\Rightarrow 0 = 0 \\Rightarrow T$\n",
    "- If $A$ is intvertible, then $A$ and $I_n$ are row equivalent so $A = E_p E_{p-1} \\cdots E_1 I_n = E_p E_{p-1} \\cdots E_1$ <br>\n",
    "  Therefore, $|AB| = |E_p \\cdots E_1 B| = |E_p| |E_{p-1} \\cdots E_1 B| = \\cdots = |E_p| \\cdots |E_1| |B| = |E_p \\cdots E_1| |B| = |A| |B|$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6328e6",
   "metadata": {},
   "source": [
    "### 3) Cramer's Rule, Volume, and Linear Transformations\n",
    "\n",
    "For any $n \\times n$ matrix $A$ and any $b$ in $R^n$, let $A_i(b) = \\begin{bmatrix} a_1 & \\cdots & b & \\cdots & a_n \\end{bmatrix}$ <br>\n",
    "Theorem 7 : Cramer's Rule\n",
    "- Let $A$ be an invertible $n \\times n$ matrix. For any $b$ in $R^n$, the unique solution $x$ of $Ax = b$ <br>\n",
    "  has entries given by $x_i = \\frac{\\det A_i(b)}{\\det A}, \\ \\ \\ i = 1, 2, ..., n$\n",
    "- proof : Denote the columns of $A$ by $a_1, ..., a_n$ and the columns of the $n \\times n$ indentity matrix $I$ by $e_1, ..., e_n$\n",
    "  - If $Ax = b$, then $A(I_i(x)) = A \\begin{bmatrix} e_1 & \\cdots & x & \\cdots & e_n \\end{bmatrix}$ <br>\n",
    "    $= \\begin{bmatrix} A e_1 & \\cdots & A x & \\cdots & A e_n \\end{bmatrix} = \\begin{bmatrix} a_1 & \\cdots & b & \\cdots & a_n \\end{bmatrix} = A_i(b)$\n",
    "  - so $\\det A(I_i(x)) = (\\det A)(\\det I_i(x)) = \\det A_i(b)$\n",
    "  - and $\\det I_i(x) = \\begin{vmatrix} 1 & 0 & \\cdots & 0 & x_1 & 0 & \\cdots & 0 & 0 \\\\\n",
    "                                       0 & 1 & \\cdots & 0 & x_2 & 0 & \\cdots & 0 & 0 \\\\\n",
    "                                       \\vdots & \\vdots & \\ddots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n",
    "                                       0 & 0 & \\cdots & 1 & x_{i-1} & 0 & \\cdots & 0 & 0 \\\\\n",
    "                                       0 & 0 & \\cdots & 0 & x_i & 0 & \\cdots & 0 & 0 \\\\\n",
    "                                       0 & 0 & \\cdots & 0 & x_{i+1} & 1 & \\cdots & 0 & 0 \\\\\n",
    "                                       \\vdots & \\vdots & \\ddots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n",
    "                                       0 & 0 & \\cdots & 0 & x_{n-1} & 0 & \\cdots & 1 & 0 \\\\\n",
    "                                       0 & 0 & \\cdots & 0 & x_n & 0 & \\cdots & 0 & 1 \\end{vmatrix} = x_i$\n",
    "  - therefore, $(\\det A) x_i = \\det A_i(b) \\Rightarrow x_i = \\frac{\\det A_i(b)}{\\det A}$\n",
    "\n",
    "Theorem 8 : An Inverse Formula\n",
    "- Let $A$ be an invertible $n \\times n$ matrix.\n",
    "- The $j$th column of $A^{-1}$ is a vector $x$ that satisfies $Ax = e_j$ <br>\n",
    "  and by Cramer's rule, $x_i = \\frac{\\det A_i(e_j)}{\\det A}$, which is $(i, j)$-entry of $A^{-1}$\n",
    "- and cofactor expansion down column $i$ of $A_i(e_j)$ shows that $\\det A_i(e_j) = (-1)^{i+j} \\det A_{ji} = C_{ji}$\n",
    "- therefore, $A^{-1} = \\frac{1}{\\det A} \\begin{bmatrix} C_{11} & C_{21} & \\cdots & C_{n1} \\\\\n",
    "                                                        C_{12} & C_{22} & \\cdots & C_{n2} \\\\\n",
    "                                                        \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "                                                        C_{1n} & C_{2n} & \\cdots & C_{nn} \\end{bmatrix}$, <br>\n",
    "  where the matrix of cofactors is called the adjugate (or classical adjoint) of $A$, denoted by adj $A$\n",
    "  \n",
    "Theorem 9\n",
    "- If $A$ is a $2 \\times 2$ matrix, the area of the parallelogram determined by the columns of $A$ is $\\begin{vmatrix} \\det A \\end{vmatrix}$\n",
    "- If $A$ is a $3 \\times 3$ matrix, the volume of the parallelepiped determined by the columns of $A$ is $\\begin{vmatrix} \\det A \\end{vmatrix}$\n",
    "\n",
    "Theorem 10\n",
    "- Let $T : R^2 \\rightarrow R^2$ be the linear transformation determined by a $2 \\times 2$ matrix $A$\n",
    "- If $S$ is a parallelogram in $R^2$, then { area of $T(S)$ } $= \\begin{vmatrix} \\det A \\end{vmatrix} \\cdot$ { area of $S$ }\n",
    "  - parallelogram at the origin in $R^2$ determined by the columns of the matrix $B = \\begin{bmatrix} b_1 & b_2 \\end{bmatrix}$ <br>\n",
    "    has the form $S = \\{s_1b_1 + s_2b_2 : 0 \\le s_1 \\le 1, 0 \\le s_2 \\le 1\\}$ \n",
    "  - The image of $S$ under $T$ consists of points of the form <br>\n",
    "    $T(s_1b_1 + s_2b_2) = s_1T(b_1) + s_2T(b_2) = s_1Ab_1 + s_2Ab_2$ where $0 \\le s_1 \\le 1, 0 \\le s_2 \\le 1$, <br>\n",
    "    which follows that $T(S)$ is the parallelogram determined by the columns of the matrix $\\begin{bmatrix} Ab_1 & Ab_2 \\end{bmatrix}$\n",
    "  - Therefore, { area of $T(S)$ } $= \\begin{vmatrix} \\det AB \\end{vmatrix} = \\begin{vmatrix} \\det A \\end{vmatrix} \\begin{vmatrix} \\det B \\end{vmatrix} = \\begin{vmatrix} \\det A \\end{vmatrix} \\cdot$ { area of $S$ }\n",
    "- If $T$ is determined by a $3 \\times 3$ matrix $A$, and if $S$ is a parallelepiped in $R^3$, <br>\n",
    "  then { volume of $T(S)$ } $= \\begin{vmatrix} \\det A \\end{vmatrix} \\cdot$ { volume of $S$ }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0921f1",
   "metadata": {},
   "source": [
    "*****"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2294494e",
   "metadata": {},
   "source": [
    "## Chapter 4 : Vector Spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ad1f75",
   "metadata": {},
   "source": [
    "### 1) Vector Spaces and Subspaces\n",
    "\n",
    "vector space : nonempty set $V$ of objects, called vectors\n",
    "- on which are defined two operations, called addition and multiplication by scalars\n",
    "- subject to the ten axioms. The axioms must hold for all vectors $u$, $v$, and $w$ in $V$ and for all scalars $c$ and $d$:\n",
    "  1. The sum of $u$ and $v$, denoted by $u+v$, is in $V$\n",
    "  2. $u+v = v+u$\n",
    "  3. $(u+v)+w = u+(v+w)$\n",
    "  4. There is a zero vector $0$ in $V$ such that $u+0 = u$\n",
    "  5. For each $u$ in $V$, there is a vector $-u$ in $V$ such that $u + (-u) = 0$\n",
    "  6. The scalar multiple of $u$ by $c$, denoted by $cu$, is in $V$\n",
    "  7. $c(u+v) = cu + cv$\n",
    "  8. $(c+d)u = cv + du$\n",
    "  9. $c(du) = (cd)u$\n",
    "  10. $1u = u$\n",
    "\n",
    "subspace of a vector space $V$ : a subset $H$ of $V$ that has three properties:\n",
    "- The zero vector of $V$ is in $H$\n",
    "- $H$ is closed under vector addition. That is, for each $u$ and $v$ in $H$, the sum $u+v$ is in $H$\n",
    "- $H$ is closed under multiplication by scalars. That is, for each $u$ in $H$ and each scalar $c$, the vector $cu$ is in $H$\n",
    "\n",
    "Theorem 1 : if $v_1,...,v_p$ are in a vector space $V$, then Span$\\{ v_1,...,v_p \\}$ is a subspace of $V$\n",
    "- the subspace spanned (or generated) by $\\{v_1,...,v_p\\}$ is Span $\\{v_1,...,v_p\\}$ \n",
    "- Given any subspace $H$ of $V$, the set $\\{v_1,...,v_p\\}$ in $H$ such that $H = $ Span $\\{v_1,...,v_p\\}$, is a spanning set for $H$\n",
    "  - if every vector in $H$ can be expressed as a linear combination of the vectors in the set, then the set is called by spanning set for $H$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42dfa3f6",
   "metadata": {},
   "source": [
    "### 2) Null Spaces, Column Spaces, Row Spaces, and Linear Transformations\n",
    "\n",
    "null space of an $m \\times n$ matrix $A$, written as Nul $A$ : the set of all solution of the homogeneous equation $Ax = 0$\n",
    "- In set notation, Nul $A = \\{x : x$ is in $R^n$ and $Ax = 0\\}$\n",
    "\n",
    "column space of an $m \\times n$ matrix $A$, written as Col $A$ : the set of all linear combinations of the columns of $A$\n",
    "- If $A = \\begin{bmatrix} a_1 & \\cdots & a_n \\end{bmatrix}$, then Col $A = $ Span $\\{a_1,...,a_n\\}$\n",
    "\n",
    "The set of all linear combinations of the row vectors in $A$ is called the row space of $A$ and is denoted by Row $A$ <br>\n",
    "\n",
    "\n",
    "Theorem 2 : The null space of an $m \\times n$ matrix $A$ is a subspace of $R^n$\n",
    "- Let $u$ and $v$ represent any two vectors in Nul $A$, Then $Au = 0$ and $Av = 0$\n",
    "  - $0$ is in Nul $A$\n",
    "  - $A(u+v) = Au + Av = 0 + 0 = 0$, which shows that $u+v$ is in Nul $A$\n",
    "  - $A(cu) = c(Au) = c(0) = 0$ for any scalar $c$, which shows that $cu$ is in Nul $A$ \n",
    "- the set of vectors of which the general solution of $Ax = 0$ is the linear combination <br>\n",
    "  where the weights are the free variables, is spanning set for Nul $A$\n",
    "\n",
    "Theorem 3 : The columns space of an $m \\times n$ matrix $A$ is a subspace of $R^m$\n",
    "- Let $u$ and $v$ represent any two vectors in Col $A$, Then there are $x_1$, $x_2$ in $R^n$ such that $Ax_1 = u$, $Ax_2 = v$\n",
    "  - $0$ is in Col $A$\n",
    "  - $A(x_1+x_2) = Ax_1 + Ax_2 = u+v$, which shows that $u+v$ is in Col $A$\n",
    "  - $A(cx_1) = c(Ax_1) = cu$ for any scalar $c$, which shows that $cu$ is in Col $A$\n",
    "  \n",
    "A linear transformation $T$ from a vector space $V$ into a vector space $W$ <br>\n",
    ": a rule that eassigns to each vector $x$ in $V$ a unique vector $T(x)$ in $W$, such that:\n",
    "1. $T(u+v) = T(u) + T(v) \\ $ for all $u$, $v$ in $V$\n",
    "2. $T(cu) = cT(u) \\ $ for all $u$ in $V$ and all scalars $c$\n",
    "- kernal (or null space) : the set of all $u$ in $V$ such that $T(u) = 0$\n",
    "- range : the set of all vectors in $W$ of the form $T(x)$ for some $x$ in $V$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db43f214",
   "metadata": {},
   "source": [
    "### 3) Linearly Independent Sets; Bases\n",
    "\n",
    "Theorem 4 : An indexed set $\\{v_1,...,v_p\\}$ of two or more vectors, with $v_1 \\neq 0$, is linearly dependent <br>\n",
    "if and only if some $v_j$ (with $j>1$) is a linear combination of the preceding vectors, $v_1,...,v_{j-1}$ <br>\n",
    "\n",
    "Let $H$ be a subspace of a vector space $V$. A set of vectors $\\mathcal{B}$ in $V$ is a basis for $H$ if\n",
    "- $\\mathcal{B}$ is a linearly independent set\n",
    "- the subspace spanned by $\\mathcal{B}$ coincides with $H$; that is, $H =$ Span $\\mathcal{B}$\n",
    "\n",
    "Theorem 5 (The Spanning Set Theorem)\n",
    "- Let $S = \\{v_1,...,v_p\\}$ be a set in a vector space $V$, and let $H = $ Span $\\{v_1,...,v_p\\}$\n",
    "- If one of the vectors in $S$ - say, $v_k$ - is a linear combination of the remaining vectors in $S$, <br>\n",
    "  then the set formed from $S$ by removing $v_k$ still spans $H$\n",
    "- If $H \\neq \\{0\\}$, some subset of $S$ is a basis for $H$\n",
    "\n",
    "Theorem 6 : The pivot columns of a matrix $A$ form a basis for Col $A$\n",
    "- proof : Theorem 13 in Chapter 2  + The Spanning Set Theorem (about non-pivot columns of $A$) <br>\n",
    "\n",
    "\n",
    "Theorem 7 : If two matrices $A$ and $B$ are row equivalent, then their row spaces are the same\n",
    "- If $B$ is obtained from $A$ by row operations, the rows of $B$ are linear combinations of the rows of $A$\n",
    "- It follows that any linear combination of the rows of $B$ is automatically a linear combination of the rows of $A$\n",
    "  - If $B$ is in echelon form, the nonzero rows of $B$ are linearly independent\n",
    "  - Thus If $B$ is in echelon form, the nonzero row of $B$ form a basis of the row space of $B$ and $A$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1915f760",
   "metadata": {},
   "source": [
    "### 4) Coordinate Systems\n",
    "\n",
    "Theorem 8 (The Unique Representation Theorem)\n",
    "- Let $\\mathcal{B} = \\{b_1,...,b_n\\}$ be a basis for a vector space $V$\n",
    "- Then for each $x$ in $V$, there exists a unique set of scalars $c_1,...,c_n$ such that $x = c_1b_1 + \\cdots + c_nb_n$\n",
    "  - Suppose $x$ also has the representation $x = d_1b_1 + \\cdots + d_nb_n$ for scalars $d_1,...,d_n$\n",
    "  - Since $0 = x - x = (c_1 - d_1)b_1 + \\cdots + (c_n - d_n)b_n$ and $\\mathcal{B}$ is linearly independent, $c_j = d_j$ for $1 \\le j \\le n$\n",
    "\n",
    "Suppose $\\mathcal{B} = \\{b_1,...,b_n\\}$ is a basis for a vector space $V$ and $x$ is in $V$. <br>\n",
    "The coordinates of $x$ relative to the basis $\\mathcal{B}$ (or the $\\mathcal{B}$-coordinates of $x$) are the weights $c_1,...,c_n$ such that $x = c_1b_1 + \\cdots + c_nb_n$\n",
    "- $[ x ]_\\mathcal{B} = \\begin{bmatrix} c_1 \\\\ \\vdots \\\\ c_n \\end{bmatrix}$ is the coordinate vector of $x$ (relative to $\\mathcal{B}$), or the $\\mathcal{B}$-coordinate vector of $x$\n",
    "- The mapping $x \\mapsto [x]_\\mathcal{B}$ is the coordinate mapping (determined by $\\mathcal{B}$)\n",
    "- we call $P_\\mathcal{B} = \\begin{bmatrix} b_1 & b_2 & \\cdots & b_n \\end{bmatrix}$ the change-of-coordinates matrix\n",
    "\n",
    "\n",
    "Theorem 9 : Let $\\mathcal{B} = \\{b_1,...,b_n\\}$ be a basis for a vector space $V$\n",
    "- Let $u = c_1b_1 + \\cdots + c_nb_n$ and $w = d_1b_1 + \\cdots + d_nb_n$\n",
    "- $x \\mapsto [x]_\\mathcal{B}$ is linear transformation\n",
    "  - $[u+w]_\\mathcal{B} = \\begin{bmatrix} c_1 + d_1 \\\\ \\vdots \\\\ c_n + d_n \\end{bmatrix} = \\begin{bmatrix} c_1 \\\\ \\vdots \\\\ c_n \\end{bmatrix} + \\begin{bmatrix} d_1 \\\\ \\vdots \\\\ d_n \\end{bmatrix} = [u]_\\mathcal{B} + [w]_\\mathcal{B}$, and $[ru]_\\mathcal{B} = \\begin{bmatrix} rc_1 \\\\ \\vdots \\\\ rc_n \\end{bmatrix} = r \\begin{bmatrix} c_1 \\\\ \\vdots \\\\ c_n \\end{bmatrix} = r [u]_\\mathcal{B}$\n",
    "- $x \\mapsto [x]_\\mathcal{B}$ is one-to-one\n",
    "  - By the unique representation theorem, if $[u]_\\mathcal{B} = [w]_\\mathcal{B}$, then $u = w$\n",
    "- $x \\mapsto [x]_\\mathcal{B}$ is from $V$ onto $R^n$\n",
    "  - For all $[u]_\\mathcal{B}$ in $R^n$, there exists $u = P_\\mathcal{B} [u]_\\mathcal{B}$ in $V$ \n",
    "\n",
    "one-to-one linear transformation from a vector space $A$ onto a vector space $B$ <br>\n",
    "is called an isomorphism from $A$ onto $B$. Therefore $V$ is isomorphic to $R^n$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523ee859",
   "metadata": {},
   "source": [
    "### 5) The Dimension of a Vector Space\n",
    "\n",
    "Theorem 10 : If a vector space $V$ has a basis $\\mathcal{B} = \\{b_1,...,b_n\\}$, then any set in $V$ containing more than $n$ vectors must be linearly dependent\n",
    "- Let $\\{u_1,...,u_p\\}$ be a set in $V$ with more than $n$ vectors.\n",
    "- The coordinate vectors $[u_1]_\\mathcal{B},...,[u_p]_\\mathcal{B}$ form a linearly dependent set in $R^n$ <br>\n",
    "  because there are more vectors $(p)$ than entries $(n)$ in each vector\n",
    "- So there exist scalars $c_1,...,c_p$, not all zero, such that $c_1[u_1]_\\mathcal{B} + \\cdots + c_p[u_p]_\\mathcal{B} = \\begin{bmatrix} 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix}$\n",
    "- By Theorem 9, since the coordinate mapping is a linear transformation, $[c_1u_1 + \\cdots + c_pu_p]_\\mathcal{B} = \\begin{bmatrix} 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix}$ <br>\n",
    "  $\\Rightarrow c_1u_1 + \\cdots + c_pu_p = 0b_1 + \\cdots + 0b_n = 0$. Therefore $\\{u_1,...,u_p\\}$ is linearly dependent\n",
    "\n",
    "Theorem 11 : If a vector space $V$ has a basis of $n$ vectors, then every basis of $V$ must consists of exactly $n$ vectors\n",
    "- Let $\\mathcal{B}_1$ be a basis of $n$ vectors and $\\mathcal{B}_2$ be any other basis (of $V$)\n",
    "- Since $\\mathcal{B}_1$ is a basis and $\\mathcal{B}_2$ is linearly independent, $\\mathcal{B}_2$ has no more than $n$ vectors, by Theorem 10\n",
    "- Since $\\mathcal{B}_2$ is a basis and $\\mathcal{B}_1$ is linearly independent, $\\mathcal{B}_2$ has at least $n$ vectors.\n",
    "- Thus $\\mathcal{B}_2$ consists of exactly $n$ vectors\n",
    "\n",
    "If a vector space $V$ is spanned by a finite set, then $V$ is said to be finite-dimensional <br>\n",
    "and the dimension of $V$, writtne as dim $V$, is the number of vectors in a basis for $V$ <br>\n",
    "The dimension of the zero vector space $\\{0\\}$ is defined to be zero <br>\n",
    "If $V$ is not spanned by a finite set, then $V$ is said to be infinite-dimensional <br>\n",
    "\n",
    "Theorem 12 : Let $H$ be a subspace of a finite-dimensional vector space $V$\n",
    "- Any linearly independent set in $H$ can be expanded, if necessary, to a basis for $H$\n",
    "- Also, $H$ is finite-dimensional and dim $H \\le $ dim $V$\n",
    "  - If $H = \\{0\\}$, then certainly dim $H = 0 \\le $ dim $V$. Otherwise, let $S = \\{u_1,...,u_k\\}$ be any linearly independent in $H$\n",
    "  - If $S$ spans $H$, then $S$ is a basis for $H$, Otherwise, there is some $u_{k+1}$ in $H$ that is not in Span $S$.\n",
    "  - But then $\\{u_1, ..., u_k, u_{k+1}\\}$ will be linearly independent by Theorem 4\n",
    "  - So long as the new set does not span $H$, we can continue this process of exapnding $S$ to a larger linearly independent set in $H$\n",
    "  - But by Theorem 10, the number of vectors in a linearly independent expansion of $S$ can never exeed the dimension of $V$\n",
    "  - So eventually the expansion of $S$ will span $H$ and hence will be a basis for $H$, and dim $H \\le$ dim $V$\n",
    "\n",
    "Theorem 13 (The Basis Theorem) : Let $V$ be a $p$-dimensional vector space\n",
    "- Any linearly independent set of exactly $p$ elements in $V$ is automatically a basis for $V$\n",
    "  - By Theorem 12, a linearly independent set $S$ of $p$ elements can be extended to a basis for $V$\n",
    "  - But that basis must contain exactly $p$ elements. since dim $V$ = $p$. So $S$ must already be a basis for $V$\n",
    "- Any set of exactly $p$ elements that spans $V$ is automatically a basis for $V$\n",
    "  - Suppose that $S$ has $p$ elements and spans $V$\n",
    "  - By the Spanning Set Theorem, a subset $S'$ of $S$ is a basis of $V$\n",
    "  - Since dim $V$ $= p$, $S'$ must contain $p$ vectors, Hence $S = S'$\n",
    "\n",
    "The rank of $A$ is the dimension of the column space and the nullity of $A$ is the dimension of the null space <br>\n",
    "\n",
    "Theorem 14 (The Rank Theorem) : rank $A$ + nullity $A$ = number of columns in $A$\n",
    "- By Theorem 6, rank $A$ is the number of pivot columns in $A$\n",
    "- nullity $A$ equals the number of free variables in the equation $Ax = 0$. That is, nullity $A$ is the number of non-pivot columns in $A$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4757578",
   "metadata": {},
   "source": [
    "### 6) Change of Basis\n",
    "\n",
    "Theorem 15 : Let $\\mathcal{B} = \\{b_1,...,b_n\\}$ and $\\mathcal{C} = \\{c_1,...,c_n\\}$ be bases of a vector space $V$\n",
    "- there is a unique $n \\times n$ matrix $P_{\\mathcal{C} \\leftarrow \\mathcal{B}}$ such that $[x]_\\mathcal{C} = P_{C \\leftarrow B} [x]_\\mathcal{B}$\n",
    "  - Suppose $[x]_\\mathcal{B} = \\begin{bmatrix} k_1 \\\\ \\vdots \\\\ k_n \\end{bmatrix}$ such that $x = k_1b_1 + \\cdots + k_nb_n$\n",
    "  - Apply the coordinate mapping determined by $\\mathcal{C}$: <br>\n",
    "    $[x]_\\mathcal{C} = [k_1b_1 + \\cdots + k_nb_n]_\\mathcal{C} = k_1[b_1]_\\mathcal{C} + \\cdots + k_n[b_n]_\\mathcal{C} \\ \\ $ ( since the coordinate mapping is a linear transformation ) <br>\n",
    "    $= \\begin{bmatrix} [b_1]_\\mathcal{C} & \\cdots & [b_n]_\\mathcal{C} \\end{bmatrix} \\begin{bmatrix} k_1 \\\\ \\vdots \\\\ k_n \\end{bmatrix} = \\begin{bmatrix} [b_1]_\\mathcal{C} & \\cdots & [b_n]_\\mathcal{C} \\end{bmatrix} [x]_\\mathcal{B}$ \n",
    "  - Therefore $P_{\\mathcal{C} \\leftarrow \\mathcal{B}} = \\begin{bmatrix} [b_1]_\\mathcal{C} & \\cdots & [b_n]_\\mathcal{C} \\end{bmatrix}$\n",
    "\n",
    "The columns of $P_{\\mathcal{C} \\leftarrow \\mathcal{B}}$ are linearly independent\n",
    "- The statement equals that $x_1[b_1]_\\mathcal{C} + \\cdots + x_n[b_n]_\\mathcal{C} = 0$ has only the trivial solution\n",
    "- Since the coordinate mapping is a linear transformation, $x_1[b_1]_\\mathcal{C} + \\cdots + x_n[b_n]_\\mathcal{C} = [x_1b_1 + \\cdots + x_nb_n]_\\mathcal{C} = 0$ <br>\n",
    "  $\\Leftrightarrow x_1b_1 + \\cdots + x_nb_n = 0 \\ $ ( since $\\mathcal{C}$ is basis so that the columns of $\\mathcal{C}$ is linearly independent )\n",
    "- Since $b_1,...,b_n$ is linearly independent, $x_1b_1 + \\cdots + x_nb_n = 0$ has only the trivial solution. <br>\n",
    "  Therefore $x_1[b_1]_\\mathcal{C} + \\cdots + x_n[b_n]_\\mathcal{C} = 0$ has only trivial solution\n",
    "\n",
    "By Theorem 11, $[x]_\\mathcal{B}$ and $[x]_\\mathcal{C}$ is same size. Thus $P_{\\mathcal{C} \\leftarrow \\mathcal{B}}$ is sqaure.  <br>\n",
    "Therefore, since The columns of $P_{\\mathcal{C} \\leftarrow \\mathcal{B}}$ are linearly independent, there exists $(P_{\\mathcal{C} \\leftarrow \\mathcal{B}})^{-1}$. <br>\n",
    "Left-multiplying both sides of $[x]_\\mathcal{C} = P_{\\mathcal{C} \\leftarrow \\mathcal{B}} [x]_\\mathcal{B} \\ $ by $(P_{\\mathcal{C} \\leftarrow \\mathcal{B}})^{-1}$ <br>\n",
    "yields $(P_{\\mathcal{C} \\leftarrow \\mathcal{B}})^{-1} [x]_\\mathcal{C} = [x]_\\mathcal{B}$. Thus $(P_{\\mathcal{C} \\leftarrow \\mathcal{B}})^{-1} = P_{\\mathcal{B} \\leftarrow \\mathcal{C}}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d5bf0d",
   "metadata": {},
   "source": [
    "### 7) Digital Signal Processing (DSP)\n",
    "\n",
    "A signal in $\\mathbb{S}$ : an infinite sequence of numbers, $\\{y_k\\}$, where the subscripts $k$ range over all integers\n",
    "- delta ($\\delta$) : $(...,0,0,0,1,0,0,0,...) \\ \\ $ [ $\\{d_k\\}$, where if $k=0$ then $1$, else $0$ ]\n",
    "- unit step ($\\nu$) : $(...,0,0,0,1,1,1,1,...) \\ \\ $ [ $\\{u_k\\}$, where if $k \\geq 0$ then $1$, else $0$ ]\n",
    "\n",
    "Linear Time Invariant (LTI) Transformations : $T : \\mathbb{S} \\rightarrow \\mathbb{S}$ provided:\n",
    "- $T(\\{x_k + y_k\\}) = T(\\{x_k\\}) + T(\\{y_k\\})$ for all signals $\\{x_k\\}$ and $\\{y_k\\}$\n",
    "- $T(c\\{x_k\\}) = cT(\\{x_k\\})$ for all scalars $c$ and all signals $\\{x_k\\}$\n",
    "- If $T(\\{x_k\\}) = \\{y_k\\}$, then $T(\\{x_{k+q}\\}) = \\{y_{k+q}\\}$ for all integers $q$ and all signals $\\{x_k\\}$\n",
    "\n",
    "Theorem 16 : LTI Transformation is a special type of Linear Transformation <br>\n",
    "\n",
    "Let $S$ be the transformation that shifts each element in a signal to the right, specifically $S(\\{x_k\\}) = \\{y_k\\}$, where $y_k = x_{k-1}$\n",
    "- Ex) $\\ \\delta$ : $(...,0,0,1,0,0,...)$ $\\ \\Rightarrow\\ $ $S^1(\\delta) = (...,0,0,0,1,0,...)$, $S^2(\\delta) = (...,0,0,0,0,1,...)$, ...\n",
    "- $S$ is LTI Transformation\n",
    "  - $S(\\{x_k\\} + \\{y_k\\}) = \\{x_{k-1} + y_{k-1}\\} = \\{x_{k-1}\\} + \\{y_{k-1}\\} = S(\\{x_k\\}) + S(\\{y_k\\})$\n",
    "  - $S(c\\{x_k\\}) = \\{cx_{k-1}\\} = c\\{x_{k-1}\\} = cS(\\{x_k\\})$\n",
    "  - $S(\\{x_{k+q}\\}) = \\{x_{k-1+q}\\} = \\{y_{k+q}\\}$ for any integer $q$\n",
    "\n",
    "$\\mathbb{S}_n$ ( The set of signals of length $n$ ) : the set of all signals $\\{y_k\\}$ such that $y_k = 0$ whenever $k < 0$ or $k > n$ <br>\n",
    "\n",
    "Theorem 17 : $\\mathbb{S}_n$ is a subspace of $\\mathbb{S}$ isomorphic to $R^{n+1}$, and $\\mathcal{B}_n = \\{\\delta, S(\\delta), S^2(\\delta), ..., S^n(\\delta)\\}$ forms a basis for $\\mathbb{S}$\n",
    "- The zero signal is in $\\mathbb{S}_n$, and adding or scaling signals cannot create nonzeros in the positions that must contain zeros\n",
    "  - Therefore $\\mathbb{S}_n$ is a subspace of $\\mathbb{S}$\n",
    "- Let $\\{y_k\\}$ be any signal in $\\mathbb{S}_n$. Then $\\{y_k\\} = \\sum^n_{j=0} y_jS^j(\\delta) = y_0\\delta + y_1S(\\delta) + ... + y_nS^n(\\delta)$\n",
    "  - Span $\\{\\delta, S(\\delta), S^2(\\delta), ..., S^n(\\delta)\\} = \\mathbb{S}_n$, so $\\mathcal{B}_n$ is a spanning set for $\\mathbb{S}_n$ \n",
    "  - $c_0\\delta + c_1S(\\delta) + ... + c_nS^n(\\delta) = \\{0\\} \\ \\Rightarrow \\  (...,0,0,c_0,c_1,...,c_n,0,0,...) = (...,0,0,0,0,...,0,0,0,...)$ <br>\n",
    "    so $c_0 = c_1 = \\cdots = c_n = 0$ and thus the vectors in $\\mathcal{B}_n$ form a lineary independent set\n",
    "  - Therefore $\\mathcal{B}_n$ is a basis for $\\mathbb{S}_n$, and hence it is an $n+1$ dimensional vector space isomorphic to $R^{n+1}$\n",
    "    - By Theorem 9, we can check if $\\mathcal{B} = \\{b_1,...,b_n\\}$ is a basis for a vector space $V$, then $V$ is isomorphic to $R^n$\n",
    "\n",
    "$\\mathbb{S}_f$ ( The set of finitely supported signals ) : the set of signals $\\{y_k\\}$, where only finitely many of the entries are nonzero\n",
    "- Ex) the signals created by recording the daily price of a stock increase in length each day, <br>\n",
    "  but remain finitely supported, and hence these signals belong to $\\mathbb{S}_f$, but not to any particular $\\mathbb{S}_n$\n",
    "- Conversely, if a signal is in $\\mathbb{S}_n$, for some positive integer $n$, then it is also in $\\mathbb{S}_f$\n",
    "- $\\mathbb{S}_f$ is a subspace of $\\mathbb{S}$ for the same reasons as $\\mathbb{S}_n$\n",
    "\n",
    "Theorem 18 : The set $\\mathcal{B}_f = \\{S^j(\\delta) : $ where $ j \\in \\mathbb{Z}\\}$ is a basis for the infinite dimensional vector space $\\mathcal{S}_f$\n",
    "- Let $\\{y_k\\}$ be **any** signal in $\\mathbb{S}_f$, <br>\n",
    "- Since only finitely many entries in $\\{y_k\\}$ are nonzero, there exist integers $p$ and $q$ such that $y_k = 0$ for all $k < p$ and $k > q$ <br>\n",
    "  Thus $\\{y_k\\} = \\sum^q_{j=p} y_j S^j(\\delta)$\n",
    "- So $\\mathcal{B}_f$ is a spanning set for $\\mathbb{S}_f$ because every vector in $\\mathbb{S}_f$ can be expressed as a linear combination of the vectors in $\\mathcal{B}_f$\n",
    "- And $\\sum^q_{j=p} c_j S^j(\\delta) = \\{0\\} \\Rightarrow c_p = c_{p+1} = \\cdots = c_q = 0$, thus the vectors in $\\mathcal{B}_f$ form a linearly independent set\n",
    "- Thererfore $\\mathcal{B}_f$ is a basis for $\\mathbb{S}_f$. and since $\\mathcal{B}_f$ contains infinitely many signals, $\\mathbb{S}_f$ is an infinite dimensional vector space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852af5b6",
   "metadata": {},
   "source": [
    "### 8) Applications to Difference Equations\n",
    "\n",
    "Given scalars $a_0,...,a_n$, with $a_0$ and $a_n$ nonzero, and given a **signal** $\\{z_k\\}$, <br>\n",
    "the equation $a_0y_{k+n} + a_1y_{k+n-1} + \\cdots + a_{n-1}y_{k+1} + a_ny_k = z_k$ for all $k$ <br>\n",
    "is called a linear differnece equation (or linear recurrence relation) of order $n \\ \\ $ ( Note that here $z_k$ is signal )\n",
    "- For simplicity, $a_0$ is often taken equal to 1.\n",
    "- If $\\{z_k\\}$ is the zero sequence, the equation is homogeneous; otherwise, the equation is nonhomogeneous\n",
    "- $\\{y_k\\}$ that satisfies the equation is called a solution of the equation\n",
    "- In DSP, the equation describes a LTI filter, and $a_0,...,a_n$ are called the filter coefficients\n",
    "  - The LTI filter $T : \\mathbb{S} \\to \\mathbb{S}$ associated with a linear difference equation <br>\n",
    "    defines $T = a_0S^{-n} + a_1S^{-n+1} + \\cdots + a_{n-1}S^{-1} + a_nS^0$, such that $T(\\{y_k\\}) = \\{z_k\\}$  \n",
    "  - If the linear difference equation is homogeneous, then the solution set of the equation is subspace of $\\mathbb{S}$\n",
    "    1. If $\\{y_k\\} = (0,...,0) = 0$, then $T(\\{y_k\\}) = 0$, thus zero signal is in the solution set\n",
    "    2. $\\{p_k\\}$ and $\\{q_k\\}$ are in the solution set $\\Rightarrow$ $T(\\{p_k\\}) = 0$, $T(\\{q_k\\}) = 0$ <br>\n",
    "      $\\Rightarrow$ $T(\\{p_k\\}) + T(\\{q_k\\}) = T(\\{p_k + q_k\\}) = 0$ $\\Rightarrow$ $\\{p_k + q_k\\} = \\{p_k\\} + \\{q_k\\}$ is in the solution set\n",
    "    3. for any scalar $c$, $\\{p_k\\}$ is in the solution set $\\Rightarrow$ $T(\\{p_k\\}) = 0$ <br>\n",
    "      $\\Rightarrow$ $c T(\\{p_k\\}) = T(\\{c p_k\\}) = 0$ $\\Rightarrow$ $\\{c p_k\\} = c \\{p_k\\}$ is in the solution set\n",
    "    - In general, the kernel of any linear transformation with domain $\\mathbb{S}$ is a subspace of $\\mathbb{S}$\n",
    "\n",
    "Theorem 19, 20 : If $a_n \\neq 0$, $\\{z_k\\}$ is given, and $y_0,...,y_{n-1}$ are specified:\n",
    "- Theorem 19 : the equation $y_{k+n} + a_1y_{k+n-1} + \\cdots + a_{n-1}y_{k+1} + a_ny_k = z_k$ for all $k$, has a unique solution\n",
    "  - To define $y_{n+k}$ for $k \\geq 0$, use the recurrence relation $y_{k+n} = z_k - [a_1y_{k+n-1} + \\cdots + a_ny_k]$\n",
    "  - To define $y_{n+k}$ for $k < 0$, use the recurrence relation $y_k = \\frac{1}{a_n} z_k - \\frac{1}{a_n} [ y_{k+n} + a_1y_{k+n-1} + \\cdots + a_{n-1}y_{k+1}$\n",
    "- Theorem 20 : If $\\{z_k\\}$ is zero sequence, than the set $H$ of all solutions of the equation is an $n$-dimensional vector space\n",
    "  - $H$ is a subspace of $\\mathbb{S}$ because $H$ is the kernel of a linear transformation\n",
    "  - For $\\{y_k\\}$ in $H$, let $F\\{y_k\\}$ be the vector in $R^n$ given by $(y_0,y_1,...,y_{n-1})$. than $F : H \\to R^n$ is a linear transformation\n",
    "  - By Theorem 19, given any vector $(y_0,y_1,...,y_{n-1})$ in $R^n$, there is a unique signal $\\{y_k\\}$ in $H$ such that $F\\{y_k\\} = (y_0,y_1,...,y_{n-1})$ <br>\n",
    "  - This means that $F$ is a one-to-one linear transformation of $H$ onto $R^n$: that is, $F$ is an isomorphism.\n",
    "  - Thus dim $H$ = dim $R^n$ = $n$\n",
    "  \n",
    "The equation $y_{k+n} + a_1y_{k+n-1} + \\cdots + a_{n-1}y_{k+1} + a_ny_k = z_k$ for all $k$, <br>\n",
    "can be rewritten as the first-order difference equation <br>\n",
    "$x_{k+1} = Ax_k$ for all $k$, where $x_k = \\begin{bmatrix} y_k \\\\ y_{k+1} \\\\ \\vdots \\\\ y_{k+n-1} \\end{bmatrix}$, and\n",
    "$A = \\begin{bmatrix} 0 & 1 & 0 & \\cdots & 0 \\\\\n",
    "                     0 & 0 & 1 & \\cdots & 0 \\\\\n",
    "                     \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "                     0 & 0 & 0 & \\cdots & 0 \\\\\n",
    "                     -a_n & -a_{n-1} & -a_{n-2} & \\cdots & -a_1 \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c663058",
   "metadata": {},
   "source": [
    "*****"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be17cd84",
   "metadata": {},
   "source": [
    "## Chapter 5 : Eigenvalues and Eigenvectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa5766c",
   "metadata": {},
   "source": [
    "### 1) Eigenvectors and Eigenvalues\n",
    "\n",
    "eigenvector of an $n \\times n$ matrix $A$ : nonzero vector $x$ such that $Ax = \\lambda x$ for some scalar $\\lambda$ <br>\n",
    "A scalar $\\lambda$ is called an eigenvalue of $A$ if there is a nontrivial solution $x$ of $Ax = \\lambda x$ <br>\n",
    "and such an $x$ is called an eigenvector corresponding to $\\lambda$\n",
    "- $v$ is eigenvector if and only if $Av$ is a multiple of $v$\n",
    "- $\\lambda$ is eigenvalue if and only if $(A - \\lambda I) x = 0$ has a nontrivial solution\n",
    "  - The set of all solutions of $(A - \\lambda I) x = 0$ is just the null space of the matrix $A - \\lambda I$ <br>\n",
    "    So this set is a subspace of $R^n$, and is called the eigenspace of $A$ corresponding to $\\lambda$\n",
    "\n",
    "Theorem 1 : The eigenvalues of a triangular matrix are the entries on its main diagonal\n",
    "- For simplicity, consider the $3 \\times 3$ case. If $A$ is upper triangular, then  <br>\n",
    "  $A - \\lambda I = \\begin{bmatrix} a_{11} & a_{12} & a_{13} \\\\ 0 & a_{22} & a_{23} \\\\ 0 & 0 & a_{33} \\end{bmatrix} - \\begin{bmatrix} \\lambda & 0 & 0 \\\\ 0 & \\lambda & 0 \\\\ 0 & 0 & \\lambda \\end{bmatrix} = \\begin{bmatrix} a_{11} - \\lambda & a_{12} & a_{13} \\\\ 0 & a_{22} - \\lambda & a_{23} \\\\ 0 & 0 & a_{33} - \\lambda \\end{bmatrix}$\n",
    "- Thus $(A - \\lambda I)x = 0$ has a free variable if and only if at least one of the entries on the diagonal of $(A - \\lambda I) x = 0$  is zero<br>\n",
    "- Therefore $\\lambda$ equals one of the entries $a_{11}, a_{22}, a_{33}$. And similarly the case in which $A$ is lower triangular can show\n",
    "\n",
    "Theorem 2\n",
    "- If $v_1,...,v_r$ are eigenvectors that correspond to **distinct** eigenvalues $\\lambda_1,...,\\lambda_r$ of an $n \\times n$ matrix $A$, <br>\n",
    "  then the set $\\{v_1,...,v_r\\}$ is linearly independent\n",
    "- proof : Suppose $\\{v_1,...,v_r\\}$ is linearly dependent\n",
    "  - Since $v_1$ is nonzero, by Theorem 7 in Chapter 1, There is $p$ that is the least index <br>\n",
    "    such that $v_{p+1}$ is a linear combination of the preceding (linearly independent) vectors, <br>\n",
    "    then there exist scalars $c_1,...,c_p$ such that $c_1v_1 + \\cdots + c_pv_p = v_{p+1} \\ \\ \\cdots \\ $ (1)\n",
    "  - Multiplying both sides of (1) by $A$ and using the fact that $Av_k = \\lambda_kv_k$ for each $k$, <br>\n",
    "    we have $c_1Av_1 + \\cdots + c_pAv_p = Av_{p+1} \\Rightarrow c_1\\lambda_1v_1 + \\cdots + c_p\\lambda_pv_p = \\lambda_{p+1}v_{p+1} \\ \\ \\cdots \\ $ (2) \n",
    "  - Multiplying both sides of (1) by $\\lambda_{p+1}$ and subtracting the result from (2), <br>\n",
    "    we have $c_1(\\lambda_1 - \\lambda_{p+1})v_1 + \\cdots + c_p(\\lambda_p - \\lambda_{p+1})v_p = 0 \\ \\ \\cdots \\ $ (3)\n",
    "  - Since $\\{v_1,...,v_p\\}$ is linearly independent, the weights in (3) are all zero, <br>\n",
    "    But none of the factors $\\lambda_i - \\lambda_{p+1}$ are zero, because the eigenvalues are distinct. Hence $c_i = 0$ for $i = 1,...,p$.\n",
    "  - But then (1) says that $v_{p+1} = 0$, which is impossible. <br>\n",
    "    Hence $\\{v_1,...,v_r\\}$ cannot be linearly dependent and therefore must be linearly independent\n",
    "\n",
    "For the first-order difference equation $x_{k+1} = Ax_k \\ \\ (k = 0,1,2,...)$, If $A$ is an $n \\times n$ matrix, <br>\n",
    "then $x_k = \\lambda^k x_0$, where $x_0$ is eigenvector corresponding to $\\lambda$, is a one of the solutions of the equation <br>\n",
    "because $Ax_k = A(\\lambda^k x_0) = \\lambda^k (Ax_0) = \\lambda^k (\\lambda x_0) = \\lambda^{k+1} x_0 = x_{k+1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49672f09",
   "metadata": {},
   "source": [
    "### 2) The Characteristic Equation\n",
    "\n",
    "Theorem 3 : Properties of Determinants ( = Section 2 of Chapter 3 )\n",
    "\n",
    "The Invertible Matrix Theorem (continued) : 18. The number $0$ is not an eigenvalue of $A$\n",
    "- $A$ is invertible if and only if the equation $Ax = 0$ has only the trivial solution, <br>\n",
    "  so there is not a nonzero vector $x$ such that $Ax = 0x$, which means that $0$ is not an eigenvalue\n",
    "\n",
    "The characteristic equation : $\\det(A - \\lambda I) = 0$\n",
    "- A scalar $\\lambda$ is an eigenvalue of an $n \\times n$ matrix $A$ if and only if $\\lambda$ satisfies the characteristic equation\n",
    "- if $A$ is an $n \\times n$ matrix, then $\\det(A - \\lambda I)$ is a polynomial of degree $n$ called the characteristic polynomial of $A$\n",
    "\n",
    "If $A$ and $B$ are $n \\times n$ matrices, then $A$ is similar to $B$ : there is an invertible matrix $P$ such that $P^{-1}AP = B$\n",
    "- If $A$ is similar to $B$, then writing $Q$ for $P^{-1}$, we have $Q^{-1}BQ = A$. So $B$ is also similar to $A$ <br>\n",
    "  so we say simply that $A$ and $B$ are similar\n",
    "- Changing $A$ into $P^{-1}AP$ is called a similarity transformation\n",
    "\n",
    "Theroem 4\n",
    "- If $n \\times n$ matrices $A$ and $B$ are similar, then they have the same characteristic polynomial <br>\n",
    "  and hence the same eigenvalues (with the same multiplicities)\n",
    "  - If $B = P^{-1}AP$, then $B - \\lambda I = P^{-1}AP - \\lambda P^{-1}P = P^{-1}(AP - \\lambda P) = P^{-1}(A - \\lambda I)P$\n",
    "  - By Theorem 3, $\\det(B - \\lambda I) = \\det [P^{-1} (A - \\lambda I) P] = \\det(P^{-1}) \\cdot \\det(A - \\lambda I) \\cdot \\det(P)$\n",
    "  - Therefore $\\det(B - \\lambda I) = \\det(A - \\lambda I)$ because $\\det(P^{-1}) \\cdot \\det(P) = \\det (P^{-1}P) = \\det I = 1$\n",
    "  \n",
    "Let $A = \\begin{bmatrix} 0.95 & 0.03 \\\\ 0.05 & 0.97 \\end{bmatrix}$. Analyze the long-term behavior (as $k$ increases) of the dynamical system <br>\n",
    "defined by $x_{k+1} = Ax_k \\ \\ ( k = 0, 1, 2, ... )$, with $x_0 = \\begin{bmatrix} 0.6 \\\\ 0.4 \\end{bmatrix}$\n",
    "- The characteristic equation for $A$ is $\\det \\begin{bmatrix} 0.95 - \\lambda & 0.03 \\\\ 0.05 & 0.97 - \\lambda \\end{bmatrix} = \\lambda^2 - 1.92 \\lambda + 0.92 = 0$\n",
    "- By the quadratic formula, the eigenvalues of $A$ are $\\lambda_1 = 1$ and $\\lambda_2 = 0.92$\n",
    "- Let $v_1, v_2$ be a eigenvectors corresponding to $\\lambda_1, \\lambda_2$, <br>\n",
    "  then $Av_1 = \\lambda_1v_1 = v_1 \\Rightarrow (A - I)v_1 = 0 \\Rightarrow$ a basis of eigenspace is $\\{\\begin{bmatrix} 3 \\\\ 5 \\end{bmatrix}\\}$ so let $v_1 = \\begin{bmatrix} 3 \\\\ 5 \\end{bmatrix}$, <br>\n",
    "  and $Av_2 = \\lambda_2v_2 = 0.92 v_2 \\Rightarrow (A - 0.92 I)v_2 = 0 \\Rightarrow$ a basis of eigenspace is $\\{\\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}\\}$ so let $v_2 = \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}$\n",
    "- Since $\\{v_1, v_2\\}$ is a basis for $R^2$, we can write $x_0 = c_1v_1 + c_2v_2 = \\begin{bmatrix} v_1 & v_2 \\end{bmatrix} \\begin{bmatrix} c_1 \\\\ c_2 \\end{bmatrix}$ <br>\n",
    "  In fact, $\\begin{bmatrix} c_1 \\\\ c_2 \\end{bmatrix} = \\begin{bmatrix} v_1 & v_2 \\end{bmatrix}^{-1} x_0 = \\begin{bmatrix} 3 & 1 \\\\ 5 & -1 \\end{bmatrix}^{-1} \\begin{bmatrix} 0.6 \\\\ 0.4 \\end{bmatrix} = \\frac{1}{-8} \\begin{bmatrix} -1 & -1 \\\\ -5 & 3 \\end{bmatrix} \\begin{bmatrix} 0.6 \\\\ 0.4 \\end{bmatrix} = \\begin{bmatrix} 0.125 \\\\ 0.225 \\end{bmatrix}$\n",
    "- So, $x_k = A^k x_0 = A^{k-1} ( A x_0 ) = A^{k-1} ( c_1Av_1 + c_2Av_2 ) = A^{k-1} ( c_1v_1 + c_2(0.92)v_2 )$ <br>\n",
    "  $= A^{k-2} ( c_1Av_1 + c_2(0.92)Av_2 ) = A^{k-2} ( c_1v_1 + c_2(0.92)^2v_2 ) = \\cdots = c_1v_1 + c_2(0.92)^kv_2$  <br>\n",
    "  $= 0.125 \\begin{bmatrix} 3 \\\\ 5 \\end{bmatrix} + 0.225 (0.92)^k \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix} \\ \\ (k = 0,1,2,...)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441b0d0f",
   "metadata": {},
   "source": [
    "### 3) Diagonalization\n",
    "\n",
    "A sqaure matrix $A$ is diagonalizable : $A$ is similar to a digonal matrix $D \\ \\ $ [ $A = PDP^{-1}$ for some invertible matrix $P$ ]\n",
    "\n",
    "Theorem 5 : The Diagonalization Theorem \n",
    "- An $n \\times n$ matrix $A$ is diagonalizable if and only if $A$ has $n$ linearly independent eigenvectors\n",
    "- In other words, $A$ is diagonalizable if and only if there are enough eigenvectors to form a basis of $R^n$, <br>\n",
    "  and we call such a basis an eigenvector basis of $R^n$\n",
    "- In fact, $A = PDP^{-1}$, with $D$ a diagonal matrix, if and only if the columns of $P$ are $n$ linearly independent eigenvectors of $A$\n",
    "- In this case, the diagonal entries of $D$ are eigenvalues of $A$ that correspond, respectively, to the eigenvectors in $P$\n",
    "  - Suppose $n \\times n$ matrix $A$ is diagnoalizable. Then there is $n \\times n$ invertible matrix $P$ and $n \\times n$ diagonal matrix $D$ <br>\n",
    "    such that $A = PDP^{-1}$. Let $P$ be matrix with columns $v_1,...,v_n$ and $D$ be matrix with diagonal entries $\\lambda_1,...,\\lambda_n$\n",
    "  - Right-multiplying $A = PDP^{-1}$ by $P$, we have $AP = PD$. And <br>\n",
    "    $AP = A \\begin{bmatrix} v_1 & v_2 & \\cdots & v_n \\end{bmatrix} = \\begin{bmatrix} Av_1 & Av_2 & \\cdots & Av_n \\end{bmatrix}$ <br>\n",
    "    $PD = P \\begin{bmatrix} \\lambda_1 & 0 & \\cdots & 0 \\\\ 0 & \\lambda_2 & \\cdots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & \\lambda_n \\end{bmatrix} = \\begin{bmatrix} \\lambda_1v_1 & \\lambda_2v_2 & \\cdots & \\lambda_nv_n \\end{bmatrix}$ <br>\n",
    "    Thus $AP = PD \\Rightarrow \\begin{bmatrix} Av_1 & Av_2 & \\cdots & Av_n \\end{bmatrix} = \\begin{bmatrix} \\lambda_1v_1 & \\lambda_2v_2 & \\cdots & \\lambda_nv_n \\end{bmatrix}$ <br>\n",
    "    $\\Rightarrow Av_1 = \\lambda_1v_1$, $Av_2 = \\lambda_2v_2$, ..., $Av_n = \\lambda_nv_n \\ \\ \\cdots \\ $ (1)\n",
    "  - Since $P$ is invertible, its columns $v_1,...,v_n$ are nonzero <br>\n",
    "    So the equations in (1) show that $\\lambda_1,...,\\lambda_n$ are eigenvalues and $v_1,...,v_n$ are corresponding eigenvectors <br>\n",
    "    And since $P$ is invertible, its columns $v_1,...,v_n$ must be linearly independent <br>\n",
    "    Therefore if $A$ is diagonalizable, then $A$ has $n$ linearly independent eigenvectors\n",
    "  - Conversely, if $A$ has $n$ linearly independent eigenvectors, then let's define $P$ and $D$ exactly as above <br>\n",
    "    Then we can check $AP = PD$ and $P$ is invertible so $A = PDP^{-1}$. Therefore $A$ is diagonalizable\n",
    "\n",
    "Theorem 6 : An $n \\times n$ matrix with $n$ distinct eigenvalues is diagonalizable\n",
    "- Let $v_1,...,v_n$ be eigenvectors corresponding to the $n$ distinct eigenvalues of a matrix $A$\n",
    "- Then by Theorem 2, $\\{v_1,...,v_n\\}$ is linearly independent\n",
    "- Hence by Theorem 5, $A$ is diagonalizable\n",
    "\n",
    "Theorem 7 : Let $A$ be an $n \\times n$ whose distinct eigenvalues are $\\lambda_1,...,\\lambda_p$\n",
    "1. For $1 \\le k \\le p$, the dimension of the eigenspace for $\\lambda_k$ is less than or equal to the multiplicity of the eigenvalue $\\lambda_k$\n",
    "2. The matrix $A$ is diagonalizable if and only if **the sum of the dimensions of the eigenspaces equals $n$**, which happens if and only if <br>\n",
    "   (i) the characteristic polynomial factors completely into linear factors and <br>\n",
    "   (ii) the dimension of the eigenspace for each $\\lambda_k$ equals the multiplicity of $\\lambda_k$\n",
    "3. If $A$ is diagonalizable and $\\mathcal{B}_k$ is a basis for the eigenspace corresponding to $\\lambda_k$ for each $k$, <br>\n",
    "   then the total collection of vectors in the sets $\\mathcal{B}_1,...,\\mathcal{B}_p$ forms an eigenvector basis for $R^n$\n",
    "- proof : (link)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c3bc47",
   "metadata": {},
   "source": [
    "### 4) Eigenvectors and Linear Transformations\n",
    "\n",
    "Let $V$ be a vector space ( notice the definition of vector space, such that $\\mathbb{S}$ and $\\mathbb{P}$ is vector space )\n",
    "- there is $T : V \\to V$ that is linear transformation but not matrix transformation <br>\n",
    "  if the vectors of $V$ cannot be multiplied by matrices  ( If $V$ is $\\mathbb{P}$, then the vectors here are polynomials ) <br>\n",
    "  or if $V$ is infinite-dimensional space and does not have a finite basis\n",
    "\n",
    "An eigenvector of a linear transformation $T : V \\to V$ is a nonzero vector $x$ in $V$ such taht $T(x) = \\lambda x$ for some scalar $\\lambda$ <br>\n",
    "A scalar $\\lambda$ is called an eigenvalue of $T$ if there is a nontrivial solution $x$ of $T(x) = \\lambda x$; <br>\n",
    "such an $x$ is called an eigenvector corresponding to $\\lambda$ <br>\n",
    "\n",
    "The Matrix of a Linear Transformation\n",
    "- Let $V$ be an $n$-dimensional vector space and let $T$ be any lienar transformation from $V$ to $V$. <br>\n",
    "- Choose any basis $\\mathcal{B}$ for $V$. Then given any $x$ in $V$, the coordinate vector $[x]_\\mathcal{B}$ and its image $[T(x)]_\\mathcal{B}$ are in $\\mathbb{R}^n$ <br>\n",
    "- If $[x]_\\mathcal{B} = \\begin{bmatrix} r_1 \\\\ \\vdots \\\\ r_n \\end{bmatrix}$, then $T(x) = T(r_1b_1 + \\cdots + r_nb_n) = r_1T(b_1) + \\cdots + r_nT(b_n)$ because $T$ is linear\n",
    "- Thus, by Theorem 8 in Chapter 4, since the coordinate mapping from $V$ to $\\mathbb{R}^n$ is linear, <br>\n",
    "  $[T(x)]_\\mathcal{B} = [r_1T(b_1) + \\cdots + r_nT(b_n)]_\\mathcal{B} = r_1[T(b_1)]_\\mathcal{B} + \\cdots + r_n[T(b_n)]_\\mathcal{B}$\n",
    "- Since $\\mathcal{B}$-coordinate vectors are in $\\mathbb{R}^n$, we can write $[T(x)]_\\mathcal{B} = M[x]_\\mathcal{B}$ <br>\n",
    "  where $M = \\begin{bmatrix} [T(b_1)]_\\mathcal{B} & [T(b_2)]_\\mathcal{B} & \\cdots & [T(b_n)]_\\mathcal{B} \\end{bmatrix}$, called the matrix for $T$ relative to the basis $\\mathcal{B}$ and denoted by $[T]_\\mathcal{B}$\n",
    "\n",
    "The mapping $T : \\mathbb{P}_2 \\to \\mathbb{P}_2$ defined by $T(a_0 + a_1t + a_2t^2) = a_1 + 2a_2t$ is a linear transformation. Let $x = a_0 + a_1t + a_2t^2$\n",
    "1. $x \\to [x]_\\mathcal{B} \\ $ ( $\\mathbb{P}^2 \\to \\mathbb{R}^3$ ) : Let $\\mathcal{B} = \\{1, t, t^2\\}$, then $[a_0 + a_1t + a_2t^2]_\\mathcal{B} = \\begin{bmatrix} a_0 \\\\ a_1 \\\\ a_2 \\end{bmatrix}$\n",
    "2. $[x]_\\mathcal{B} \\to [T(x)]_\\mathcal{B}$ : $[T(x)]_\\mathcal{B} = [T]_\\mathcal{B} [x]_\\mathcal{B} \\ $ where $[T]_\\mathcal{B} = \\begin{bmatrix} [T(1)]_\\mathcal{B} & [T(t)]_\\mathcal{B} & [T(t^2)]_\\mathcal{B} \\end{bmatrix} = \\begin{bmatrix} 0 & 1 & 0 \\\\ 0 & 0 & 2 \\\\ 0 & 0 & 0 \\end{bmatrix}$  <br>\n",
    "   so $[T(x)]_\\mathcal{B} = \\begin{bmatrix} 0 & 1 & 0 \\\\ 0 & 0 & 2 \\\\ 0 & 0 & 0 \\end{bmatrix} \\begin{bmatrix} a_0 \\\\ a_1 \\\\ a_2 \\end{bmatrix} = \\begin{bmatrix} a_1 \\\\ 2a_2 \\\\ 0 \\end{bmatrix}$\n",
    "3. $[T(x)]_\\mathcal{B} \\to T(x) \\ $ ( $\\mathbb{R}^3 \\to \\mathbb{P}^2$ ) : $T(x) = \\begin{bmatrix} 1 & t & t^2 \\end{bmatrix} [T(x)]_\\mathcal{B} = a_1 + 2a_2 t$\n",
    "\n",
    "Theorem 8 : Suppose $A = PCP^{-1}$ where $C$ is $n \\times n$, and $\\mathcal{B}$ is the basis for $\\mathbb{R}^n$ formed from the columns of $P$\n",
    "- Then, $C$ is the $\\mathcal{B}$-matrix for the transformation $x \\mapsto Ax$\n",
    "  - Let $\\mathcal{B} = \\{b_1,...,b_n\\}$, then $P = \\begin{bmatrix} b_1 & \\cdots & b_n \\end{bmatrix}$, which is the change-of-coordinates matrix <br>\n",
    "    So $P[x]_\\mathcal{B} = x$ and $[x]_\\mathcal{B} = P^{-1}x$\n",
    "  - If $T(x) = Ax$ for $x$ in $\\mathbb{R}^n$, then $[T]_\\mathcal{B} = \\begin{bmatrix} [T(b_1)]_\\mathcal{B} & \\cdots & [T(b_n)]_\\mathcal{B} \\end{bmatrix} = \\begin{bmatrix} [Ab_1]_\\mathcal{B} & \\cdots & [Ab_n]_\\mathcal{B} \\end{bmatrix}$ <br>\n",
    "    $= \\begin{bmatrix} P^{-1} Ab_1 & \\cdots & P^{-1} Ab_n \\end{bmatrix} = P^{-1} A \\begin{bmatrix} b_1 & \\cdots & b_n \\end{bmatrix} = P^{-1}AP$\n",
    "  - Since $A = PCP^{-1}$, we have $[T]_\\mathcal{B} = P^{-1}AP = C$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140a61eb",
   "metadata": {},
   "source": [
    "### 5) Complex Eigenvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a493ec",
   "metadata": {},
   "source": [
    "The matrix eigenvalue-eigenvector theory developed for $\\mathbb{R}^n$ applies equally well to $\\mathbb{C}^n$ <br>\n",
    "\n",
    "The real and imaginary parts of a complex vector $x$ are the vectors $\\mathrm{Re}\\ x$ and $\\mathrm{Im}\\ x$ : $x = \\mathrm{Re}\\ x + i \\mathrm{Im}\\ x$ <br>\n",
    "\n",
    "If $B$ is an $m \\times n$ matrix with possibly complex entires, then $\\bar{B}$ denotes the matrix <br>\n",
    "whose entries are the complex conjugates of the entries in $B \\ \\ $ ( $a + bi \\rightarrow a - bi$ ) <br>\n",
    "Let $u = a + bi$ and $v = c + di$, then $\\bar{uv} = \\bar{ac-bd + (ad+bc)i} = ac-bd - (ad+bc)i = (a - bi)(c - di) = \\bar{u}\\bar{v}$. <br>\n",
    "Thus, let $r$ be a complex number and $x$ any vector, then $\\bar{rx} = \\bar{r}\\bar{x}$, and $\\bar{Bx} = \\bar{B}\\bar{x}$ <br>\n",
    "Let $A$ be an $n \\times n$ matrix whose entries are real. Then $\\bar{A} = A$ <br>\n",
    "So, If $\\lambda$ is an eigenvalue of $A$ and $x$ is a corresponding eigenvector in $\\mathbb{C}$, <br>\n",
    "then $A\\bar{x} = \\bar{A}\\bar{x} = \\bar{Ax} = \\bar{\\lambda x} = \\bar{\\lambda}\\bar{x}$. Hence $\\bar{\\lambda}$ is also an eigenvalue of $A$ corresponding to eigenvector $\\bar{x}$ <br>\n",
    "This shows that when $A$ is real, its complex eigenvalues occur in conjugate pairs <br>\n",
    "\n",
    "Theorem 9\n",
    "- Let $A$ be real $2 \\times 2$ matrix with a complex eigenvalue $\\lambda = a - bi \\ \\ (b \\neq 0)$ and an associated eigenvector $v$ in $\\mathbb{C}^2$ <br>\n",
    "  Then $A = PCP^{-1}$, where $P = \\begin{bmatrix} \\mathrm{Re}\\ v & \\mathrm{Im}\\ v \\end{bmatrix}$ and $C = \\begin{bmatrix} a & -b \\\\ b & a \\end{bmatrix}$\n",
    "  - $Av = \\mathrm{Re}\\ Av + i \\mathrm{Im}\\ Av$, and, $Av = A ( \\mathrm{Re}\\ v + i \\mathrm{Im}\\ v ) = A\\ \\mathrm{Re}\\ v + A\\ i \\mathrm{Im}\\ v$ <br>\n",
    "    So, (1) : $\\mathrm{Re}\\ Av = A ( \\mathrm{Re}\\ x )$, and (2) : $\\mathrm{Im}\\ Av = A ( \\mathrm{Im}\\ x )$\n",
    "  - $Av = \\mathrm{Re}\\ Av + i \\mathrm{Im}\\ Av$, and, $Av = \\lambda v = (a - ib)(\\mathrm{Re}\\ v + i \\mathrm{Im}\\ v) = (a\\ \\mathrm{Re}\\ v + b\\ \\mathrm{Im}\\ v) + i (- b\\ \\mathrm{Re}\\ v + a\\ \\mathrm{Im}\\ v)$ <br>\n",
    "    So, (3) : $\\mathrm{Re}\\ Av = a\\ \\mathrm{Re}\\ v + b\\ \\mathrm{Im}\\ v$, and (4) : $\\mathrm{Im}\\ Av = - b\\ \\mathrm{Re}\\ v + a\\ \\mathrm{Im}\\ v$\n",
    "  - $P = \\begin{bmatrix} \\mathrm{Re}\\ v & \\mathrm{Im}\\ v \\end{bmatrix}$ is invertible\n",
    "    - Let's assume that $\\mathrm{Re}\\ v$ and $\\mathrm{Im}\\ v$ are linearly dependent\n",
    "    - If $\\mathrm{Re}\\ v = 0$, then (1) $\\Rightarrow \\mathrm{Re}\\ Av = 0$, thus (3) $\\Rightarrow \\mathrm{Im}\\ v = 0$, which is contradiction\n",
    "    - If $\\mathrm{Re}\\ v \\neq 0$, then there is scalar $c$ in $\\mathbb{R}$ such that $\\mathrm{Im}\\ v = c\\ \\mathrm{Re}\\ v$ <br>\n",
    "        So, (3) $\\Rightarrow \\mathrm{Re}\\ Av = (a + bc)\\ \\mathrm{Re}\\ v$, and (4) $\\Rightarrow c\\ \\mathrm{Re}\\ Av = (- b + ac)\\ \\mathrm{Re}\\ v$ <br>\n",
    "      Therefore, $c \\cdot$(3) $=$ (4) : $c(a + bc) = - b + ac \\Rightarrow b(1 + c^2) = 0$, which implies $b = 0$, which is contradiction\n",
    "    - Therefore, $\\mathrm{Re}\\ v$ and $\\mathrm{Im}\\ v$ are linearly independent, which means that $P$ is invertible\n",
    "  - $AP = \\begin{bmatrix} A\\ \\mathrm{Re}\\ v & A\\ \\mathrm{Im}\\ v \\end{bmatrix}$, and by (1) and (2), $= \\begin{bmatrix} \\mathrm{Re}\\ Av & \\mathrm{Im}\\ Av \\end{bmatrix}$, <br>\n",
    "    and by (3) and (4), $= \\begin{bmatrix} a\\ \\mathrm{Re}\\ v + b\\ \\mathrm{Im}\\ v & - b\\ \\mathrm{Re}\\ v + a\\ \\mathrm{Im}\\ v \\end{bmatrix}$\n",
    "  - $PC = \\begin{bmatrix} \\mathrm{Re}\\ v & \\mathrm{Im}\\ v \\end{bmatrix} \\begin{bmatrix} a & -b \\\\ b & a \\end{bmatrix} = \\begin{bmatrix} a\\ \\mathrm{Re}\\ v + b\\ \\mathrm{Im}\\ v & - b\\ \\mathrm{Re}\\ v + a\\ \\mathrm{Im}\\ v \\end{bmatrix}$\n",
    "  - Therefore, Since $AP = PC$ and $P$ is invertible, $A = PCP^{-1}$\n",
    "- $P$ is viewed as change-of-coordinates matrix (like Section 4 in Chapter 5) <br>\n",
    "  and $x \\mapsto Cx$ is viewed as the composition of a rotation and a scaling by $|\\lambda|$\n",
    "  - $C = |\\lambda| \\begin{bmatrix} a/|\\lambda| & -b/|\\lambda| \\\\ b/|\\lambda| & a/|\\lambda| \\end{bmatrix} = |\\lambda| \\begin{bmatrix} \\cos \\varphi  & - \\sin \\varphi \\\\ \\sin \\varphi & \\cos \\varphi \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74bacb6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.axis([-3, 3, -3, 3]), plt.axhline(y=0, color='black'); plt.axvline(x=0, color='black')\n",
    "A = np.array([[0.5, -0.6], [0.75, 1.1]])\n",
    "Re_v = np.array([-2, 5]); Im_v = np.array([-4, 0]) # v = Re v + i Im v, is eigenvector of A \n",
    "plt.plot(np.array([-3, 3]), np.array([-3, 3]) * (Re_v[1]/Re_v[0]), linewidth = 5, color = 'b')\n",
    "plt.plot(np.array([-3, 3]), np.array([-3, 3]) * (Im_v[1]/Im_v[0]), linewidth = 5, color = 'r')\n",
    "x = np.array([2, 0])\n",
    "plt.plot(x[0], x[1], 'o')\n",
    "\n",
    "for _ in range(100):\n",
    "    plt.plot(x[0], x[1], 'o', color='k')\n",
    "    x = np.dot(A, x)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fd8b0d",
   "metadata": {},
   "source": [
    "### 6) Discrete Dynamical Systems\n",
    "\n",
    "( = Dynamical system in Section 2 in Chapter 4 ) <br>\n",
    "\n",
    "At the dynamical system $x_{k+1} = A x_k \\Rightarrow x_k = c_1(\\lambda_1)^kv_1 + \\cdots + c_n(\\lambda_n)^kv_n$:\n",
    "- trajectory : The graph of $x_0 = \\begin{bmatrix} c_1 \\\\ \\vdots \\\\ c_n \\end{bmatrix}, x_1, x_2, ...$\n",
    "  - When both eigenvalues are less than 1 in magnitude so all trajectories tend toward $0$, the origin is called an attractor\n",
    "  - When both eigenvalues are larger than 1 so all trajectories tend away from the origin, the origin is called a repeller\n",
    "  - Otherwise, so the origin attracts from some directions and repel them in other directions, the origin is called saddle point\n",
    "- If $A$ is diagonalizable so $A = PDP^{-1}$, let $y_k = P^{-1}x_k$, which means $y_k$ is the coordinate vector of $x_k$ with respect to $\\{v_1,...,v_n\\}$ <br>\n",
    "  than $Py_{k+1} = x_{k+1} = A x_k = AP y_k = (PDP^{-1})Py_k = PDy_k \\Rightarrow y_{k+1} = D y_k$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6435b322",
   "metadata": {},
   "source": [
    "### 7) Applications to Differential Equations\n",
    "\n",
    "( Section 8 in Chapter 4 + Dynamical system ) <br>\n",
    "\n",
    "System of differential equation : $x'(t) = A x(t)$ <br>\n",
    "where $x(t) = \\begin{bmatrix} x_1(t) \\\\ \\vdots \\\\ x_n(t) \\end{bmatrix}$, $x'(t) = \\begin{bmatrix} x_1'(t) \\\\ \\vdots \\\\ x_n'(t) \\end{bmatrix}$, and $A = \\begin{bmatrix} a_{11} & \\cdots & a_{1n} \\\\ \\vdots & \\ddots & \\vdots \\\\ a_{n1} & \\cdots & a_{nn} \\end{bmatrix}$\n",
    "- A solution of the equation is a vector-valued function that satisfies the equation for all $t$ in some interval of **real numbers**\n",
    "- Let $x(t) = ve^{\\lambda t}$, then $x'(t) = \\lambda v e^{\\lambda t}$. On the other hand, $Ax(t) = Ave^{\\lambda t}$, so $x'(t) = A x(t) \\Rightarrow \\lambda v e^{\\lambda t} = A v e^{\\lambda t}$ <br>\n",
    "  So, if $v$ is eigenvector corresponding to eigenvalue $\\lambda$, then $x(t) = ve^{\\lambda t}$ is the solution of the equation, called eigenfunction\n",
    "- Suppose $A$ has $n$ linearly independent eigenvectors $v_1, ..., v_n$ corresponding to $\\lambda_1, ..., \\lambda_n$, <br>\n",
    "  then eigenfunctions $x_1(t) = v_1e^{\\lambda_1 t}$, ..., $x_n(t) = v_ne^{\\lambda_n t}$ are linearly independent, so form a basis of $\\mathbb{R}^n$\n",
    "- Therefore, we can find general solution $x(t) = c_1v_1e^{\\lambda_1 t} + \\cdots + c_nv_ne^{\\lambda_n t}$ if $x(0)$ is given\n",
    "\n",
    "Suppose $A$ has a pair of complex eigenvalues $\\lambda$ and $\\bar{\\lambda}$, with associated complex eigenvectors $v$ and $\\bar{v}$, <br>\n",
    "so two **complex** solutions of $x' = Ax$ are $x_1(t) = ve^{\\lambda t}$ and $x_2(t) = \\bar{v}e^{\\bar{\\lambda} t}$ <br>\n",
    "Let $\\lambda = a + bi$, then by Euler's formula, $e^{\\lambda t} = e^{at}e^{ibt} = e^{at}(\\cos bt + i \\sin bt)$, <br>\n",
    "and $e^{\\bar{\\lambda} t} = e^{at}e^{-ibt} = e^{at}(\\cos (-bt) + i \\sin (-bt)) = e^{at}(\\cot bt - i \\sin bt)$, thus $\\bar{e^{\\lambda t}} = e^{\\bar{\\lambda} t}$ <br>\n",
    "Therefore $x_2(t) = \\bar{v}e^{\\bar{\\lambda} t} = \\bar{v}\\bar{e^{\\lambda t}} = \\bar{v e^{\\lambda t}} = \\bar{x_1(t)}$, which means that $\\bar{x_1(t)}$ is also the solution of $x' = Ax$ <br>\n",
    "Thus, the function $\\mathrm{Re}(x_1(t)) = \\frac{1}{2}[x_1(t) + \\bar{x_1(t)}]$ and $\\mathrm{Im}(x_1(t)) = \\frac{1}{2i}[x_1(t) - \\bar{x_1(t)}]$ are the **real** solutions of $x' = Ax$<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40beafcf",
   "metadata": {},
   "source": [
    "### 8) Iterative Estimates for Eigenvalues\n",
    "\n",
    "Assume that $A$ has $n$ linearly independent eigenvectors $v_1,...,v_n$ corresponding to eigenvalues $|\\lambda_1| > |\\lambda_2| \\geq |\\lambda_3| \\geq \\cdots \\geq |\\lambda_n|$ <br>\n",
    "If $x$ in $\\mathbb{R}^n$ is written as $x = c_1v_1 + \\cdots + c_nv_n$, then $A^kx = c_1(\\lambda_1)^kv_1 + c_2(\\lambda_2)^kv_2 + \\cdots + c_n(\\lambda_n)^kv_n$ <br>\n",
    "Assume $c_1 \\neq 0$, then $\\frac{1}{(\\lambda_1)^k} A^k x = c_1v_1 + c_2 (\\frac{\\lambda_2}{\\lambda_1})^kv_2 + \\cdots + c_n (\\frac{\\lambda_n}{\\lambda_1})^kv_n$. then $\\frac{\\lambda_2}{\\lambda_1}, ..., \\frac{\\lambda_n}{\\lambda_1}$ are all less than 1 <br>\n",
    "So $(\\lambda_1)^{-k} A^k x \\rightarrow c_1v_1$ as $k \\rightarrow \\infty$, which implies that for large $k$, $A^k x$ determines almost the same direction as the eigenvector $v_1$ <br>\n",
    "\n",
    "Apply the power method to $A = \\begin{bmatrix} 6 & 5 \\\\ 1 & 2 \\end{bmatrix}$\n",
    "1. Select an initial vector $x_0$ whose largest entry is $1$, I will select $x_0 = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$\n",
    "2. For $k = 0, 1, ...$, compute $x_{k+1} = \\frac{1}{\\mu_k}Ax_k$ where $\\mu_k$ is an entry in $Ax_k$ whose absolute value is as large as possible\n",
    "  - Since $Ax_0 = \\begin{bmatrix} 6 & 5 \\\\ 1 & 2 \\end{bmatrix} \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 5 \\\\ 2 \\end{bmatrix}$, $\\mu_0 = 5$, so $x_1 = \\frac{1}{5} \\begin{bmatrix} 5 \\\\ 2 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 0.4 \\end{bmatrix}$\n",
    "  - Since $Ax_1 = \\begin{bmatrix} 6 & 5 \\\\ 1 & 2 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 0.4 \\end{bmatrix} = \\begin{bmatrix} 8 \\\\ 1.8 \\end{bmatrix}$, $\\mu_1 = 8$, so $x_2 = \\frac{1}{8} \\begin{bmatrix} 8 \\\\ 1.8 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 0.225 \\end{bmatrix}$\n",
    "  - If we repeat this, we can get $x_3 = \\begin{bmatrix} 1 \\\\ 0.2035 \\end{bmatrix}$, $x_4 = \\begin{bmatrix} 1 \\\\ 0.2005 \\end{bmatrix}$, $x_5 = \\begin{bmatrix} 1 \\\\ 0.20007 \\end{bmatrix}$, ... <br>\n",
    "    with $\\mu_3 = 7.125$, $\\mu_4 = 7.0175$, $\\mu_5 = 7.0025$, ...\n",
    "3. For almost all choices of $x_0$, $\\{\\mu_k\\}$ approaches the dominant eigenvalue, and $\\{x_k\\}$ approaches a corresponding eigenvector <br>\n",
    "   We can suggest that $\\{x_k\\}$ approaches $(1, 0.2)$ and $\\{y_k\\}$ approaches $7$. <br>\n",
    "   If so, then $(1, 0.2)$ is an eigenvector and $7$ is the dominant eigenvalue. <br>\n",
    "   we can check $A \\begin{bmatrix} 1 \\\\ 0.2 \\end{bmatrix} = \\begin{bmatrix} 6 & 5 \\\\ 1 & 2 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 0.2 \\end{bmatrix} = \\begin{bmatrix} 7 \\\\ 1.4 \\end{bmatrix} = 7 \\begin{bmatrix} 1 \\\\ 0.2 \\end{bmatrix}$\n",
    "- We can change $\\mu_k$ to $|Ax_k|$. than $|x_k| = 1 \\ \\ $ (inner product)\n",
    "\n",
    "If $Av = \\lambda v$ and $\\alpha$ is not eigenvalue of $A$, than $(A - \\alpha I)^{-1} v = \\frac{1}{\\lambda - \\alpha} v$ because $(A - \\alpha I) v = (\\lambda - a) v$ <br>\n",
    "Therefore, If $v_1,...,v_n$ are linearly independent eigenvectors of $A$, then so is $(A - \\alpha I)^{-1}$ <br>\n",
    "and corresponding eigenvalues are $\\frac{1}{\\lambda_1 - \\alpha}, \\cdots, \\frac{1}{\\lambda_n - \\alpha}$ <br>\n",
    "If we apply the power method to $(A - \\alpha I)^{-1}$, than we will find the strictly dominant eigenvalue, formed by $\\frac{1}{\\lambda - \\alpha}$, <br>\n",
    "which means that we will find $\\lambda$ the eigenvalue of $A$ which is nearest to $\\alpha$\n",
    "\n",
    "Apply the inverse power method to $A = \\begin{bmatrix} 10 & -8 & -4 \\\\ -8 & 13 & 4 \\\\ -4 & 5 & 4 \\end{bmatrix}$\n",
    "1. Select an initial estimate $\\alpha$ sufficiently close to $\\lambda$. I will select $\\alpha = 1.9$\n",
    "2. Select an inttial vector $x_0$ whose largest entry is $1$. I will select $x_0 = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix}$\n",
    "3. For $k = 0, 1, ...$, solve $(A - \\alpha I)y_k = x_k$ for $y_k$ because it is better than computing $y_k = (A - \\alpha I)^{-1} x_k$, <br>\n",
    "   and compute $v_k = \\alpha + (1/\\mu)$ to find the eigenvalue of $A$, and compute $x_{k+1} = (1/\\mu_k)y_k$\n",
    "   - If we repeat this, we can get $x_1 = \\begin{bmatrix} 0.5736 \\\\ 0.0646 \\\\ 1 \\end{bmatrix}$, ..., $x_4 = \\begin{bmatrix} 0.50003 \\\\ 0.00002 \\\\ 1 \\end{bmatrix}$, ... <br>\n",
    "     with $v_1 = 2.03$, ..., $v_4 = 2.0000002$, ...\n",
    "4. For almost all choices of $x_0$, $\\{v_k\\}$ approaches the eigenvalue of $A$, and $\\{x_k\\}$ approaches a corresponding eigenvector <br>\n",
    "   So we can find the eigenvalue of $A$ is $2$, and corresponding eigenvector is $\\begin{bmatrix} 0.5 \\\\ 0 \\\\ 1 \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225a4116",
   "metadata": {},
   "source": [
    "### 9) Applications to Markov Chains\n",
    "\n",
    "A vector with nonnegative entries that add up to $1$ is called a probability vector <br>\n",
    "A stochastic matrix is a square matrix whose columns are probability vectors <br>\n",
    "A Markov chain is a sequence of probability (state) vectors $x_0, x_1, ...$ together with a stochastic matrix $P$, <br>\n",
    "such that $x_1 = Px_0$, $x_2 = Px_1$, $\\cdots$. Thus it is described by the first-order difference equation $x_{k+1} = Px_k$ for $k = 0, 1, ...$ <br>\n",
    "\n",
    "Theorem 10 : If $P$ is a stochastic matrix, then $1$ is an eigenvalue of $P$\n",
    "- Since the colums of $P$ sum to $1$, the rows of $P^T$ will also sum to $1$\n",
    "- Let $e$ represent the vector for which every entry is $1$, then $P^T e = e$, <br>\n",
    "  hence $e$ is an eigenvector of $P^T$ with eigenvalue $1$\n",
    "- Since $\\det (P^T - \\lambda I) = \\det ((P - \\lambda I)^T) = \\det (P - \\lambda I)$ <br>\n",
    "  $P$ and $P^T$ have the same eigenvalues, therefore $1$ is also an eigenvalue of $P$\n",
    "\n",
    "stready-state vector for $P$ : probability vector $q$ such that $Pq = q$, which is eigenvector of $P$ corresponding to eigenvalue $1$ <br>\n",
    "$P$ is regular : some matrix power $P^k$ contains only strictly positive entries <br>\n",
    "\n",
    "Theorem 11  ( proof is in Chapter 10 )\n",
    "- If $P$ is an $n \\times n$ regular stochastic matrix, then $P$ has a unique steady-state vector $q$\n",
    "- Further, if $x_0$ is any initial state and $x_{k+1} = Px_k$ for $k = 0, 1, ...$, then the Markov chain $\\{x_k\\}$ converges to $q$ as $k \\to \\infty$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fed4d6",
   "metadata": {},
   "source": [
    "*****"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94165f2b",
   "metadata": {},
   "source": [
    "## Chapter 6 : Orthogonality and Least Squares"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5151fb2",
   "metadata": {},
   "source": [
    "### 1) Inner Product, Length, and Orthogonality\n",
    "\n",
    "$u^T v$ is called the inner product of $u$ and $v$, and often it is written as $u \\cdot v$, so is also referred to as a dot product <br>\n",
    "Theroem 1 : Let $u$, $v$, and $w$ be vectors in $\\mathbb{R}^n$, and let $c$ be a scalar. Then:\n",
    "- $u \\cdot v = v \\cdot u$\n",
    "- $(u+v) \\cdot w = u \\cdot w + v \\cdot w$\n",
    "- $(cu) \\cdot v = c(u \\cdot v) = u \\cdot (cv)$\n",
    "- $u \\cdot u \\geq 0$, and $u \\cdot u = 0$ if and only if $u = 0$\n",
    "\n",
    "The length (or norm) of $v$ is the nonnegative scalar $||v|| = \\sqrt{v \\cdot v} = \\sqrt{v_1^2 + v_2^2 + \\cdots + v_n^2}$, and $||v||^2 = v \\cdot v$ <br>\n",
    "- normalizing $v$ : The process of creating unit vector ( $u$ where $||u|| = 1$ ) from $v$\n",
    "\n",
    "For $u$ and $v$ in $\\mathbb{R}^n$, the distance between $u$ and $v$, written as dist($u$, $v$), is $||u - v||$ <br>\n",
    "Two vectors $u$ and $v$ in $\\mathrm{R}^n$ are orthogonal (to each other) if $u \\cdot v = 0$\n",
    "\n",
    "Theorem 2 : Two vectors $u$ and $v$ are orthogonal if and only if $||u+v||^2 = ||u||^2 + ||v||^2$\n",
    "- $||u+v||^2 = (u+v) \\cdot (u+v) = u \\cdot u + u \\cdot v + v \\cdot u + v \\cdot v = ||u||^2 + 2 u \\cdot v + ||v||^2$\n",
    "- If $u$ and $v$ are orthogonal, then $u \\cdot v = 0$, therefore $||u+v||^2 = ||u||^2 + ||v||^2$ <br>\n",
    "  If $||u+v||^2 = ||u||^2 + ||v||^2$, then $u \\cdot v = 0$, therefore $u$ and $v$ are orthogonal\n",
    "\n",
    "If a vector $z$ is orthogonal to every vector in a subspace $W$ of $\\mathbb{R}^n$, then $x$ is said to be orthogonal to $W$ <br>\n",
    "The set of all vectors $z$ that are orthogonal to $W$ is called the orthogonal complement of $W$ and is denoted by $W^\\bot$ <br>\n",
    "\n",
    "$W^\\bot$ is a subspace of $\\mathbb{R}^n$\n",
    "- $0 \\cdot x = 0$ for all $x$ in $W$, so zero vector is in $W^\\bot$\n",
    "- If $u$ and $v$ are in $W^\\bot$, then for all $x$ in $W$, $(u+v) \\cdot x = u \\cdot x + v \\cdot x = 0$. Therefore $u+v$ is in $W^\\bot$\n",
    "- If $u$ is in $W^\\bot$, then for all $x$ in $W$ and scalar $c$, $(cu) \\cdot x = c(u \\cdot x) = 0$. Therefore $cu$ is in $W^\\bot$\n",
    "\n",
    "Theorem 3 : Let $A$ be an $m \\times n$ matrix\n",
    "- $(\\mathrm{Row}\\ A)^\\bot = \\mathrm{Nul}\\ A$\n",
    "  - If $x$ is in $\\mathrm{Nul}\\ A$, then $Ax = 0$, which means that $x$ is orthogonal to each row of $A$, so $x$ is in $(\\mathrm{Row}\\ A)^\\bot$\n",
    "  - If $x$ is in $(\\mathrm{Row}\\ A)^\\bot$, then $x$ is orthogonal to each row of $A$, hence $Ax = 0$, so $x$ is in $\\mathrm{Nul}\\ A$\n",
    "- $(\\mathrm{Col}\\ A)^\\bot = \\mathrm{Nul}\\ A^T$\n",
    "  - Since $(\\mathrm{Row}\\ A)^\\bot = \\mathrm{Nul}\\ A$ for any matrix, $(\\mathrm{Row}\\ A^T)^\\bot = \\mathrm{Nul}\\ A^T$ <br>\n",
    "    and since $\\mathrm{Row}\\ A^T = \\mathrm{Col}\\ A$, $(\\mathrm{Row}\\ A^T)^\\bot = (\\mathrm{Col}\\ A)^\\bot = \\mathrm{Nul}\\ A^T$\n",
    "    \n",
    "Let $W$ be a subspace of $\\mathbb{R}^n$, then $\\dim W + \\dim W^\\bot = n$\n",
    "- Suppose $W$ has a basis $\\{v_1,...,v_p\\}$, and let $A$ be the $p \\times n$ matrix having rows $v_1^T,...,v_p^T$ <br>\n",
    "  Then $W = \\mathrm{Row}\\ A$, and by Theorem 3, $W^T = (\\mathrm{Row}\\ A)^T = \\mathrm{Nul}\\ A$\n",
    "- Therefore $\\dim W + \\dim W^T = \\dim \\mathrm{Row}\\ A + \\dim \\mathrm{Nul}\\ A = \\mathrm{rank}\\ A + \\mathrm{nullity}\\ A = n\\ $ by the Rank Theorem\n",
    "  - by Theorem 7 in Chapter 4, nonzero rows of reduced row echelon form of $A$ are basis of $\\mathrm{Row}\\ A$\n",
    "  - Meanwhile, each nonzero rows of reduced row echelon form of $A$ has pivot position\n",
    "  - Therefore $\\dim \\mathrm{Row}\\ A = \\dim \\mathrm{Col}\\ A = \\mathrm{rank}\\ A$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a72eb5",
   "metadata": {},
   "source": [
    "### 2) Orthogonal Sets\n",
    "\n",
    "A set of vectors $\\{u_1,...,u_p\\}$ in $\\mathbb{R}^n$ is orthogonal set : $u_i \\cdot u_j = 0$ whenever $i \\neq j$ <br>\n",
    "\n",
    "Theorem 4\n",
    "- If $S = \\{u_1,...,u_p\\}$ is an orthogonal set of nonzero vectors in $\\mathbb{R}^n$ <br>\n",
    "  then $S$ is linearly independent and hence is a basis for the subspace spanned by $S$\n",
    "  - If $0 = c_1u_1 + \\cdots + c_pu_p$ for some scalar $c_1,...,c_p$, then <br>\n",
    "    $0 = 0 \\cdot u_1 = (c_1u_1 + \\cdots + c_pu_p) \\cdot u_1 = (c_1u_1) \\cdot u_1 + \\cdots + (c_pu_p) \\cdot u_1$ <br>\n",
    "    $= c_1(u_1 \\cdot u_1) + \\cdots + c_p(u_p \\cdot u_1) = c_1(u_1 \\cdot u_1)$ because $u_1$ is orthogonal to $u_2, ..., u_p$\n",
    "  - since $u_1$ is nonzero, $u_1 \\cdot u_1$ is not zero and so $c_1 = 0$, and similiarly we can show $c_2, ..., c_p$ are zero <br>\n",
    "    Therefore $S$ is linearly independent\n",
    "\n",
    "An orthogonal basis for a subspace $W$ of $\\mathbb{R}^n$ is a basis for $W$ that is also an orthogonal set <br>\n",
    "\n",
    "Theorem 5 : Let $\\{u_1,...,u_p\\}$ be an orthogonal basis for a subspace $W$ of $\\mathbb{R}^n$\n",
    "- For each $y$ in $W$, the weights in the linear combination $y = c_1u_1 + \\cdots + c_pu_p$ are given by $c_j = \\frac{y \\cdot u_j}{u_j \\cdot u_j}$\n",
    "  - $y \\cdot u_j = (c_1u_1 + c_2u_2 + \\cdots + c_pu_p) \\cdot u_j = c_j (u_j \\cdot u_j)$, therefore $c_j = \\frac{y \\cdot u_j}{u_j \\cdot u_j}$ \n",
    "\n",
    "Given a nonzero vector $u$ in $\\mathbb{R}^n$, consider the problem of decomposing a vector $y$ in $\\mathbb{R}^n$ into the sum of two vectors, <br>\n",
    "one a multiple of $u$ and the other orthogonal to $u$: $y = \\hat{y} + z$ where $\\hat{y} = cu$ and $z$ is some vector orthgonal to $u$ <br>\n",
    "Then, since $z = y - cu$ is orthogonal to $u$, $0 = (y - cu) \\cdot u = y \\cdot u - c(u \\cdot u)$, thereforer $c = \\frac{y \\cdot u}{u \\cdot u}$, so $\\hat{y} = \\frac{y \\cdot u}{u \\cdot u}u$ <br>\n",
    "The $\\hat{y}$ is called the orthogonal projection of $y$ onto $u$, and the vector $z$ is called the component of $y$ orthgonal to $u$ <br>\n",
    "since the orthogonal projection of $y$ onto $cu$ is exactly the same as the orthogonal projection of $y$ onto $u$, <br>\n",
    "$\\hat{y}$ is determined by the subspace $L$ spanned by $u$, so denoted by $\\mathrm{proj}_Ly$, called the orthogonal projection of $y$ onto $L$ <br>\n",
    "\n",
    "A set $\\{u_1,...,u_p\\}$ is an orthonormal set if it is an orthogonal set of unit vectors <br>\n",
    "If $W$ is the subspace sapnned by such a set, then $\\{u_1,...,u_p\\}$ is an orthonormal basis for $W$ <br>\n",
    "\n",
    "Theorem 6 : An $m \\times n$ matrix $U$ has orthonormal columns if and only if $U^TU = I$\n",
    "- Let $U = \\begin{bmatrix} u_1 & \\cdots & u_n \\end{bmatrix}$, then $U^TU = \\begin{bmatrix} u_1^T \\\\ \\vdots \\\\ u_n^T \\end{bmatrix} \\begin{bmatrix} u_1 & \\cdots & u_3 \\end{bmatrix} = \\begin{bmatrix} u_1^Tu_1 & \\cdots & u_1^Tu_n \\\\ \\vdots & \\ddots & \\vdots \\\\ u_n^Tu_1 & \\cdots & u_n^Tu_n \\end{bmatrix}$\n",
    "- Since $u_i \\cdot u_j = 0$ whenever $i \\neq j$, and $u_i \\cdot u_i = 1$ for all $i$, The theorem follows\n",
    "\n",
    "Theorem 7 : Let $U$ be an $m \\times n$ matrix with orthonormal columns, and let $x$ and $y$ be in $\\mathbb{R}^n$\n",
    "- $(Ux) \\cdot (Uy) = x \\cdot y$\n",
    "  - $(Ux) \\cdot (Uy) = (Ux)^T (Uy) = x^T U^T U y = x^T y = x \\cdot y\\ $ by Theorem 6\n",
    "- $||Ux|| = ||x||$\n",
    "  - $||Ux||^2 = (Ux) \\cdot (Ux) = x \\cdot x = ||x||^2$, thus $||Ux|| = ||x||$ because $||x|| = \\sqrt{x_1^2 + \\cdots + x_n^2} \\geq 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd40c6b",
   "metadata": {},
   "source": [
    "### 3) Orthogonal Projections\n",
    "\n",
    "Theorem 8 : The Orthogonal Decomposition Theorem\n",
    "- Let $W$ be a nonzero subspace of $\\mathbb{R}^n$. Then each $y$ in $\\mathbb{R}^n$ can be written uniquely in the form $y = \\hat{y} + z$ where $\\hat{y}$ is in $W$ and $z$ is in $W^\\bot$\n",
    "- In fact, if $\\{u_1,...,u_p\\}$ is any orthogonal basis of $W$, then $\\hat{y} = \\frac{y \\cdot u_1}{u_1 \\cdot u_1}u_1 + \\cdots + \\frac{y \\cdot u_p}{u_p \\cdot u_p}u_p$ and $z = y - \\hat{y}$\n",
    "  - $\\hat{y} = \\frac{y \\cdot u_1}{u_1 \\cdot u_1}u_1 + \\cdots + \\frac{y \\cdot u_p}{u_p \\cdot u_p}u_p$ is in $W$ because $\\hat{y}$ is a linear combination of the basis $u_1,...,u_p$\n",
    "  - $z \\cdot u_i = (y - \\hat{y}) \\cdot u_i = y \\cdot u_1 - 0 - \\cdots - (\\frac{y \\cdot u_1}{u_1 \\cdot u_1})u_1 \\cdot u_1 - \\cdots - 0 = y \\cdot u_1 - y \\cdot u_1 = 0$ <br>\n",
    "    Therefore $z$ is orthogonal to $u_i$ for all $i$, that is, $z$ is in $W^\\bot$\n",
    "  - To show that the decomposition is unique, suppose $y$ can also be written as $y = \\hat{y_1} + z_1$ with $\\hat{y_1}$ in $W$ and $z_1$ in $W^\\bot$ <br>\n",
    "    Then $\\hat{y} + z = \\hat{y_1} + z_1$, so $\\hat{y} - \\hat{y_1} = z_1 - z$, but $\\hat{y} - \\hat{y_1}$ is in $W$, and $z_1 - z$ is in $W^\\bot$ <br>\n",
    "    Therefore, $\\hat{y} - \\hat{y_1}$ and $z_1 - z$ must be $0$ because $W$ and $W^\\bot$ are disjoint subspace, so $\\hat{y} = \\hat{y_1}$ and $z_1 = z$\n",
    "\n",
    "If $y$ is in $W = \\mathrm{Span}\\{u_1,...,u_]\\}$, then $\\mathrm{proj}_Wy = y$ <br>\n",
    "\n",
    "Theorem 9 : The Best Approximation Theorem\n",
    "- Let $W$ be a subspace of $\\mathbb{R}^n$, let $y$ be any vector in $\\mathbb{R}^n$, and let $\\hat{y}$ be the orthogonal projection of $y$ onto $W$ <br>\n",
    "  Then $\\hat{y}$ is the closest point in $W$ to $y$, in the sense that $||y - \\hat{y}|| < ||y - v||$ for all $v$ in $W$ distinct from $\\hat{y}$\n",
    "  - Take $v$ in $W$ distinct from $\\hat{y}$. Then since $\\hat{y} - v$ is in $W$, $y - \\hat{y}$ is orthogonal to $\\hat{y} - v$\n",
    "  - Therefore, since $y - v = (y - \\hat{y}) + (\\hat{y} - v)$, by Pythagorean Theorem, $||y - v||^2 = ||y - \\hat{y}||^2 + ||\\hat{y} - v||^2 > ||y - \\hat{y}||^2$ \n",
    "- $\\hat{y}$ is called the best approximation to $y$ by elements of $W$: The error $||y - v||$ is minimized when $v = \\hat{y}$\n",
    "\n",
    "Theorem 10\n",
    "- If $\\{u_1,...,u_p\\}$ is an orthonormal basis for a subspace $W$ of $\\mathbb{R}^n$, <br>\n",
    "  then $\\mathrm{proj}_Wy = (y \\cdot u_1)u_1 + (y \\cdot u_2)u_2 + \\cdots + (y \\cdot u_p)u_p$\n",
    "  - this follows immediately from Theorem 8 and $u_i \\cdot u_i = 1$ for all $i$\n",
    "- If $U = \\begin{bmatrix} u_1 & u_2 & \\cdots & u_p \\end{bmatrix}$, then $\\mathrm{proj}_Wy = UU^Ty$ for all $y$ in $\\mathbb{R}^n$\n",
    "  - $\\mathrm{proj}_Wy = (y \\cdot u_1)u_1 + \\cdots + (y \\cdot u_p)u_p = (u_1^Ty)u_1 + \\cdots (u_p^Ty)u_T = U(U^Ty) = UU^Ty$\n",
    "\n",
    "If $U$ is an $n \\times n$ matrix with orthonormal columns, then $U$ is an orthogonal matrix <br>\n",
    "The columns space $W$ is all of $\\mathbb{R}^n$, and thus $\\mathrm{proj}_Wy = UU^Ty = Iy = y$ for all $y$ in $\\mathbb{R}^n$ <br>\n",
    "( Also, since $UU^T = I$ and $U^TU = I$, $U^T = U^{-1}$ )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9300626a",
   "metadata": {},
   "source": [
    "### 4) The Gram-Schmidt Process\n",
    "\n",
    "Theroem 11 : The Gram-Schmidt Process\n",
    "- Given a basis $\\{x_1,...,x_p\\}$ for a nonzero subspace $W$ of $\\mathbb{R}^n$, <br>\n",
    "  define $v_1 = x_1$, and $v_2 = x_2 - \\frac{x_2 \\cdot v_1}{v_1 \\cdot v_1}v_1$, and $v_3 = x_3 - \\frac{x_3 \\cdot v_1}{v_1 \\cdot v_1}v_1 - \\frac{x_3 \\cdot v_2}{v_2 \\cdot v_2}v_2$, <br>\n",
    "  $\\cdots$, and $v_p = x_p - \\frac{x_p \\cdot v_1}{v_1 \\cdot v_1}v_1 - \\frac{x_p \\cdot v_2}{v_2 \\cdot v_2}v_2 - \\cdots - \\frac{x_p \\cdot v_{p-1}}{v_{p-1} \\cdot v_{p-1}}v_{p-1}$\n",
    "- Then $\\{v_1,...,v_p\\}$ is an orthogonal basis for $W$ <br>\n",
    "  In addition, $\\mathrm{Span}\\{v_1,...,v_k\\} = \\mathrm{Span}\\{x_1,...,x_k\\}$ for $1 \\le k \\le p$\n",
    "  - For $1 \\le k \\le p$, let $W_k = \\mathrm{Span}\\{x_1,...,x_k\\}$. Set $v_1 = x_1$, so that $\\mathrm{Span}\\{v_1\\} = \\mathrm{Span}\\{x_1\\}$\n",
    "  - Suppose, for some $k < p$, we have constructed $v_1,...,v_k$ so that $\\{v_1,...,v_k\\}$ is an orthogonal basis for $W_k$\n",
    "  - Define $v_{k+1} = x_{k+1} - \\mathrm{proj}_{W_k}x_{k+1}$, then by the Orthogonal Decomposition Theorem, $v_{k+1}$ is orthogonal to $W_k$ <br>\n",
    "    And since $W_{k+1}$ is subspace and $x_{k+1}$, $\\mathrm{proj}_{W_k}x_{k+1}$ are in $W_{k+1}$, $v_{k+1}$ is in $W_{k+1}$ <br>\n",
    "    And $v_{k+1} \\neq 0$ because $x_{k+1}$ is not in $W_k = \\mathrm{Span}\\{x_1,...,x_k\\}$\n",
    "  - Hence $\\{v_1,...,v_{k+1}\\}$ is an orthogonal set in $(k+1)$-dimensional space $W_{k+1}$, <br>\n",
    "    and thus $\\{v_1,...,v_{k+1}\\}$ is an orthogonal basis for $W_{k+1}$. When $k+1 = p$, the process stops \n",
    "\n",
    "Theorem 12 : The QR Factorization\n",
    "- If $A$ is an $m \\times n$ matrix with linearly independent columns, then $A$ can be factored as $A = QR$, where <br>\n",
    "  $Q$ is an $m \\times n$ matrix whose columns form an orthonormal basis for $\\mathrm{Col}\\ A$ and <br>\n",
    "  $R$ is an $n \\times n$ upper triangular invertible matrix with positive entries on its diagonal\n",
    "  - The columns of $A$ form a basis $\\{x_1,...,x_n\\}$ for $\\mathrm{Col}\\ A$ <br>\n",
    "    and using the Gram-Schmidt process, construct an orthonormal basis $\\{u_1,...,u_n\\}$ for $W = \\mathrm{Col}\\ A$\n",
    "  - For $k = 1,...,n$, $x_k$ is in $\\mathrm{Span}\\{x_1,...,x_k\\} = \\mathrm{Span}\\{u_1,...,u_k\\}$ <br>\n",
    "    So there are constants $r_{1k},...,r_{kk}$, such that $x_k = r_{1k}u_1 + \\cdots + r_{kk}u_k + 0 u_{k+1} + \\cdots + 0 u_n$ <br>\n",
    "    ( We may assume that $r_{kk} \\geq 0$. If $r_{kk} < 0$, multiply both $r_{kk}$ and $u_k$ by $-1$ )\n",
    "  - Let $Q = \\begin{bmatrix} u_1 & u_2 & \\cdots & u_n \\end{bmatrix}$, then $x_k$ is a linear combination of the columns of $Q$ <br>\n",
    "    using as weights the entries in the vector $r_k = \\begin{bmatrix} r_{1k} \\\\ \\vdots \\\\ r_{kk} \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix}$. That is, $x_k = Qr_k$ for $k = 1, ..., n$\n",
    "  - Let $R = \\begin{bmatrix} r_1 & \\cdots & r_n \\end{bmatrix}$. Then $A = \\begin{bmatrix} x_! & \\cdots & x_n \\end{bmatrix} = \\begin{bmatrix} Qr_1 & \\cdots & Qr_n \\end{bmatrix} = QR$ <br>\n",
    "    and since $Rx = 0 \\Rightarrow QRx = 0 \\Rightarrow Ax = 0$ and the columns of $A$ is linearly independent, $R$ is invertible\n",
    "- In the Gram-Schemidt Process, we can get $r_k$ by $x_k = v_k + \\frac{x_k \\cdot v_1}{v_1 \\cdot v_1}v_1 + \\cdots + \\frac{x_k \\cdot v_{k-1}}{v_{k-1} \\cdot v_{k-1}}v_{k-1}$\n",
    "- If $Q$ is already constructed, since $Q$ is orthogonal matrix, we can find $R = IR = Q^TQR = Q^TA$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd3374c",
   "metadata": {},
   "source": [
    "### 5) Least-Squares Problems\n",
    "\n",
    "If $A$ is $m \\times n$ and $b$ is in $\\mathbb{R}^m$, a least-squares solution of $Ax = b$ is an $\\hat{x}$ in $\\mathbb{R}^n$ such that $||b - A\\hat{x}|| \\le ||b - Ax||$ for all $x$ in $\\mathbb{R}^n$ <br>\n",
    "By Best Approximation Theorem, $\\hat{b} = \\mathrm{proj}_{\\mathrm{Col}\\ A}b$ is the closest point in $\\mathrm{Col}\\ A$ to $b$. Thus $\\hat{x}$ is existed and satisfies $A\\hat{x} = \\hat{b}$ <br>\n",
    "Since $b - \\hat{b}$ is orthogonal to $\\mathrm{Col}\\ A$, if $a_j$ is any column of $A$, then $a_j \\cdot (b - A\\hat{x}) = a_j^T (b - A\\hat{x}) = 0$  for all $j$ <br>\n",
    "Therefore $A^T(b - A\\hat{x}) = 0$, so $A^TA\\hat{x} = A^Tb$, Thus each least-squares solution of $Ax = b$ satisfies $A^TAx = A^Tb$ <br>\n",
    "\n",
    "Theorem 13\n",
    "- The set of least-squares solutions of $Ax = b$ coincides with <br>\n",
    "  the nonempty set of solutions of the normal equations $A^TAx = A^Tb$\n",
    "  - As shown, the set of least-sqaures solution is nonempty and each least-squares solution $\\hat{x}$ satisfies the normal equations\n",
    "  - Conversely, suppose $\\hat{x}$ satisfies $A^TA\\hat{x} = A^Tb$. Then $A^T(b - A\\hat{x}) = 0$, <br>\n",
    "    which shows that $b - A\\hat{x}$ is orthogonal to the rows of $A^T$ and hence is orthogonal to the columns of $A$ <br>\n",
    "    And since the columns of $A$ span $\\mathrm{Col}\\ A$, $b - A\\hat{x}$ is orthogonal to all of $\\mathrm{Col}\\ A$\n",
    "  - Hence $b = A\\hat{x} + (b - A\\hat{x})$ is a decomposition of $b$ into the sum of a vector in $\\mathrm{Col}\\ A$ and a vector orthogonal to $\\mathrm{Col}\\ A$\n",
    "  - By the uniqueness of the orthogonal decomposition, $A\\hat{x}$ must be the orthogonal projection of $b$ onto $\\mathrm{Col}\\ A$ <br>\n",
    "    That is, $A\\hat{x} = \\hat{b}$, and $\\hat{x}$ is a least-squares solution\n",
    "\n",
    "Theorem 14\n",
    "- Let $A$ be an $m \\times n$ matrix. The following statements are logically equivalent:\n",
    "  1. The equation $Ax = b$ has a unique least-squares solution for each $b$ in $\\mathbb{R}^m$\n",
    "  2. The columns of $A$ are linearly independent\n",
    "  3. The matrix $A^TA$ is invertible\n",
    "- $1 \\rightarrow 2$ proof\n",
    "  - least-squares solution $\\hat{x}$ satisfies $A\\hat{x} = \\hat{b} = \\mathrm{proj}_{\\mathrm{Col}\\ A}b$, which is in $\\mathrm{Col}\\ A$\n",
    "  - So, if $\\hat{x}$ is unique, then $Ax = \\hat{b}$ has no free variable, which means that the columns of $A$ are linearly independent\n",
    "- $2 \\rightarrow 3$ proof\n",
    "  - By Theorem 3, $\\mathrm{Nul}\\ A^T = (\\mathrm{Col}\\ A)^\\bot$. And If $x$ is both in $(\\mathrm{Col}\\ A)^\\bot$ and in $\\mathrm{Col}\\ A$, then $x = 0$\n",
    "  - Therefore, $A^TAx = A^T(Ax) = 0 \\Rightarrow Ax = 0$, and since the columns of $A$ are linearly independent, $\\Rightarrow x = 0$ <br>\n",
    "    which means that the columns of $A^TAx$ are linearly independent, and since $A^TA$ is square, $A^TA$ is invertible\n",
    "- $3 \\rightarrow 1$ proof\n",
    "  - $Ax = b \\Rightarrow A^TAx = A^Tb$, and since $A^TA$ is invertible, $\\Rightarrow x = (A^TA)^{-1}A^Tb$, which is unique\n",
    "- When these statements are true, the least-sqaures solution $\\hat{x}$ is given by $\\hat{x} = (A^TA)^{-1}A^Tb$ <br>\n",
    "  ( And we have $\\hat{b} = A\\hat{x} = A(A^TA)^{-1}A^Tb$ )\n",
    "\n",
    "Theorem 15\n",
    "- Given an $m \\times n$ matrix $A$ with linearly independent columns, let $A = QR$ be a QR factorization of $A$ <br>\n",
    "  Then, for each $b$ in $\\mathbb{R}^m$, the equation $Ax = b$ has aunique least-squares solution, given by $\\hat{x} = R^{-1}Q^Tb$\n",
    "  - since the columns of $Q$ form an orthonormal basis for $\\mathrm{Col}\\ A$, $QQ^Tb$ is the orthogonal projection $\\hat{b}$ of $b$ onto $\\mathrm{Col}\\ A$\n",
    "  - Therefore, $A\\hat{x} = QRR^{-1}Q^Tb = QQ^Tb = \\hat{b}$, which means that $\\hat{x} = R^{-1}Q^Tb$ is least-squares solution <br>\n",
    "    and by Theorem 14, since $A$ is linearly independent, $\\hat{x}$ is unique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e8ba1a",
   "metadata": {},
   "source": [
    "### 6) Machine Learning and Linear Models\n",
    "\n",
    "$X\\beta = y$ : $X$ is the design matrix, $\\beta$ is the parameter vector, and $y$ is the observation vector\n",
    "- Note that this is a least-squares problem. Generally, the equation may have no solution <br>\n",
    "  Predicted value $X\\beta$ is denoted by $\\hat{y}$. Note that $\\hat{y} = X\\beta$ is simple true equation\n",
    "- In polynomial regression, If data points are $(x_1, y_1),...,(x_n, y_n)$, then $X = \\begin{bmatrix} 1 & x_1 & \\cdots & x_1^p \\\\ 1 & x_2 & \\cdots & x_2^p \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 1 & x_n & \\cdots & x_n^p \\end{bmatrix}$, $\\beta = \\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_p \\end{bmatrix}$, $y = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix}$\n",
    "- The least-squares line : $y = \\beta_0 + \\beta_1x + \\cdots + \\beta_px^p$ that minimizes the sum of the squares of the residuals <br>\n",
    "  which is also called a line of regression of $y$ on $x$, and coefficients $\\beta_0, \\beta_1, ...\\beta_p$ are called regression coefficients\n",
    "  - Let $\\epsilon$ be the residual vector between observed $y$-value and predicted $y$-value, denoted by $y = X\\beta + \\epsilon$ <br>\n",
    "    then $\\epsilon_1 = y_1 - (\\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_p x_1^p)$, ..., $\\epsilon_n = y_n - (\\beta_0 + \\beta_1 x_n + \\cdots + \\beta_p x_n^p)$ \n",
    "  - And $||y - X\\beta|| = \\sqrt{e_1^2 + \\cdots + e_n^2}$. which is square root of the sum of the squares of the residuals\n",
    "  - Therefore, the least-sqaures solution of $X\\beta = y$ is equivalent to finding the least-squares line <br>\n",
    "    And the least-sqaures solution $\\hat{\\beta}$ is a solution of the normal equations $X^TX \\beta = X^T y$\n",
    "- We can fit of other curves by letting $X = \\begin{bmatrix} f_0(x_1) & \\cdots & f_p(x_1) \\\\ \\vdots & \\ddots & \\vdots \\\\ f_0(x_n) & \\cdots & f_p(x_n) \\end{bmatrix}$\n",
    "- We can fit on an experiment involving multiple independent variables. Suppose the data points are $(u_1, v_1, y_1),...,(u_n, v_n, y_n)$ <br>\n",
    "  where $u, v$ is independent variables and $y$ is dependent variable. then linear model defines as $y = \\beta_0f_0(u, v) + \\cdots + \\beta_pf_p(u, v)$ <br>\n",
    "  In this two variables case, the solution is called the least-squares plane "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5624464c",
   "metadata": {},
   "source": [
    "### 7) Inner Product Spaces\n",
    "\n",
    "An inner product on a vector space $V$ : function that, to each pair of vectors $u$ and $v$ in $V$, associates a real number $\\langle u, v \\rangle$\n",
    "- inner product satisfies the following axioms, for all $u$, $v$, and $w$ in $V$ and all scalars $c$:\n",
    "  - $\\langle u, v \\rangle = \\langle v, u \\rangle$\n",
    "  - $\\langle u+v, w \\rangle = \\langle u, w \\rangle + \\langle v, w \\rangle$\n",
    "  - $\\langle cu, v \\rangle = c\\langle u, v \\rangle$\n",
    "  - $\\langle u, u \\rangle \\geq 0$, and $\\langle u, u \\rangle = 0$ if and only if $u = 0$\n",
    "- A vector space with an inner product is called an inner product space. <br>\n",
    "  $\\mathbb{R}^n$ with standard inner product, dealt with in Chapter 6, is an inner product space\n",
    "- we can define inner product in $\\mathbb{P}_n$, such as $\\langle p, q \\rangle = p \\langle t_0 \\rangle q \\langle t_0 \\rangle + \\cdots + p \\langle t_n \\rangle q \\langle t_n \\rangle$ for distinct real numbers $t_0, ..., t_n$\n",
    "\n",
    "The length (norm) is $\\|v\\| = \\sqrt{\\langle v, v \\rangle}$, and definitions of unit vector, distance, and orthogonal, are same <br>\n",
    "So, we can apply the Gram-Schmidt Process or Best Approximation in Inner Product Space, not only $\\mathbb{R}^n$, but also $\\mathbb{P}_n$ <br>\n",
    "\n",
    "Let $V$ be $\\mathbb{P}_4$ with $\\langle p, q \\rangle = p \\langle -2 \\rangle q \\langle -2 \\rangle + p \\langle -1 \\rangle q \\langle -1 \\rangle + p \\langle 0 \\rangle q \\langle 0 \\rangle + p \\langle 1 \\rangle q \\langle 1 \\rangle + p \\langle 2 \\rangle q \\langle 2 \\rangle$ <br>\n",
    "- Produce an orthogonal basis for $\\mathbb{P}_2$, which is subspace of $V$\n",
    "  - we can suppose that the standard basis of $\\mathbb{P}_2$ is $\\{1, t, t^2\\}$, I will apply the Gram-Schmidt Process to $\\{1, t, t^2\\}$\n",
    "  - let $p_0(t) = 1$, than $p_1(t) = t - \\frac{\\langle t, p_0 \\rangle}{\\langle p_0, p_0 \\rangle} p_0 = t$ <br>\n",
    "    And $p_2(t) = t^2 - \\frac{\\langle t^2, p_0 \\rangle}{\\langle p_0, p_0 \\rangle} p_0(t) - \\frac{\\langle t^2, p_1 \\rangle}{\\langle p_1, p_1 \\rangle} p_1(t) = t^2 - 2 p_0(t) = t^2 - 2$\n",
    "  - Therefore, An orthogonal basis for the subspace $\\mathbb{P}_2$ of $V$ is $\\{p_0, p_1, p_2\\} = \\{1, t, t^2 - 2\\}$\n",
    "    - Each polynomial in $\\mathbb{P}_4$ is uniquely determined by its value at the five numbers $-2, ..., 2$ <br>\n",
    "      So, we also can find orthogonal basis for $\\mathbb{P_2}$ in $\\mathbb{R}^5$ with standard inner product\n",
    "- Find the best approximation to $p(t) = 5 - \\frac{1}{2}t^4$ by polynomials in $\\mathbb{P}_2$\n",
    "  - By the Best Approximation Theorem, the best approximation to $p$ is $\\hat{p} = \\mathrm{proj}_{\\mathbb{P}_2}p$\n",
    "  - And by the Orthogonal Decomposition Theorem, $\\mathrm{proj}_{\\mathbb{P}_2}p = \\frac{\\langle p, p_0 \\rangle}{\\langle p_0, p_0 \\rangle} p_0 + \\frac{\\langle p, p_1 \\rangle}{\\langle p_1, p_1 \\rangle} p_1 + \\frac{\\langle p, p_2 \\rangle}{\\langle p_2, p_2 \\rangle} p_2$ <br>\n",
    "    $= \\frac{8}{3} p_0 + \\frac{-31}{14}p_2 = \\frac{8}{5} - \\frac{31}{14}(t^2 - 2)$. This polynomial is the cloest to $p$ of all polynomials in $\\mathbb{P}_2$\n",
    "\n",
    "Theorem 16 ( The Cauchy-Schwarz Inequality ) : For all $u$, $v$ in $V$, $|\\langle u, v \\rangle| \\le \\|u\\|\\|v\\|$\n",
    "- If $u = 0$, then both sides are zero, and hence the inequality is true\n",
    "- If $u \\neq 0$, let $W$ be the subspace spanned by $u$. Then $\\|\\mathrm{proj}_Wv\\| = \\| \\frac{\\langle v, u \\rangle}{\\langle u, u \\rangle} u \\|$ <br>\n",
    "  and since $\\| cu \\| = |c| \\| u \\|$ for any scalar $c$, $= \\frac{|\\langle v, u \\rangle|}{|\\langle u, u \\rangle|} \\| u \\| = \\frac{|\\langle v, u \\rangle|}{\\| u \\|^2} \\| u \\| = \\frac{|\\langle v, u \\rangle|}{\\| u \\|}$\n",
    "- Since $\\|\\mathrm{proj}_Wv\\| \\le \\|v\\|$, we have $\\frac{|\\langle u, v \\rangle|}{\\|u\\|} \\le \\|v\\|$, which gives the Inequality\n",
    "\n",
    "Theorem 17 ( The Triangle Ineqaulity ) : For all $u$, $v$ in $V$, $\\|u+v\\| \\le \\|u\\| + \\|v\\|$\n",
    "- $\\|u+v\\|^2 = \\langle u+v, u+v \\rangle = \\langle u, u \\rangle + 2 \\langle u, v \\rangle + \\langle v, v \\rangle \\le \\|u\\|^2 + 2|\\langle u, v \\rangle| + \\|v\\|^2$ <br>\n",
    "  and by Cauchy-Schwarz Inequality, $\\le \\|u\\|^2 + 2 \\|u\\| \\|v\\| + \\|v\\|^2 = (\\|u\\| + \\|v\\|)^2$, which gives the Ineqaulity\n",
    "\n",
    "\n",
    "the vector space $C[a, b]$ : all continuous functions on an interval $a \\le t \\le b$ <br>\n",
    "For $f$, $g$ in $C[a, b]$, $\\langle f, g \\rangle = \\int_{a}^{b}{f(t)g(t) dt}$ is inner product on $C[a, b]$ (we can verify Inner product Axiom) <br>\n",
    "So, not only polynomial, but also continous function can find orthogonal basis and best approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11d743e",
   "metadata": {},
   "source": [
    "### 8) Applications of Inner Product Spaces\n",
    "\n",
    "the sum of the squares for error : $\\mathrm{SS(E)} = (y_1 - \\hat{y}_1)^2 + \\cdots + (y_n - \\hat{y}_n)^2$, <br>\n",
    "which is simply $||y - \\hat{y}||^2$, using the standard length in $\\mathbb{R}^n$\n",
    "\n",
    "Weighted $\\mathrm{SS(E)} = w_1^2(y_1 - \\hat{y}_1)^2 + \\cdots + w_n^2(y_n - \\hat{y}_n)^2$ <br>\n",
    "which is the square of the length of $y - \\hat{y}$, where the length is derived from $\\langle x, y \\rangle = w_1^2x_1y_1 + \\cdots + w_n^2x_ny_n$ <br>\n",
    "but, $w_j^2(y_j - \\hat{y_j})^2 = (w_jy_j - w_jy)^2$. and let $W = \\begin{bmatrix} w_1 & 0 & \\cdots & 0 \\\\ 0 & w_2 & \\cdots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & w_n \\end{bmatrix}$, then $Wy = \\begin{bmatrix} w_1y_1 \\\\ w_2y_2 \\\\ \\vdots \\\\ w_ny_n \\end{bmatrix}$ <br>\n",
    "Therefore the weighted $\\mathrm{SS(E)}$ is the square of the ordinary length in $\\mathbb{R}^n$ of $Wy - W\\hat{y}$, which we write as $\\|Wy - W\\hat{y}\\|^2$ <br>\n",
    "\n",
    "By least-squares problem $Ax = y$, let $\\hat{x}$ be the least-squares solution. <br>\n",
    "If the measure of closeness is the weighted error $\\|Wy - W\\hat{y}\\|^2 = \\|Wy - WA\\hat{x}\\|^2$, <br>\n",
    "then $\\hat{x}$ is the least-squares solution of the equation $WAx = Wy$, <br>\n",
    "So the normal equation for the least-squares solution is $(WA)^TWAx = (WA)^TWy$ <br>\n",
    "\n",
    "( trend analysis : linear model + inner product ) <br>\n",
    "Fit a quadratic trend function to the data $(-2, 3)$, $(-1, 5)$, $(0, 5)$, $(1, 4)$, and $(2, 3)$\n",
    "- for $p(t)$ in $\\mathbb{R}_4$, $p \\mapsto \\begin{bmatrix} p(-2) \\\\ p(-1) \\\\ p(0) \\\\ p(1) \\\\ p(2) \\end{bmatrix}$ is an isomorphism, so we list orthogonal basis of $\\mathbb{P}_2$ (already found) as a vector in $\\mathbb{R}^5$: <br>\n",
    "  $p_0 = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{bmatrix}$, $p_1 = \\begin{bmatrix} -2 \\\\ -1 \\\\ 0 \\\\ 1 \\\\ 2 \\end{bmatrix}$, $p_2 = \\begin{bmatrix} 2 \\\\ -1 \\\\ -2 \\\\ -1 \\\\ 2 \\end{bmatrix}$, and let data $g = \\begin{bmatrix} 3 \\\\ 5 \\\\ 5 \\\\ 4 \\\\ 3 \\end{bmatrix}$. Note that $g$ means $g(-2) = 3, ..., g(2) = 3$\n",
    "- By the Best Approximation Theorem, the best approximation to the data by polynomials in $\\mathbb{P}_2$ is <br>\n",
    "  $\\hat{p} = \\mathrm{proj}_{\\mathbb{P}_2}g = \\frac{\\langle g, p_0 \\rangle}{\\langle p_0, p_0 \\rangle} p_0 + \\frac{\\langle g, p_1 \\rangle}{\\langle p_1, p_1 \\rangle} g_1 + \\frac{\\langle p, p_2 \\rangle}{\\langle p_2, p_2 \\rangle} g_2 = \\frac{20}{3}p_0 - \\frac{1}{10}p_1 - \\frac{7}{14}p_2$ <br>\n",
    "  Therefore $\\hat{p}(t) = 4 - 0.1t - 0.5(t^2 - 2)$\n",
    "\n",
    "\n",
    "It turns out that any function in $C[0, 2\\pi]$ can be approximated as closely as desired by a function of the form: <br>\n",
    "$\\frac{a_0}{2} + a_1 \\cos t + \\cdots + a_n \\cos nt + b_1 \\sin t + \\cdots + b_n \\sin nt$ for a sufficiently large value of $v$ <br>\n",
    "This function is called a trigonometric polynomial. and if $a_n$ and $b_n$ are not both zero, the polynomial is said to be of order $n$\n",
    "\n",
    "- for any $n \\geq 1$, the set $\\{1, \\cos t, ..., \\cos nt. \\sin t, ..., \\sin nt\\}$ is orthogonal with respect to the inner product $\\langle f, g \\rangle = \\int_{0}^{2\\pi}{f(t)g(t) dt}$\n",
    "  - $\\langle \\cos mt, \\cos nt \\rangle = \\int_{0}^{2\\pi}{\\cos mt \\cos nt\\ dt} = \\frac{1}{2} \\int_{0}^{2\\pi}{[\\cos (mt + nt) + \\cos (mt - nt)]dt}$ <br>\n",
    "    $= \\frac{1}{2} [\\frac{\\sin (mt + nt)}{m + n} + \\frac{\\sin (mt - nt)}{m - n}]|^{2\\pi}_{0} = 0$ when $m \\neq n$ and $m. n > 0$\n",
    "  - $\\langle \\sin mt, \\sin nt \\rangle = \\int_{0}^{2\\pi}{\\sin mt \\sin nt\\ dt} = \\frac{1}{2} \\int_{0}^{2\\pi}{[- \\cos (mt + nt) + \\cos (mt - nt)]dt}$ <br>\n",
    "    $= \\frac{1}{2} [- \\frac{\\sin (mt + nt)}{m + n} + \\frac{\\sin (mt - nt)}{m - n}]|^{2\\pi}_{0} = 0$ when $m \\neq n$ when $m \\neq n$ and $m, n > 0$\n",
    "  - $\\langle \\sin mt, \\cos nt \\rangle = \\int_{0}^{2\\pi}{\\sin mt \\cos nt\\ dt} = \\frac{1}{2} \\int_{0}^{2\\pi}{[\\sin (mt + nt) + \\sin (mt - nt)]dt}$ <br>\n",
    "    $= \\frac{1}{2} [- \\frac{\\cos (mt + nt)}{m + n} - \\frac{\\cos (mt - nt)}{m + n}]|^{2\\pi}_{0} = -\\frac{1}{m+n} + \\frac{1}{m+n} = 0$ when $m, n > 0$\n",
    "  - $\\langle 1, \\cos nt \\rangle = [\\frac{sin\\ nt}{n}]|^{2\\pi}_{0} = 0$, and $\\langle 1, \\sin nt \\rangle = [\\frac{-cos\\ nt}{n}]|^{2\\pi}_{0} = -\\frac{1}{n} + \\frac{1}{n} = 0$\n",
    "- Let $W$ be the subspace of $C[0, 2\\pi]$ spanned by the functions in $\\{1, \\cos t, ..., \\cos nt. \\sin t, ..., \\sin nt\\}$ <br>\n",
    "  Given $f$ in $C[0, 2\\pi]$, the best approximation to $f$ by functions in $W$ is called the $n$th-order Fourier approximation to $f$ on $[0, 2\\pi]$ <br>\n",
    "  and since the functions are orthogonal, the best approximation is given by the orthogonal projection onto $W$\n",
    "  - $\\langle \\cos kt, \\cos kt \\rangle = \\int_{0}^{2\\pi}{(\\cos kt)^2\\ dt} = \\frac{1}{2} \\int_{0}^{2\\pi}{[1 + \\cos 2kt]dt} = \\frac{1}{2} [x + \\frac{\\sin 2kt}{2k}]|^{2\\pi}_{0} = \\pi$\n",
    "  - $\\langle \\sin kt, \\sin kt \\rangle = \\int_{0}^{2\\pi}{(\\sin kt)^2\\ dt} = \\frac{1}{2} \\int_{0}^{2\\pi}{[1 - \\cos 2kt]dt} = \\frac{1}{2} [x - \\frac{\\sin 2kt}{2k}]|^{2\\pi}_{0} = \\pi$\n",
    "  - Therefore, for trigonoemtric polynomial $\\frac{a_0}{2} + a_1 \\cos t + \\cdots + a_n \\cos nt + b_1 \\sin t + \\cdots + b_n \\sin nt$ <br>\n",
    "    $a_k = \\frac{\\langle f, \\cos kt \\rangle}{\\langle \\cos kt, \\cos kt \\rangle} = \\frac{1}{\\pi} \\int_{0}^{2\\pi}{f(t) \\cos kt\\ dt}$, and $b_k = \\frac{\\langle f, \\sin kt \\rangle}{\\langle \\sin kt, \\sin kt \\rangle} = \\frac{1}{\\pi} \\int_{0}^{2\\pi}{f(t) \\sin kt\\ dt}$ for $k \\geq 1$ <br>\n",
    "    and since $\\frac{\\langle f, 1 \\rangle}{\\langle 1, 1 \\rangle} = \\frac{1}{2\\pi} \\int_{0}^{2\\pi}{f(t) \\cdot 1\\ dt} = \\frac{1}{2}[\\frac{1}{\\pi} \\int_{0}^{2\\pi}{f(t) \\cos(0 \\cdot t)\\ dt}]$ <br>\n",
    "    $a_0$ is defined, and this explains why the constant term in the trigonometric polynomial is written as $\\frac{a_0}{2}$\n",
    "- The norm of the difference between $f$ and a Fourier approximation is called the **mean** square error in the approximation <br>\n",
    "  ( The term **mean** refers to the fact that the norm is determined by an integral )\n",
    "- It can be shown that the mean square error approaches zero as the order of the Fourier approximation increases <br>\n",
    "  For this reason, $f(t) = \\frac{a_0}{2} + \\sum_{m=1}^{\\infty}(a_m \\cos mt + b_m \\sin mt)$ is called the Fourier series for $f$ on $[0, 2\\pi]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b457b6",
   "metadata": {},
   "source": [
    "*****"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a6e83c",
   "metadata": {},
   "source": [
    "## Chapter 7 : Symmetric Matrices and Quadratic Forms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b89a9c3",
   "metadata": {},
   "source": [
    "### 1) Diagonalization of Symmetric Matrices\n",
    "\n",
    "A symmetric matrix is a matrix $A$ such that $A^T = A$. Such a matrix is necessarily square <br>\n",
    "\n",
    "$A$ is orthogonally diagonalizable : there are an orthogonal matrix $P$ and a diagonal matrix $D$ such that $A = PDP^{-1} = PDP^T$ <br>\n",
    "\n",
    "Theorem 1~3  : The Spectral Theorem for Symmetric Matrices\n",
    "- An $n \\times n$ symmetric matrix $A$ has the following properties:\n",
    "  1. $A$ has $n$ real eigenvalues, counting multiplicities\n",
    "  2. The dimension of the eigenspace for each eigenvalue $\\lambda$ equals <br>\n",
    "     the multiplicity of $\\lambda$ as a root of the characteristic equation\n",
    "  3. The eigenspaces are mutually orthogonal, in the sense that <br>\n",
    "     eigenvectors corresponding to different eigenvalues are orthogonal\n",
    "  4. $A$ is orthogonally diagnoalizable\n",
    "  - proof : [link]\n",
    "\n",
    "If $n \\times n$ matrix $B$ is orthogonally diagonalizable, then $B$ is a symmetric matrix\n",
    "- $B^T = (PDP^T)^T = (P^T)^TD^TP^T = PD^TP^T = PDP^T = B$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa8d478",
   "metadata": {},
   "source": [
    "### 2) Quadratic Forms\n",
    "\n",
    "Quadratic form is a function $Q$ whose value at a vector $x$ can be computed by an expression of the form $Q(x) = x^TAx$, <br>\n",
    "where $A$ is $n \\times n$ symmetric matrix, called the matrix of the quadratic form <br>\n",
    "\n",
    "Theorem 4 : The Principal Axes Theorem\n",
    "- Let $A$ be an $n \\times n$ symmetric matrix. Then there is an orthogonal change of variable, $x = Py$, <br>\n",
    "  that transforms the quadratic form $x^TAx$ into a quadratic form $y^TDy$ with no cross-product term\n",
    "  - If $x$ represents a variable vector in $\\mathbb{R}^n$, then a change of variable is an equation of the form $x = Py$ <br>\n",
    "    where $P$ is invertible matrix, and $y$ is a new variable vector in $\\mathbb{R}^n$, and also <br>\n",
    "    $y$ is the coordinate vector of $x$ relative to the basis of $\\mathbb{R}^n$ determined by the columns of $P$, called the principal axes \n",
    "  - Then, $x^TAx = (Py)^TA(Py) = y^TP^TAPy = y^T(P^TAP)y$, thus the new matrix of the quadratic form is $P^TAP$\n",
    "  - By the Spectral Theorem, there is an orthogonal matrix $P$ such that $P^TAP$ is a diagonal matrix $D$\n",
    "  - Then we can check $y^TDy = \\begin{bmatrix} y_1 & \\cdots & y_n \\end{bmatrix} \\begin{bmatrix} \\lambda_1 & \\cdots & 0 \\\\ \\vdots & \\ddots & \\vdots \\\\ 0 & \\cdots & \\lambda_n \\end{bmatrix} \\begin{bmatrix} y_1 \\\\ \\vdots \\\\ y_n \\end{bmatrix} = \\lambda_1y_1^2 + \\cdots \\lambda_ny_1^n$, has no cross-product term\n",
    "\n",
    "Quadratic form $Q$ is:\n",
    "- positive definite if $Q(x) > 0$ for all $x \\neq 0$, or positive semidefinite if $Q(x) \\geq 0$ for all $x$\n",
    "- negative definite if $Q(x) < 0$ for all $x \\neq 0$, or negative semidefinite if $Q(x) \\le 0$ for all $x$\n",
    "- indefinite if $Q(x)$ assumes both positive and negative values\n",
    "\n",
    "Theorem 5 : Quadratic Forms and Eigenvalues\n",
    "- Let $A$ be an $n \\times n$ symmetric matrix. Then a quadratic form $x^TAx$ is:\n",
    "1. positive definite if and only if the eigenvalues of $A$ are all positive\n",
    "2. negative definite if and only if the eigenvalues of $A$ are all negative\n",
    "3. indefinite if and only if $A$ has both positive and negative eigenvalues\n",
    "  - By the Principal Axes Theorem, there exsits an orthogonal change of variable $x = Py$ such that: <br>\n",
    "    $Q(x) = x^TAx = y^TDy = \\lambda_1y_1^2 + \\cdots + \\lambda_ny_n^2$ where $\\lambda_1, ..., \\lambda_n$ are the eigenvalues of $A$ because of diagonalization\n",
    "  - Since $P$ is invertible, there is a one-to-one correspondence between all nonzero $x$ and all nonzero $y$ <br>\n",
    "    Therefore we can check the Theorem is true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb05353",
   "metadata": {},
   "source": [
    "### 3) Constrained Optimization\n",
    "\n",
    "Theorem 6 : Let $A$ be a symmetric $n \\times n$ matrix\n",
    "- Define $m = \\min \\{x^TAx : \\|x\\| = 1 \\}, \\ M = \\max \\{x^TAx : \\|x\\| = 1 \\}$ <br>\n",
    "  then $M$ is the greatest eigenvalue of $A$ and $m$ is the least eigenvalue of $A$ <br>\n",
    "  and the value of $x^TAx$ is $M, m$ when $x$ is a unit eigenvector corresponding to $M, m$\n",
    "  - By the Spectral Theorem, $A$ can be orthogonally diagonalized as $PDP^{-1}$ <br>\n",
    "    then $x^TAx = y^TDy$ when $x = Py$, and also $\\|x\\| = \\|Py\\| = \\|y\\|$ by Theorem 7 in Chapter 6\n",
    "  - Suppose that $A$ has eigenvalues $\\lambda_1 \\geq \\lambda_2 \\geq \\cdots \\geq \\lambda_n$. <br>\n",
    "    Arrange the eigenvector columns of $P$ so that $P = \\begin{bmatrix} u_1 & u_2 & \\cdots & u_n \\end{bmatrix}$ and $D = \\begin{bmatrix} \\lambda_1 & 0 & \\cdots & 0 \\\\ 0 & \\lambda_2 & \\cdots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & \\lambda_n \\end{bmatrix}$\n",
    "  - Given any unit vector $y = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix}$, observe that $\\lambda_2y_2^2 \\le \\lambda_1y_2^2$, ..., $\\lambda_ny_n^2 \\le \\lambda_ny_n^2$ <br>\n",
    "    and obtain $y^TDy = \\lambda_1y_1^2 + \\lambda_2y_2^2 + \\cdots + \\lambda_ny_n^2 \\le \\lambda_1y_1^2 + \\lambda_1y_2^2 + \\cdots + \\lambda_1y_n^2 = \\lambda_1(y_1^2 + y_2^2 + \\cdots + y_n^2) = \\lambda_1$\n",
    "  - Thus $M \\le \\lambda_1$, but $y^TDy = \\lambda_1$ when $y = e_1$, so in fact $M = \\lambda_1$ <br>\n",
    "    And if $y = e_1$, then $x = Py = u_1$. Therefore $y^TDy = x^TAx = M = \\lambda_1$ when $x$ is $u_1$\n",
    "  - Similarly we can show that $x^TAx = m = \\lambda_n$ when $x$ is $u_n$\n",
    "\n",
    "Theorem 7 (and 8) : Let $A$, $P$, and $D$ be as in proof of Theorem 6\n",
    "- For $k = 2, ..., n$, the maximum value of $x^TAx$ subject to the constraints $x^Tx = 1$ and $x^Tu_1 = 0$, ..., $x^Tu_{k-1} = 0$ <br>\n",
    "  is the eigenvalue $\\lambda_k$, and this maximum is attained at $x = u_k$\n",
    "  - Every $x$ can be described as $x = Py = y_1u_1 + \\cdots + y_nu_n$ <br>\n",
    "    Then $x^Tu_i = y_1\\|u_i\\|^2$ for all $i$. So if $x^Tu_i = 0$, then $y_i = 0$. Therefore $y_1, ..., y_{k-1}$ are all zeros\n",
    "  - Then $y^TDy = \\lambda_ky_k^2 + \\cdots + \\lambda_ny_n^2 \\le \\lambda_k(y_k^2 + \\cdots + y_n^2) = \\lambda_k$. <br>\n",
    "    Therefore, similarly to proof of Theorem 6, the maximum value of $x^TAx$ is $\\lambda_k$ when $x = u_k$\n",
    "\n",
    "Exmaple : Find the maximum value of $q(x, y) = xy$ subject to the constraint $4x^2 + 9y^2 = 36$\n",
    "- $4x^2 + 9y^2 = 36 \\Rightarrow (\\frac{x}{3})^2 + (\\frac{y}{2})^2 = 1$, and define $x_1 = \\frac{x}{3}$, $x_2 = \\frac{y}{2}$ <br>\n",
    "  Then the constraint equation becomes $x_1^2 + x_2^2 = 1$, and $q(x, y) = q(3x_1 2x_2) = 6x_1x_2$ <br>\n",
    "  Thus the problem is to maximize $Q(x) = 6x_1x_2$ subject to $x^Tx = 1$\n",
    "- Let $A = \\begin{bmatrix} 0 & 3 \\\\ 3 & 0 \\end{bmatrix}$, then $Q(x) = x^TAx$. and we can find $\\lambda_1 = 3$, $\\lambda_2 = -3$ and $u_1 = \\begin{bmatrix} 1/\\sqrt{2} \\\\ 1/\\sqrt{2} \\end{bmatrix}$, $u_2 = \\begin{bmatrix} -1/\\sqrt{2} \\\\ 1/\\sqrt{2} \\end{bmatrix}$ <br>\n",
    "  Therefore, by Theorem 6, the maximum value of $Q(x) = q(x_1, x_2)$ is $3$, attained when $x_1 = 1/\\sqrt{2}$ and $x_2 = 1/\\sqrt{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555bd7ca",
   "metadata": {},
   "source": [
    "### 4) The Singular Value Decomposition\n",
    "\n",
    "For $m \\times n$ matrix $A$, Observe that $\\|Ax\\|^2 = (Ax)^T(Ax) = x^TA^TAx = x^T(A^TA)x$ <br>\n",
    "Also, $A^TA$ is a symmetric matrix, since $(A^TA)^T = A^TA^TT = A^TA$. So if constraint $\\|x\\| = 1$ is existed, <br>\n",
    "then by Theorem 6, we can find the a unit vector $x$ at which $\\|Ax\\|$ is maximized <br>\n",
    "\n",
    "On the other hand, since $A^TA$ is orthogonally diagonalizable, let $\\lambda_1 \\geq \\lambda_2 \\geq \\cdots \\geq \\lambda_n$ be an eigenvalues of $A^TA$, <br>\n",
    "and $v_1,v_2,...,v_n$ be the associated **orthonormal** eigenvectors of $A^TA$ <br>\n",
    "Then, for $1 \\le i \\le n$, we have $\\|Av_i\\|^2 = (Av_i)^TAv_i = v_i^TA^TAv_i = v_i^T\\lambda_iv_i = \\lambda_i \\geq 0$ <br>\n",
    "The **singular values** of $A$ : the square roots of the eigenvalues of $A^TA$, denoted by $\\sigma_i = \\sqrt{\\lambda_i}$ <br>\n",
    "Note that $\\lambda_1 \\geq \\lambda_2 \\geq \\cdots \\geq \\lambda_n \\geq 0$ so that $\\sigma_1,...,\\sigma_n$ is exsited and arranged in decreasing order <br>\n",
    "and $\\sigma_i = \\sqrt{\\lambda_i} = \\|Av_i\\|$, which means that the sigular values of $A$ are the lengths of the vectors $Av_1,...,Av_n$ <br>\n",
    "\n",
    "Theroem 9\n",
    "- Suppose $\\{v_1,...,v_n\\}$ is an orthonormal basis of $\\mathbb{R}^n$ consisting of eigenvectors of $A^TA$ <br>\n",
    "  arranged so that the corresponding eigenvalues of $A^TA$ satisfy $\\lambda_1 \\geq \\cdots \\geq \\lambda_n$ <br>\n",
    "  and suppose $A$ has $r$ nonzero singular values.\n",
    "- Then $\\{Av_1,...,Av_r\\}$ is an orthogonal basis for $\\mathrm{Col}\\ A$, and $\\mathrm{rank}\\ A = r$\n",
    "  - $(Av_i)^T(Av_j) = v_i^TA^TAv_j = v_i^T(\\lambda_jv_j) = 0$ for $i \\neq j$. Thus $\\{Av_1,...,Av_n\\}$ is an orthogonal set\n",
    "  - Since the lengths of the vectors $Av_1,...,Av_n$ are the sigular values of $A$, and since there are $r$ nonzero singular values, <br>\n",
    "    $Av_i \\neq 0$ if and only if $1 \\le i \\le r$. So by Theorem 4 in Chapter 6, $Av_1,...,Av_r$ are linearly independent vectors\n",
    "  - For any $y$ in $\\mathrm{Col}\\ A$, that is, for any $y = Ax$, we can write $x = c_1v_1 + \\cdots + c_nv_n$ and<br>\n",
    "    $y = Ax = c_1Av_1 + \\cdots + c_rAv_r + c_{r+1}Av_{r+1} + \\cdots + c_nAv_n = c_1Av_1 + \\cdots + c_rAv_r + 0 + \\cdots + 0$ <br>\n",
    "    Thus $y$ is in $\\mathrm{Span}\\{Av_1,...,Av_r\\}$, which shows that $\\{Av_1,...,Av_r\\}$ is an orthogonal basis for $\\mathrm{Col}\\ A$\n",
    "\n",
    "Theorem 10 : The Singular Value Decomposition\n",
    "- Let $A$ be an $m \\times n$ matrix with $\\mathrm{rank}\\ r$. Then there exists an $m \\times n$ matrix $\\Sigma = \\begin{bmatrix} D & 0 \\\\ 0 & 0 \\end{bmatrix}$ <br>\n",
    "  for which the diagonal entries in $r \\times r$ matrix $D$ are the first $r$ sigular values of $A$, $\\sigma_1 \\geq \\sigma_2 \\geq \\cdots \\geq \\sigma_r > 0$, <br>\n",
    "  and there exist an $m \\times m$ orthogonal matrix $U$ and an $n \\times n$ orthogonal matrix $V$ such that $A = U\\Sigma V^T$\n",
    "  - Let $\\lambda_i$ and $v_i$ be as in Theorem 9, so that $\\{Av_1,...,Av_r\\}$ is an orthogonal basis for $\\mathrm{Col}\\ A$ \n",
    "  - Normalize each $Av_i$ to obtain an orthonormal basis $\\{u_1, ..., u_r\\}$, where $u_i = \\frac{Av_i}{\\|Av_i\\|} = \\frac{Av_i}{\\sigma_i}$ so $Av_i = \\sigma_iu_i$ for $1 \\le i \\le r$\n",
    "  - Now extend $\\{u_1,...,u_r\\}$ to an orthonormal basis $\\{u_1,...,u_m\\}$ of $\\mathbb{R}^m$, <br>\n",
    "    and let $U = \\begin{bmatrix} u_1 & u_2 & \\cdots & u_m \\end{bmatrix}$ and $V = \\begin{bmatrix} v_1 & v_2 & \\cdots & v_n \\end{bmatrix}$ <br>\n",
    "    Note that $U$ and $V$ are orthogonal matrices\n",
    "  - $AV = \\begin{bmatrix} Av_1 & \\cdots & Av_r & 0 & \\cdots & 0 \\end{bmatrix} = \\begin{bmatrix} \\sigma_1u_1 & \\cdots & \\sigma_ru_r & 0 & \\cdots & 0 \\end{bmatrix}$ \n",
    "  - $U\\Sigma = \\begin{bmatrix} u_1 & \\cdots & u_m \\end{bmatrix} \\begin{bmatrix} \\sigma_1 & \\cdots & 0 & 0 & \\cdots & 0 \\\\\n",
    "                                                                                \\vdots & \\ddots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "                                                                                0 & \\cdots & \\sigma_r & 0 & \\cdots & 0 \\\\\n",
    "                                                                                0 & \\cdots & 0 & 0 & \\cdots & 0 \\\\\n",
    "                                                                                \\vdots & \\ddots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "                                                                                0 & \\cdots & 0 & 0 & \\cdots & 0 \\end{bmatrix}\n",
    "             = \\begin{bmatrix} \\sigma_1u_1 & \\cdots & \\sigma_ru_r & 0 & \\cdots & 0 \\end{bmatrix}$\n",
    "  - Therefore $AV = U\\Sigma$, and since $V$ is an orthogonal matrix, $A = U\\Sigma V^{-1} = U\\Sigma V^T$\n",
    "- $Ax = U\\Sigma V^T x = U\\Sigma \\begin{bmatrix} v_1^Tx \\\\ \\vdots \\\\ v_n^Tx \\end{bmatrix} = U\\begin{bmatrix} \\sigma_1v_1^Tx \\\\ \\vdots \\\\ 0 \\end{bmatrix} = \\sigma_1u_1v_1^Tx + \\cdots + \\sigma_ru_rv_r^Tx$, so $A = \\sigma_1u_1v_1^T + \\cdots + \\sigma_ru_rv_r^T$\n",
    "- SVD is not unique, that is, $V$, $U$ is not unique (not always consisted by eigenvectors) but $\\Sigma$ is unique\n",
    "\n",
    "Example : Find the SVD of $A = \\begin{bmatrix} 3 & 2 & 2 \\\\ 2 & 3 & -2 \\end{bmatrix}$\n",
    "- If $U\\Sigma V^T$ is the SVD of $m \\times n$ matrix $A$, then $V\\Sigma^T U^T$ is the SVD of $A^T$\n",
    "  - Let $v_1,...,v_n$ are eigenvectors of $A^TA$, then we have $A^TAv_i = \\lambda_iv_i$ <br>\n",
    "    If $\\lambda_i = 0$, then $A^TAv_i = \\lambda_iv_i \\Rightarrow A^TAv_i = 0 \\Rightarrow Av_i = 0$ because $\\mathrm{Nul}\\ A^T = (\\mathrm{Col}\\ A)^\\bot$ <br>\n",
    "    If $\\lambda_i \\neq 0$, then $A^TAv_i \\neq 0 \\Rightarrow Av_i \\neq 0$, and since $A^TAv_i = \\lambda_iv_i \\Rightarrow AA^T(Av_i) = \\lambda_i(Av_i)$, <br>\n",
    "    we conclude $\\lambda_i$ is also the eigenvalue of $AA^T$ corresponding to the eigenvector $Av_i$ when $\\lambda_i \\neq 0$ <br>\n",
    "  - Conversely, by the same way, we can show the nonzero eigenvalue of $AA^T$ is also the eigenvalue of $A^TA$ <br>\n",
    "  - Therefore, $AA^T$, $A^TA$ have same nonzero eigenvalues, which means that $A$ and $A^T$ has same nonzero singular values\n",
    "  - So, since $V$ is $n \\times n$ orthogonal matrix and $U$ is $m \\times m$ orthogonal matrix and $\\Sigma^T = \\begin{bmatrix} D & 0 \\\\ 0 & 0 \\end{bmatrix}$ is $n \\times m$ matrix, <br>\n",
    "    by definition of SVD, we conclude $V\\Sigma^T U^T$ is the SVD of $A^T$ \n",
    "  - ( As an aside, by proof, we have $Av_1, ..., Av_r$ is eigenvectors of $AA^T$, that is, $u_1, ..., u_r$ is eigenvectors of $AA^T$ )\n",
    "    - since $AA^T$, $AA^T$ is symmetric, by The Spectral Theroem, the eigenvectors of $AA^T$, $AA^T$ are orthogonal\n",
    "    - So, if $U$, $V$ is consisted of unit eigenvectors of $AA^T$, $AA^T$, then $A = U\\Sigma V^T$ is the SVD of $A$\n",
    "- because $A^TA$ is $3 \\times 3$ and $AA^T$ is $2 \\times 2$, I will find the SVD of $A^T$ and then find the SVD of $A$ by transpose\n",
    "- $A^T = \\begin{bmatrix} 3 & 2 \\\\ 2 & 3 \\\\ 2 & -2 \\end{bmatrix}$ and $(A^T)^TA^T = AA^T = \\begin{bmatrix} 17 & 8 \\\\ 8 & 17 \\end{bmatrix}$, and the eigenvalues of $AA^T$ are $\\lambda_1 = 25$ and $\\lambda_2 = 9$ <br>\n",
    "  Hence the singular values of $A^T$ is $\\sigma_1 = 5$ and $\\sigma_2 = 3$, so let $\\Sigma = \\begin{bmatrix} 5 & 0 \\\\ 0 & 3 \\\\ 0 & 0 \\end{bmatrix}$\n",
    "- And unit eigenvectors associated $\\lambda_1, \\lambda_2$, are $v_1 = \\begin{bmatrix} 1/\\sqrt{2} \\\\ 1/\\sqrt{2} \\end{bmatrix}$, $v_2 = \\begin{bmatrix} -1/\\sqrt{2} \\\\ 1/\\sqrt{2} \\end{bmatrix}$, so let $V = \\begin{bmatrix} 1/\\sqrt{2} & -1/\\sqrt{2} \\\\ 1/\\sqrt{2} & 1/\\sqrt{2} \\end{bmatrix}$\n",
    "- Next, we compute $u_1 = \\frac{A^Tv_1}{\\sigma_1} = \\begin{bmatrix} 1/\\sqrt{2} \\\\ 1/\\sqrt{2} \\\\ 0 \\end{bmatrix}$ and $u_2 = \\frac{A^Tv_2}{\\sigma_2} = \\begin{bmatrix} -1/\\sqrt{18} \\\\ 1/\\sqrt{18} \\\\ -4/\\sqrt{18} \\end{bmatrix}$, which are orthonormal by Theorem 9 <br>\n",
    "- $\\{u_1, u_2\\}$ is not a basis for $\\mathbb{R}^3$. So let $u_3 = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix}$ be a unit vector such that $u_1^Tu_3 = 0$ and $u_2^Tu_3 = 0$ <br>\n",
    "  then $\\{u_1, u_2, u_3\\}$ is orthonormal set of nonzero vectors, thus $\\{u_1, u_2, u_3\\}$ is a basis for $\\mathbb{R}^3$ by Theorem 4 in Chapter 6\n",
    "- $u_1^Tu_3 = 0 \\Rightarrow x_1 + x_2 + 0 = 0$, and $u_2^Tu_3 = 0 \\Rightarrow -x_1 + x_2 - 4x_3 = 0$ <br>\n",
    "  Therefore $x_1 = -x_2$ and $x_2 = 2x_3$ so $x_1 = -2x_3$, thus $u_3 = \\begin{bmatrix} -2 \\\\ 2 \\\\ 1 \\end{bmatrix} x_3 = \\begin{bmatrix} -2/3 \\\\ 2/3 \\\\ 1/3 \\end{bmatrix}$ since $u_3$ is unit vector\n",
    "- Therefore, we have $U = \\begin{bmatrix} 1/\\sqrt{2} & -1/\\sqrt{18} & -2/\\sqrt{3} \\\\ 1/\\sqrt{2} & 1/\\sqrt{18} & 2/3 \\\\ 0 & -4/\\sqrt{18} & 1/3 \\end{bmatrix}$ and $A^T = U\\Sigma V^T$, thus $A = V\\Sigma^T U^T$\n",
    "\n",
    "Suppose SVD for an $m \\times n$ matrix $A$ is constructed by proof of Theorem 10 <br>\n",
    "Note that by Theroem 4 in Chapter 6, $(\\mathrm{Col}\\ A)^\\bot = \\mathrm{Nul}\\ A^T$ and $(\\mathrm{Nul}\\ A)^\\bot = \\mathrm{Row}\\ A$, and $\\mathrm{Col}\\ A = (\\mathrm{Nul}\\ A^T)^\\bot = \\mathrm{Row}\\ A^T$ <br>\n",
    "By Theorem 9, $\\{u_1,...,u_r\\}$ is an orthonormal basis for $\\mathrm{Col}\\ A = \\mathrm{Row}\\ A^T$ <br>\n",
    "Therefore $\\{u_{r+1},...,u_m\\}$ is an orthonormal basis for $(\\mathrm{Col}\\ A)^\\bot = \\mathrm{Nul}\\ A^T$ <br>\n",
    "Since $\\|Av_i\\| = 0$ for $i > r$, we can show $n - r$ vectors $v_{r+1},...,v_n$ span $\\mathrm{Nul}\\ A$ by the Rank Theorem <br>\n",
    "Therefore $\\{v_{r+1},...,v_n\\}$ is an orthonormal basis for $\\mathrm{Nul}\\ A$ <br>\n",
    "Hence $\\{v_1,...,v_r\\}$ is an orthonormal basis for $(\\mathrm{Nul}\\ A)^\\bot = \\mathrm{Row}\\ A$  <br>\n",
    "\n",
    "The Invertible Matrix Theorem (concluded) : Let $A$ be an $n \\times n$ matrix\n",
    "- 19. $(\\mathrm{Col}\\ A)^\\bot = \\{0\\}$\n",
    "- 20. $(\\mathrm{Nul}\\ A)^\\bot = \\mathbb{R}^n$\n",
    "- 21. $\\mathrm{Row}\\ A = \\mathbb{R}^n$\n",
    "- 22. $A$ has $n$ nonzero singular values\n",
    "\n",
    "When $\\Sigma$ contains rows or columns of zeros, more compact decomposition of $A$ is possible. Let $U\\Sigma V^T$ be a SVD of $A$ <br>\n",
    "And let $U = \\begin{bmatrix} U_r & U_{m-r} \\end{bmatrix}$, $V = \\begin{bmatrix} V_r & V_{n-r} \\end{bmatrix}$, where $U_r = \\begin{bmatrix} u_1 & \\cdots & u_r \\end{bmatrix}$, $V_r = \\begin{bmatrix} v_1 & \\cdots & v_r \\end{bmatrix}$ <br>\n",
    "Then since $U_r$ is $m \\times r$ and $V_r^T$ is $r \\times n$, we have $A = \\begin{bmatrix} U_r & U_{m-r} \\end{bmatrix} \\begin{bmatrix} D & 0 \\\\ 0 & 0 \\end{bmatrix} \\begin{bmatrix} V_r^T \\\\ V_{n-r}^T \\end{bmatrix} = U_rDV_r^T$, called a reduced SVD <br>\n",
    "\n",
    "Pesudoinverse : $A^+ = V_rD^{-1}U_r^T$ when a reduced SVD of $A$ is $U_rDV_r^T$\n",
    "- $AA^+ y = U_rDV_r^TV_rD^{-1}U_r^T y = U_rDD^{-1}U_r^T y = U_rU_r^T y$ <br>\n",
    "  Since $\\{u_1,...,u_r\\}$ is an orthonormal basis for $\\mathrm{Col}\\ A$, we conclude $AA^+ y$ is the orthogonal projection of $y$ onto $\\mathrm{Col}\\ A$\n",
    "- $A^+A x = V_rD^{-1}U_r^TU_rDV_r^T x = V_rD^{-1}DV_r^T x = V_rV_r^T x$ <br>\n",
    "  Since $\\{v_1,...,v_r\\}$ is an orthonormal basis for $\\mathrm{Row}\\ A$, we conclude $A^+A x$ is the orthogonal projection of $x$ onto $\\mathrm{Nul}\\ A$\n",
    "- $AA^+A = U_rDV_r^T V_rD^{-1}U_r^T U_rDV_r^T = U_rDV_r^T = A$, and $A^+AA^+ = V_rD^{-1}U_r^T U_rDV_r^T V_rD^{-1}U_r^T = V_rD^{-1}U_r^T = A^+$\n",
    "\n",
    "Suppose the equation $Ax = b$ is consistent, and let $x^+ = A^+b$\n",
    "- Let $v$ be the solution of $Ax = b$. Then $Av = b \\Rightarrow A^+Av = A^+b = x^+$ <br>\n",
    "  Since $A^+Av$ is the orthogonal projection of $v$ onto $\\mathrm{Row}\\ A$, we conclude $x^+$ is in $\\mathrm{Row}\\ A$\n",
    "- On the other hand, $v$ can be represented by $p + u$ where $p$ is in $\\mathrm{Row}\\ A$ and $u$ is in $\\mathrm{Nul}\\ A$ because $(\\mathrm{Row}\\ A)^\\bot = \\mathrm{Nul}\\ A$ <br>\n",
    "  But, Since $Av = A(p+u) = Ap + Au = Ap + 0 = Ap$, we conclude $\\mathrm{proj}_{\\mathrm{Row}\\ A}v$ is also the solution of $Ax = b$ <br>\n",
    "  Therefore $x^+$ is the solution of $Ax = b$\n",
    "\n",
    "Suppose the equation $Ax = b$ is inconsistent, and let $x^+ = A^+b$ <br>\n",
    "Let $v$ be the least-squares solution of minimum length, then $Av = \\mathrm{proj}_{\\mathrm{Col}\\ A}b$ <br>\n",
    "Therefore, since $AA^+b$ is the orthogonal projection of $b$ onto $\\mathrm{Col}\\ A$, we conclude $v = x^+$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca52b98c",
   "metadata": {},
   "source": [
    "### 5) Applications to Image Processing and Statistics\n",
    "\n",
    "Let $X_1, ..., X_N$ denote the observation vector in $\\mathbb{R}^p$, then $\\begin{bmatrix} X_1 & \\cdots & X_N \\end{bmatrix}$ is called the matrix of observations <br>\n",
    "The sample mean : $M = \\frac{1}{N}(X_1 + \\cdots + X_N)$. And for $k = 1, ..., N$, let $\\hat{X}_k = X_k - M$ <br>\n",
    "$B$ is in the mean-deviation form : $B = \\begin{bmatrix} \\hat{X}_1 & \\cdots & \\hat{X}_N \\end{bmatrix}$. whose sample mean is zero <br>\n",
    "The (sample) covariance matrix : $S = \\frac{1}{N-1}BB^T$, which is $p \\times p$ matrix <br>\n",
    "\n",
    "For $k = 1, ..., p$, let $x_k$ be a scalar that varies over the set of $k$-th coordinates of $X_1,...,X_n$, and let $X = \\begin{bmatrix} x_1 \\\\ \\vdots \\\\ x_p \\end{bmatrix}$ <br>\n",
    "Denote the covariance matrix $S = [s_{ij}]$, then the diagonal entry $s_{jj}$ in $S$ is called the **variance** of $x_j$ <br>\n",
    "so **the total variance** is the trace of $S$, written $\\mathrm{tr}(S)$, which means that the sum of the diagonal entries <br>\n",
    "and the entry $s_{ij}$ in $S$ for $i \\neq j$ is called the **covariance** of $x_i$ and $x_j$ <br>\n",
    "Statistician say that if $s_{ij} = 0$, then $x_i$ and $x_j$ are uncorrelated ( Warning : 'uncorrelated' $\\Rightarrow$ 'independent' is false ) <br>\n",
    "\n",
    "\n",
    "Principal Component Analysis (PCA) : find the variables which are uncorrelated\n",
    "- Assume that $B = \\begin{bmatrix} X_1 & \\cdots & X_N \\end{bmatrix}$ is already in mean-deviation form <br>\n",
    "  The goal of PCA is to find an orthogonal $p \\times p$ matrix $P = \\begin{bmatrix} u_1 & \\cdots & u_p \\end{bmatrix}$ that determines a change of variable, $X = PY$ <br>\n",
    "  where $X = \\begin{bmatrix} x_1 \\\\ \\vdots \\\\ x_p \\end{bmatrix}$, $Y = \\begin{bmatrix} y_1 \\\\ \\vdots \\\\ y_p \\end{bmatrix}$, and $y_1,...,y_p$ are uncorrelated variables arranged in order of decreasing variance\n",
    "- $X = PY$ means that each observation vector $X_k$ receives a $Y_k$ such that $X_k = PY_k$ <br>\n",
    "  because each $x_i$ varies over the set of $i$-th coordinates of $X_1,...,X_N$, and $y_i$ also does\n",
    "- Let $S$ be the covariance matrix of $X_1,...,X_N$, then the covariance matrix of $Y_1,...,Y_N$ is $P^TSP$ \n",
    "  - Since $B$ is in mean-deviation form, $X_1 + \\cdots + X_N = 0$, and<br>\n",
    "    $\\Rightarrow B\\begin{bmatrix} 1 \\\\ \\vdots \\\\ 1 \\end{bmatrix} = 0 \\Rightarrow P^{-1}B\\begin{bmatrix} 1 \\\\ \\vdots \\\\ 1 \\end{bmatrix} = 0 \\Rightarrow P^{-1}X_1 + \\cdots + P^{-1}X_N = 0 \\Rightarrow Y_1 + \\cdots + Y_N = 0$. <br>\n",
    "    Therefore $P^{-1}B = P^TB = \\begin{bmatrix} Y_1 & ... & Y_N \\end{bmatrix}$ is in mean-deviation form\n",
    "  - Since $S = \\frac{1}{N-1}BB^T$, the covariance matrix of $Y_1,...,Y_N$ is $\\frac{1}{N-1}(P^TB)(P^TB)^T = P^T(\\frac{1}{N-1}BB^T)P = P^TSP$\n",
    "- Since $S = \\frac{1}{N-1}BB^T$, which is symmetric matrix, $S$ is orthogonally diagonalizable <br>\n",
    "  So we have $S = PDP^T$ where $D$ is a diagonal matrix with the eigenvalues arranged so that $\\lambda_1 \\geq \\cdots \\geq \\lambda_p \\geq 0$ <br>\n",
    "  and $P$ is an orthogonal matrix whose columns are the corresponding unit eigenvectors $u_1,...,u_p$ <br>\n",
    "  ( For $1 \\le i \\le p$, $\\frac{1}{N-1}\\|B^Tu_i\\|^2 = \\frac{1}{N-1}(B^Tu_i)^T(B^Tu_i) = \\frac{1}{N-1}u_i^TBB^Tu_i = u_i^T\\lambda_iu_i = \\lambda_i \\geq 0$. Therefore $\\lambda_1 \\geq \\cdots \\geq \\lambda_p \\geq 0$ )\n",
    "- Then $P^TSP = D$. So, at this time, the covariance matrix of $Y_1,...,Y_N$ is $D$ <br>\n",
    "  Therefore $y_1,...,y_p$ are uncorrelated because $D$ is diagonal, <br>\n",
    "  and arranged in order of decreasing variance because $\\lambda_1 \\geq \\cdots \\geq \\lambda_p$\n",
    "- $u_1,...,u_p$ of $S$ are called the principal components of the data in the matrix of beservations <br>\n",
    "  and first PC is $u_1$, and second PC is $u_2$, and so on. Note that $\\lambda_1 \\geq \\lambda_2 \\geq \\cdots \\geq \\lambda_n$\n",
    "- $Y = P^{-1}X = P^TX$ shows that $y_1 = u_1^TX = c_1x_1 + \\cdots + c_px_p$, <br>\n",
    "  thus $y_1$ is a linear combination of the original variables $x_1,...,x_p$, using the entries in the eigenvector $u_1$ as weights <br>\n",
    "  In a similar fashion, $u_2$ determines the varaible $y_2$, and so on\n",
    "  \n",
    "An orthogonal change of variables $X = PY$ does not change the total variance of the data\n",
    "- If $S$ is the covariance matrix of $X$, than the covariance matrix of $Y$ is $P^TSP$\n",
    "- Since $\\mathrm{tr}(PS) = \\sum_{i=1}^n\\sum_{j=1}^n p_{ij}s_{ji} = \\sum_{j=1}^n\\sum_{i=1}^n s_{ji}p_{ij} = \\mathrm{tr}(SP)$, <br>\n",
    "  we have $\\mathrm{tr}(P^{-1}PS) = \\mathrm{tr}(S) = \\mathrm{tr}(P^{-1}SP) = \\mathrm{tr}(P^TSP)$, <br>\n",
    "  which means that total variance of $X$ is equal to the total variance of $Y$\n",
    "\n",
    "The variance of $y_j$ is $\\lambda_j$, and the quotient $\\frac{\\lambda_j}{\\mathrm{tr}(S)}$ measures the fraction of the total variance that is \"explained\" by $y_j$ <br>\n",
    "If $\\frac{\\lambda_j}{\\mathrm{tr}(S)}$ is very small, than we can remove $y_j$ to reduce the dimension <br>\n",
    "\n",
    "In PCA, we can check that the variance of $y_1$ is the result of maximizing the variance of $y_1$\n",
    "- If $v$ is any unit vector and if $y_1 = v^TX$, the variance of $y_1$ becomes $v^TSv$\n",
    "- By Theorem 8 in Chapter 6, the maximum value of $u^TSu$ is the largest eigenvalue $\\lambda_1$ of $S$, <br>\n",
    "  and this variance is attained when $v$ is the corresponding eigenvector $u_1$, and in fact, $v = u_1$\n",
    "- In the same way, we can show $y_2$ has maximum possible variance among all varaibles that are uncorrelated with $y_1$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671fd8f5",
   "metadata": {},
   "source": [
    "*****"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3164960",
   "metadata": {},
   "source": [
    "## Chapter 8 : The Geometry of Vector Spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac038b9a",
   "metadata": {},
   "source": [
    "### 1) Affine Combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b9f9cb",
   "metadata": {},
   "source": [
    "### 2) Affine Independence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0e36c4",
   "metadata": {},
   "source": [
    "### 3) Convex Combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd19f391",
   "metadata": {},
   "source": [
    "### 4) Hyperplanes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ba959c",
   "metadata": {},
   "source": [
    "### 5) Polytopes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544eaa8e",
   "metadata": {},
   "source": [
    "### 6) Curves and Surfaces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355ea05a",
   "metadata": {},
   "source": [
    "*****"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c793a72e",
   "metadata": {},
   "source": [
    "## Chapter 9 : Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b095af18",
   "metadata": {},
   "source": [
    "### 1) Matrix Games"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6d94b9",
   "metadata": {},
   "source": [
    "### 2) Linear Programming ─ Geometric Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8425ad21",
   "metadata": {},
   "source": [
    "### 3) Linear Programming ─ Simplex Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d65213",
   "metadata": {},
   "source": [
    "### 4) Duality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58732f1f",
   "metadata": {},
   "source": [
    "*****"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36482243",
   "metadata": {},
   "source": [
    "## Chapter 10 : Finite-State Markov Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e076fce9",
   "metadata": {},
   "source": [
    "### 1) Introduction and Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d01e54",
   "metadata": {},
   "source": [
    "### 2) The Steady-State Vector and Google's PageRank"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f5c7c3",
   "metadata": {},
   "source": [
    "### 3) Communication Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e041e82",
   "metadata": {},
   "source": [
    "### 4) Classification of States and Periodicity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e4dc9a",
   "metadata": {},
   "source": [
    "### 5) The Fundamental Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998881ac",
   "metadata": {},
   "source": [
    "### 6) Markov Chains and Baseball Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1c4452",
   "metadata": {},
   "source": [
    "*****"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
